{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Math Kernel Library (oneMKL) and SYCL Basic Parallel Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next set of modules, we will explore how we can utilize oneAPI and SYCL to implement Matrix Multiplication using Intel® oneAPI Math Kernel Library (oneMKL) and also implement Matrix Multiplication in the most basic parallel form and then improve performance by tuning the kernel code while trying to maintain performance portability. All code improvements will be measured in terms of relative performance to oneMKL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Gain familiarity with oneMKL and able to use it for a two dimensional GEMM.\n",
    "- Use a basic GEMM application for basis of enhancements.\n",
    "- Interpret roofline and VTune™ analyzer results as a method to measure the GEMM applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intel oneAPI Math Kernel Library (oneMKL)\n",
    "\n",
    "One of the best ways to achieve performance portable code is to take advantage of a library.  In this case oneMKL offers a compelling GEMM implementation that we will use as our baseline.  If there is a library, it should always be ones first attempt at achieving performant portable code.  All other implementations will be measured against oneMKL.  \n",
    "\n",
    "Intel oneMKL is included in the Intel oneAPI toolkits and there is extensive documentation at [Get Started with Intel oneAPI Math Kernel Library.](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html)\n",
    "\n",
    "The Intel® oneAPI Math Kernel Library (oneMKL) helps you achieve maximum performance with a math computing library of highly optimized, extensively parallelized routines for CPU and GPU. The library has C and Fortran interfaces for most routines on CPU, and SYCL interfaces for some routines on both CPU and GPU. You can find comprehensive support for several math operations in various interfaces including:\n",
    "\n",
    "SYCL on CPU and GPU \n",
    "(Refer to the Intel® oneAPI Math Kernel Library—Data Parallel C++ Developer Reference for more details.)\n",
    "- Linear algebra\n",
    "- BLAS\n",
    "- Selected Sparse BLAS functionality\n",
    "- Selected LAPACK functionality\n",
    "- Fast Fourier Transforms (FFT)\n",
    "- 1D r2c FFT\n",
    "- 1D c2c FFT\n",
    "- Random number generators\n",
    "- Single precision Uniform, Gaussian, and Lognormal distributions\n",
    "- Selected Vector Math functionality\n",
    "\n",
    "The example below uses the GEMM function from the oneMKL BLAS routine,  which computes a scalar-matrix-matrix product and add the result to a scalar-matrix product, with general matrices. The operation is defined as:\n",
    "\n",
    "```cpp\n",
    "void gemm(queue &exec_queue, transpose transa, transpose transb, std::int64_t m, std::int64_t n, std::int64_t k, T alpha, buffer<T, 1> &a, std::int64_t lda, buffer<T, 1> &b, std::int64_t ldb, T beta, buffer<T, 1> &c, std::int64_t ldc)\n",
    "```\n",
    "\n",
    "This one line of oneMKL function does all of the necessary optimizations for CPU/GPU offload compute and as you go through the exercises you will discover that it did indeed deliver the best results with the least amount of code across all of the platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication with Math Kernel Library (oneMKL)\n",
    "The following SYCL code below uses a oneMKL kernel: Inspect code, there are no modifications necessary:\n",
    "\n",
    "1. __Run__ ▶the cell following __Select offload device__, in Jupyter everything is linear, a subsequent run will need to choose a new target and then the following cell will need to be executed to get the updated results.\n",
    "2. Inspect the following code cell and click __Run__ ▶ to save the code to a file.\n",
    "3. Next run -- the cell in the __Build and Run__ section below the code to compile and execute the code.\n",
    "\n",
    "#### Select Offload Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run accelerator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/mm_dpcpp_mkl.cpp\n",
    "//==============================================================\n",
    "// Matrix Multiplication: SYCL oneMKL\n",
    "//==============================================================\n",
    "// Copyright © 2021 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "#include \"oneapi/mkl/blas.hpp\"  //# oneMKL DPC++ interface for BLAS functions\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "void mm_kernel(queue &q, std::vector<float> &matrix_a, std::vector<float> &matrix_b, std::vector<float> &matrix_c, size_t N, size_t M) {\n",
    "    std::cout << \"Configuration         : MATRIX_SIZE= \" << N << \"x\" << N << \"\\n\";\n",
    "    \n",
    "    //# Create buffers for matrices\n",
    "    buffer a(matrix_a);\n",
    "    buffer b(matrix_b);\n",
    "    buffer c(matrix_c);\n",
    "\n",
    "    //# scalar multipliers for oneMKL\n",
    "    float alpha = 1.f, beta = 1.f;\n",
    "\n",
    "    //# transpose status of matrices for oneMKL\n",
    "    oneapi::mkl::transpose transA = oneapi::mkl::transpose::nontrans;\n",
    "    oneapi::mkl::transpose transB = oneapi::mkl::transpose::nontrans;\n",
    "\n",
    "    //# Submit MKL library call to execute on device\n",
    "    oneapi::mkl::blas::gemm(q, transA, transB, N, N, N, alpha, b, N, a, N, beta, c, N);\n",
    "    c.get_access<access::mode::read>();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click __Run__ ▶ to compile and execute the code on selected device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_mm_mkl.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_mm_mkl.sh \"{device.value}\"; else ./run_mm_mkl.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roofline Report\n",
    "\n",
    "Execute the following line to display the roofline results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run display_data/mm_mkl_roofline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VTune™ Profiler Summary\n",
    "\n",
    "Execute the following line to display the VTune results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run display_data/mm_mkl_vtune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Parallel Kernel Implementation\n",
    "\n",
    "In this section we will look at how matrix multiplication can be implemented using a SYCL basic parallel kernel. This is the most simplest implementation using SYCL without any optimizations. In the next few modules we will add optimization on top of this implementation to improve the performance. \n",
    "\n",
    "<img src=\"Assets/naive.PNG\">\n",
    "\n",
    "We can define the kernel with `parallel_for` with a 2-dimentional range for the matrix and perform matrix multiplication as shown below:\n",
    "\n",
    "\n",
    "```cpp\n",
    "        h.parallel_for(range<2>{N,N}, [=](item<2> item){\n",
    "            const int i = item.get_id(0);\n",
    "            const int j = item.get_id(1);\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i*N+j] += A[i*N+k] * B[k*N+j];\n",
    "            }\n",
    "        });\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication with basic parallel kernel\n",
    "\n",
    "The following SYCL code shows the basic parallel kernel implementation of matrix multiplication. Inspect code; there are no modifications necessary:\n",
    "\n",
    "1. Run the cell in the __Select Offload Device__ section to choose a target device to run the code on.\n",
    "2. Inspect the following code cell and click __Run__ ▶ to save the code to a file.\n",
    "3. Next, run the cell in the __Build and Run__ section to compile and execute the code.\n",
    "\n",
    "#### Select Offload Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run accelerator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lab/mm_dpcpp_basic.cpp\n",
    "//==============================================================\n",
    "// Matrix Multiplication: SYCL Basic Parallel Kernel\n",
    "//==============================================================\n",
    "// Copyright © 2021 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "void mm_kernel(queue &q, std::vector<float> &matrix_a, std::vector<float> &matrix_b, std::vector<float> &matrix_c, size_t N, size_t M) {\n",
    "    std::cout << \"Configuration         : MATRIX_SIZE= \" << N << \"x\" << N << \"\\n\";\n",
    "    \n",
    "    //# Create buffers for matrices\n",
    "    buffer a(matrix_a);\n",
    "    buffer b(matrix_b);\n",
    "    buffer c(matrix_c);\n",
    "\n",
    "    //# Submit command groups to execute on device\n",
    "    auto e = q.submit([&](handler &h){\n",
    "        //# Create accessors to copy buffers to the device\n",
    "        accessor A(a, h, read_only);\n",
    "        accessor B(b, h, read_only);\n",
    "        accessor C(c, h, write_only);\n",
    "\n",
    "        //# Parallel Compute Matrix Multiplication\n",
    "        h.parallel_for(range<2>{N,N}, [=](item<2> item){\n",
    "            const int i = item.get_id(0);\n",
    "            const int j = item.get_id(1);\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i*N+j] += A[i*N+k] * B[k*N+j];\n",
    "            }\n",
    "        });\n",
    "    });\n",
    "    host_accessor hc(c, read_only);\n",
    "    \n",
    "    //# print kernel compute duration from event profiling\n",
    "    auto kernel_duration = (e.get_profiling_info<info::event_profiling::command_end>() - e.get_profiling_info<info::event_profiling::command_start>());\n",
    "    std::cout << \"Kernel Execution Time : \" << kernel_duration / 1e+9 << \" seconds\\n\";\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click __Run__ ▶ to compile and execute the code on selected device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_mm_basic.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_mm_basic.sh \"{device.value}\"; else ./run_mm_basic.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roofline Report\n",
    "\n",
    "Execute the following line to display the roofline results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run display_data/mm_basic_roofline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VTune™ Profiler Summary\n",
    "\n",
    "Execute the following line to display the VTune results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run display_data/mm_basic_vtune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Comparing the execution times for Basic SYCL implementation and Math Kernel Library implementation for various matrix sizes, we can see that for small matrix size of 1024x1024, Basic SYCL implementation performs better than MKL implementation. When matrix size is large, MKL implementation out performs Basic SYCL implementation significantly. The graph below shows execution times on various hardware for matrix sizes 1024x1024, 5120x5120 and 10240x10240.\n",
    "\n",
    "<img src=Assets/ppp_basic_mkl_graph.PNG>\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this module we looked at oneAPI Math Kernel Library (oneMKL) and implemented matrix multiplication using oneMKL function. We also implemented matrix multiplication using SYCL basic parallel kernel. We compared performance numbers for the two implementations and can see benefits of using a library link oneMKL to implement computation rather than basic implementation using SYCL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Check out these related resources\n",
    "\n",
    "#### Intel® oneAPI Toolkit documentation\n",
    "\n",
    "* [Intel Advisor Roofline](https://software.intel.com/content/www/us/en/develop/articles/intel-advisor-roofline.html)\n",
    "* [Intel VTune](https://software.intel.com/content/www/us/en/develop/documentation/vtune-help/top/introduction.html)\n",
    "* [Intel® oneAPI main page](https://software.intel.com/oneapi \"oneAPI main page\")\n",
    "* [Intel® oneAPI programming guide](https://software.intel.com/sites/default/files/oneAPIProgrammingGuide_3.pdf \"oneAPI programming guide\")\n",
    "* [Intel® DevCloud Signup](https://software.intel.com/en-us/devcloud/oneapi \"Intel DevCloud\")  Sign up here if you do not have an account.\n",
    "* [Get Started with oneAPI for Linux*](https://software.intel.com/en-us/get-started-with-intel-oneapi-linux)\n",
    "* [Get Started with oneAPI for Windows*](https://software.intel.com/en-us/get-started-with-intel-oneapi-windows)\n",
    "* [Intel® oneAPI Code Samples](https://software.intel.com/en-us/articles/code-samples-for-intel-oneapibeta-toolkits)\n",
    "* [oneAPI Specification elements](https://www.oneapi.com/spec/)\n",
    "\n",
    "#### SYCL \n",
    "* [SYCL* 2020 Specification](https://www.khronos.org/registry/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf)\n",
    "\n",
    "#### Modern C++\n",
    "* [CPPReference](https://en.cppreference.com/w/)\n",
    "* [CPlusPlus](http://www.cplusplus.com/)\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
