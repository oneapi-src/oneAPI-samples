{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to oneAPI and DPC++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [oneAPI Programming Model Overview](#oneAPI-Software-Model-Overview)\n",
    "- [Programming Challenges for Multiple architectures](#Programming-Challenges-for-Multiple-architectures)\n",
    "- [Introducing oneAPI](#Introducing-oneAPI)\n",
    "- _Code:_ [DPC++ Hello World](#Simple-Exercise)\n",
    "- [What is Data Parallel C++](#What-is-Data-Parallel-C++)\n",
    "- [How to Compile & Run a DPC++ program](#How-to-Compile-&-Run-DPC++-program)\n",
    "- _Code:_ [Simple Vector Increment to Vector Add](#Lab-Exercise:-Simple-Vector-Increment-TO-Vector-Add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Explain how the __oneAPI__ programming model can solve the challenges of programming in a heterogeneous world \n",
    "* Use oneAPI projects to enable your workflows\n",
    "* Understand the __Data Parallel C++ (DPC++)__ language and programming model\n",
    "* Familiarization on the use Jupyter notebooks for training throughout the course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oneAPI Programming Model Overview\n",
    "The __oneAPI__ programming model provides a comprehensive and unified portfolio of developer tools that can\n",
    "be used across hardware targets, including a range of performance libraries spanning several workload\n",
    "domains. The libraries include functions custom-coded for each target architecture so the same\n",
    "function call delivers optimized performance across supported architectures. __DPC++__ is based on\n",
    "industry standards and open specifications to encourage ecosystem collaboration and innovation.\n",
    "\n",
    "### oneAPI Distribution\n",
    "Intel&reg; oneAPI toolkits are available via multiple distribution channels:\n",
    "* Local product installation: install the oneAPI toolkits from the __Intel® Developer Zone__.\n",
    "* Install from containers or repositories: install the oneAPI toolkits from one of several supported\n",
    "containers or repositories.\n",
    "* Pre-installed in the __Intel® DevCloud__: a free development sandbox for access to the latest Intel® SVMS hardware and select oneAPI toolkits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Challenges for Multiple architectures\n",
    "Currently in the data centric space there is growth in specialized workloads. Each kind of data centric hardware typically needs to be programmed using different languages and libraries as there is no common programming language or APIs, this requires maintaining separate code bases. Developers have to learn a whole set of different tools as there is inconsistent tool support across platforms. Developing software for each hardware platform requires a separate investment, with little ability to reuse that work to target a different architecture. You will also have to consider the requirement of the diverse set of data-centric hardware.\n",
    "\n",
    "<img src=\"Assets/oneapi1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing oneAPI\n",
    "__oneAPI__ is a solution to deliver unified programming model to __simplify development__ across diverse architectures. It includes a unified and simplified language and libraries for expressing __parallelism__ and delivers uncompromised native high-level language performance across a range of hardware including __CPUs, GPUs, FPGAs__. oneAPI initiative is based on __industry standards and open specifications__ and is interoperable with existing HPC programming models.\n",
    "\n",
    "<img src=\"Assets/oneapi2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Simple Exercise\n",
    "This exercise introduces DPC++ to the developer by way of a small simple code. In addition, it introduces the developer to the Jupyter notebook environment for editing and saving code; and for running and submitting programs to the Intel® DevCloud.\n",
    "\n",
    "##  Editing the simple.cpp code\n",
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "\n",
    "The first line of the cell contains the command **%%writefile 'simple.cpp'** This tells the input cell to save the contents of the cell into a file named 'simple.cpp' in your current directory (usually your home directory). As you edit the cell and run it in the Jupyter notebook, it will save your changes into that file.\n",
    "\n",
    "The code below is some simple DPC++ code to get you started in the DevCloud environment. Simply inspect the code - there are no modifications necessary. Run the first cell to create the file, then run the cell below it to compile and execute the code.\n",
    "1. Inspect the code cell below, then click run ▶ to save the code to a file\n",
    "2. Run ▶ the cell in the __Build and Run__ section below the code snippet to compile and execute the code in the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/simple.cpp\n",
    "//==============================================================\n",
    "// Copyright © 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <CL/sycl.hpp>\n",
    "using namespace sycl;\n",
    "static const int N = 16;\n",
    "int main(){\n",
    "  //# define queue which has default device associated for offload\n",
    "  queue q;\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<info::device::name>() << std::endl;\n",
    "\n",
    "  //# Unified Shared Memory Allocation enables data access on host and device\n",
    "  int *data = malloc_shared<int>(N, q);\n",
    "\n",
    "  //# Initialization\n",
    "  for(int i=0; i<N; i++) data[i] = i;\n",
    "\n",
    "  //# Offload parallel computation to device\n",
    "  q.parallel_for(range<1>(N), [=] (id<1> i){\n",
    "    data[i] *= 2;\n",
    "  }).wait();\n",
    "\n",
    "  //# Print Output\n",
    "  for(int i=0; i<N; i++) std::cout << data[i] << std::endl;\n",
    "\n",
    "  free(data, q);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_simple.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_simple.sh; else ./run_simple.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYCL\n",
    "__SYCL__ (pronounced ‘sickle’) represents an industry standardization effort that includes\n",
    "support for data-parallel programming for C++. It is summarized as “C++ Single-source\n",
    "Heterogeneous Programming for OpenCL.” The SYCL standard, like OpenCL*, is managed\n",
    "by the __Khronos Group*__.\n",
    "\n",
    "SYCL is a cross-platform abstraction layer that builds on OpenCL. It enables code\n",
    "for heterogeneous processors to be written in a “single source” style using C++. This is not\n",
    "only useful to the programmers, but it also gives a compiler the ability to analyze and\n",
    "optimize across the entire program regardless of the device on which the code is to be run.\n",
    "\n",
    "Unlike OpenCL, SYCL includes templates and lambda functions to enable higher-level application software to be cleanly coded with optimized acceleration of kernel code.\n",
    "Developers program at a higher level than OpenCL but always have access to lower-level code through seamless integration with OpenCL, as well as C/C++ libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Parallel C++\n",
    "__oneAPI__ programs are written in __Data Parallel C++ (DPC++)__. It takes advantage of modern C++ productivity benefits and familiar constructs, and incorporates the __SYCL*__ standard for data parallelism and heterogeneous programming. DPC++ is a __single source__ language where host code and __heterogeneous accelerator kernels__ can be mixed in same source files. A DPC++ program is invoked on the host computer and offloads the computation to an accelerator. Programmers use familiar C++ and library constructs with added functionliaties like a __queue__ for work targeting, __buffer__ for data management, and __parallel_for__ for parallelism to direct which parts of the computation and data should be offloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPC++ extends SYCL 1.2.1\n",
    "DPC++ programs __enhance productivity__. Simple things should be simple to express and lower verbosity and programmer burden. They also __enhance performance__ by giving programmers control over program execution and by enabling hardware-specific features. It is a fast-moving open collaboration feeding into the __SYCL* standard__, and is an __open source__ implementation with the goal of upstreaming LLVM and DPC++ extensions to become core __SYCL*__, or __Khronos*__ extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Single Node Workflow with oneAPI \n",
    "Accelerated code can be written in either a kernel (DPC++) or __directive based style__. Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA__ to __Data Parallel C++__. Existing __Fortran__ applications can use a __directive style based on OpenMP__. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__ and existing __OpenCL__ applications can remain in the OpenCL language or migrate to Data Parallel C++.\n",
    "\n",
    "__Intel® Advisor__ is recommended to  __Optimize__ the design for __vectorization and memory__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__\n",
    "\n",
    "The figure below shows the recommended approach of different starting points for HPC developers:\n",
    "\n",
    "\n",
    "<img src=\"Assets/workflow.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oneAPI Programming models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Model\n",
    "\n",
    "The platform model for oneAPI is based upon the SYCL* platform model. It specifies a host controlling one or more devices. A host is the computer, typically a CPU-based system executing the primary portion of a program, specifically the application scope and the command group scope. \n",
    "\n",
    "The host coordinates and controls the compute work that is performed on the devices. A device is an accelerator, a specialized component containing compute resources that can quickly execute a subset of operations typically more efficiently than the CPUs in the system. Each device contains one or more compute units that can execute several operations in parallel. Each compute unit contains one or more processing elements that serve as the individual engine for computation.\n",
    "\n",
    "The following figure provides a visual depiction of the relationships in the platform model. One host communicates with one or more devices. Each device can contain one or more compute units. Each compute unit can contain one or more processing elements. In this example, the CPU in a desktop computer is the host and it can also be made available as a device in a platform configuration.\n",
    "\n",
    "<img src=\"Assets/plat30.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Model\n",
    "\n",
    "The execution model is based upon the SYCL* execution model. It defines and specifies how code, termed kernels, execute on the devices and interact with the controlling host.\n",
    "The host execution model coordinates execution and data management between the host and devices via command groups. The command groups, which are groupings of commands like kernel invocation and accessors, are submitted to queues for execution.\n",
    "\n",
    "Accessors, which are formally part of the memory model, also communicate ordering requirements of execution. A program employing the execution model declares and instantiates queues. Queues can execute with an in-order or out-of-order policy controllable by the program. In-order execution is an Intel extension.\n",
    "\n",
    "The device execution model specifies how computation is accomplished on the accelerator. Compute ranging from small one-dimensional data to large multidimensional data sets are allocated across a hierarchy of ND-ranges, work-groups, sub-groups (Intel extension), and work-items, which are all specified when the work is submitted to the command queue.\n",
    "\n",
    "It is important to note that the actual kernel code represents the work that is executed for one work-item. The code outside of the kernel controls just how much parallelism is executed; the amount and distribution of the work is controlled by specification of the sizes of the ND-range and work-group.\n",
    "\n",
    "\n",
    "The following figure depicts the relationship between an ND-range, work-group, sub-group, and work-item. The total amount of work is specified by the ND-range size. The grouping of the work is specified by the work-group size. The example shows the ND-range size of X * Y * Z, work-group size of X’ * Y’ * Z’, and subgroup size of X’. Therefore, there are X * Y * Z work-items. There are (X * Y * Z) / (X’ * Y’ * Z’) work-groups and (X * Y * Z) / X’ subgroups.\n",
    "\n",
    "<img src=\"Assets/kernel30.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Model\n",
    "\n",
    "The memory model for oneAPI is based upon the SYCL* memory model. It defines how the host and devices interact with memory. It coordinates the allocation and management of memory between the host and devices. The memory model is an abstraction that aims to generalize across and be adaptable to the different possible host and device configurations.\n",
    "\n",
    "In this model, memory resides upon and is owned by either the host or the device and is specified by declaring a memory object. There are two different types of memory objects, buffers and images. Interaction of these memory objects between the host and device is accomplished via an accessor, which communicates the desired location of access, such as host or device, and the particular mode of access, such as read or write.\n",
    "\n",
    "Consider a case where memory is allocated on the host through a traditional malloc call. Once the memory is allocated on the host, a buffer object is created, which enables the host allocated memory to be communicated to the device. The buffer class communicates the type and number of items of that type to be communicated to the device for computation. Once a buffer is created on the host, the type of access allowed on the device is communicated via an accessor object, which specifies the type of access to the buffer.\n",
    "\n",
    "<img src=\"Assets/memory.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Programming Model\n",
    "The kernel programming model for oneAPI is based upon the SYCL* kernel programming model. It enables explicit parallelism between the host and device. The parallelism is explicit in the sense that the programmer determines what code executes on the host and device; it is not automatic. The kernel code executes on the accelerator. \n",
    "\n",
    "Programs employing the oneAPI programming model support single source, meaning the host code and device code can be in the same source file. However, there are differences between the source code accepted in the host code and the device code with respect to language conformance and language features. \n",
    "\n",
    "The SYCL Specification defines in detail the required language features for host code and device code. The following is a summary that is specific to the oneAPI product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Compile & Run DPC++ program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three main steps of compiling and running a DPC++ program are:\n",
    "1. Initialize environment variables\n",
    "2. Compile the DPC++ source code\n",
    "3. Run the application\n",
    " \n",
    "#### Compiling and Running on Intel&reg; DevCloud:\n",
    " \n",
    "For this training, we have written a script (q) to aid developers in developing projects on DevCloud. This script submits the `run.sh` script to a gpu node on DevCloud for execution, waits for the job to complete and prints out the output/errors. We will be using this command to run on DevCloud: `./q run.sh`\n",
    "\n",
    "\n",
    "\n",
    "#### Compiling and Running on a Local System:\n",
    "\n",
    "If you have installed the Intel&reg; oneAPI Base Toolkit on your local system, you can use the commands below to compile and run a DPC++ program:\n",
    "\n",
    "    source /opt/intel/inteloneapi/setvars.sh\n",
    "\n",
    "    dpcpp simple.cpp -o simple\n",
    "\n",
    "    ./simple\n",
    "    \n",
    "_Note: run.sh script is a combination of the three steps listec above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise: Simple Vector Increment TO Vector Add\n",
    "### Code Walkthrough\n",
    "\n",
    "__DPC++ programs__ are standard C++. The program is invoked on the __host__ computer, and offloads computation to the __accelerator__. You will use DPC++’s __queue, buffer, device, and kernel abstractions__ to direct which parts of the computation and data should be offloaded.\n",
    "\n",
    "The DPC++ compiler and the oneAPI libraries automate the tedious and error-prone aspects of compute and data offload, but still allow you to control how computation and data are distributed for best performance. The compiler knows how to generate code for both the host and the accelerator, how to launch computation on the accelerator, and how to move data back and forth. \n",
    "\n",
    "In the program below you will use a data parallel algorithm with DPC++ to leverage the computational power in __heterogenous computers__. The DPC++ platform model includes a host computer and a device. The host offloads computation to the device, which could be a __GPU, FPGA, or a multi-core CPU__.\n",
    "\n",
    "As a first step in a DPC++ program, create a __queue__. Offload computation to a __device__ by submitting tasks to a queue. You can choose CPU, GPU, FPGA, and other devices through the __selector__. This program uses the default q here, which means the DPC++ runtime selects the most capable device available at runtime by using the default selector. You will learn more about devices, device selectors, and the concepts of buffers, accessors and kernels in the upcoming modules, but here is a simple DPC++ program to get you started.\n",
    "\n",
    "Device and host can either share physical __memory__ or have distinct memories. When the memories are distinct, offloading computation requires __copying data between host and device__. DPC++ does not require you to manage the data copies. By creating __Buffers and Accessors__, DPC++ ensures that the data is available to host and device without any effort on your part. DPC++ also allows you explicit control over data movement to achieve best peformance.\n",
    "\n",
    "In a DPC++ program, we define a __kernel__, which is applied to every point in an index space. For simple programs like this one, the index space maps directly to the elements of the array. The kernel is encapsulated in a __C++ lambda function__. The lambda function is passed a point in the index space as an array of coordinates. For this simple program, the index space coordinate is the same as the array index. The __parallel_for__ in the below program applies the lambda to the index space. The index space is defined in the first argument of the parallel_for as a 1 dimensional __range from 0 to N-1__.\n",
    "\n",
    "The __parallel_for__ is nested inside another lamba function, which is passed as an argument in the below program where we __submit to the queue__. The DPC++ runtime invokes the lambda when the accelerator connected to the queue is ready. The handler argument to the lambda allows operations inside the lambda to define the __data and dependences__ with other computation that may be executed on host or devices. You will see more of this in later modules.\n",
    "\n",
    "Finally, the program does a __q.wait()__ on the queue. The earlier submit operation queues up an operation to be performed at a later time and immmediately returns. If the host wants to see the result of the computation, it must wait for the work to complete with a wait. Sometimes the device will encounter an error. The q.wait_and_throw() is a way for the host to capture and handle the error that has happened on the device.\n",
    "\n",
    "### Lab Exercise\n",
    "Vector increment is the “hello world” of data parallel computing. A vector is an array of data elements, and the program below performs the same computation on each element of the vector by adding 1. The code below shows Simple Vector Increment DPC++ code. You will change the program to create a new vector, then add the elements in the new vector to the existing vector using DPC++.\n",
    "\n",
    "1. Select the code cell below, __follow the STEPS 1 to 6__ in the code comments to change from vector-increment to vector-add and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/simple-vector-incr.cpp\n",
    "//==============================================================\n",
    "// Copyright © 2020 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <CL/sycl.hpp>\n",
    "using namespace sycl;\n",
    "//N is set as 2 as this is just for demonstration purposes. Even if you make N bigger than 2 the program still\n",
    "//counts N as only 2 as the first 2 elements are only initialized here and the rest all becomes zero.\n",
    "static const size_t N = 2;\n",
    "\n",
    "// ############################################################\n",
    "// work\n",
    "\n",
    "void work(queue &q) {\n",
    "  std::cout << \"Device : \"\n",
    "            << q.get_device().get_info<info::device::name>()\n",
    "            << std::endl;\n",
    "  // ### Step 1 - Inspect\n",
    "  // The code presents one input buffer (vector1) for which Sycl buffer memory\n",
    "  // is allocated. The associated with vector1_accessor set to read/write gets\n",
    "  // the contents of the buffer.\n",
    "  int vector1[N] = {10, 10};\n",
    "  auto R = range(N);\n",
    "  \n",
    "  std::cout << \"Input  : \" << vector1[0] << \", \" << vector1[1] << std::endl;\n",
    "\n",
    "  // ### Step 2 - Add another input vector - vector2\n",
    "  // Uncomment the following line to add input vector2\n",
    "  //int vector2[N] = {20, 20};\n",
    "\n",
    "  // ### Step 3 - Print out for vector2\n",
    "  // Uncomment the following line\n",
    "  //std::cout << \"Input  : \" << vector2[0] << \", \" << vector2[1] << std::endl;\n",
    "  buffer vector1_buffer(vector1,R);\n",
    "\n",
    "  // ### Step 4 - Add another Sycl buffer - vector2_buffer\n",
    "  // Uncomment the following line\n",
    "  //buffer vector2_buffer(vector2,R);\n",
    "  q.submit([&](handler &h) {\n",
    "    accessor vector1_accessor (vector1_buffer,h);\n",
    "\n",
    "    // Step 5 - add an accessor for vector2_buffer\n",
    "    // Uncomment the following line to add an accessor for vector 2\n",
    "    //accessor vector2_accessor (vector2_buffer,h,read_only);\n",
    "\n",
    "    h.parallel_for<class test>(range<1>(N), [=](id<1> index) {\n",
    "      // ### Step 6 - Replace the existing vector1_accessor to accumulate\n",
    "      // vector2_accessor \n",
    "      // Comment the following line\n",
    "      vector1_accessor[index] += 1;\n",
    "\n",
    "      // Uncomment the following line\n",
    "      //vector1_accessor[index] += vector2_accessor[index];\n",
    "    });\n",
    "  });\n",
    "  q.wait();\n",
    "  host_accessor h_a(vector1_buffer,read_only);\n",
    "  std::cout << \"Output : \" << vector1[0] << \", \" << vector1[1] << std::endl;\n",
    "}\n",
    "\n",
    "// ############################################################\n",
    "// entry point for the program\n",
    "\n",
    "int main() {  \n",
    "  try {\n",
    "    queue q;\n",
    "    work(q);\n",
    "  } catch (exception e) {\n",
    "    std::cerr << \"Exception: \" << e.what() << std::endl;\n",
    "    std::terminate();\n",
    "  } catch (...) {\n",
    "    std::cerr << \"Unknown exception\" << std::endl;\n",
    "    std::terminate();\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_simple-vector-incr.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_simple-vector-incr.sh; else ./run_simple-vector-incr.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "- [simple-vector-add.cpp](src/simple-vector-add.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this module you will have learned the following:\n",
    "* How oneAPI solves the challenges of programming in a heterogeneous world \n",
    "* Take advantage of oneAPI solutions to enable your workflows\n",
    "* Use the Intel® DevCloud to test-drive oneAPI tools and libraries\n",
    "* Basics of the DPC++ language and programming model\n",
    "* Become familiarized with the use of Juypter notebooks by editing of source code in context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><body><span style=\"color:green\"><h1>Survey</h1></span></body></html>\n",
    "\n",
    "[Tell us how we did in this module with a short survey. We will use your feedback to improve the quality and impact of these learning materials. Thanks!](https://intel.az1.qualtrics.com/jfe/form/SV_6m4G7BXPNSS7FBz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><body><span style=\"color:Red\"><h1>Reset Notebook</h1></span></body></html>\n",
    "\n",
    "##### Should you be experiencing any issues with your notebook or just want to start fresh run the below cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "import ipywidgets as widgets\n",
    "button = widgets.Button(\n",
    "    description='Reset Notebook',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='This will update this notebook, overwriting any changes.',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      # \"linking function with output\"\n",
    "      with out:\n",
    "          # what happens when we press the button\n",
    "          clear_output()\n",
    "          !rsync -a --size-only /data/oneapi_workshop/oneAPI_Essentials/01_oneAPI_Intro/ ~/oneAPI_Essentials/01_oneAPI_Intro\n",
    "          print('Notebook reset -- now click reload on browser.')\n",
    "# linking button and function together using a button's method\n",
    "button.on_click(on_button_clicked)\n",
    "# displaying button and its output together\n",
    "widgets.VBox([button,out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Check out these related resources\n",
    "\n",
    "#### Intel® oneAPI Toolkit documentation\n",
    "* [Intel® oneAPI main page](https://software.intel.com/oneapi \"oneAPI main page\")\n",
    "* [Intel® oneAPI programming guide](https://software.intel.com/sites/default/files/oneAPIProgrammingGuide_3.pdf \"oneAPI programming guide\")\n",
    "* [Intel® DevCloud Signup](https://software.intel.com/en-us/devcloud/oneapi \"Intel DevCloud\")  Sign up here if you do not have an account.\n",
    "* [Intel® DevCloud Connect](https://devcloud.intel.com/datacenter/connect)  Login to the DevCloud here.\n",
    "* [Get Started with oneAPI for Linux*](https://software.intel.com/en-us/get-started-with-intel-oneapi-linux)\n",
    "* [Get Started with oneAPI for Windows*](https://software.intel.com/en-us/get-started-with-intel-oneapi-windows)\n",
    "* [Intel® oneAPI Code Samples](https://software.intel.com/en-us/articles/code-samples-for-intel-oneapibeta-toolkits)\n",
    "* [oneAPI Specification elements](https://www.oneapi.com/spec/)\n",
    "\n",
    "#### SYCL \n",
    "* [SYCL* Specification (for version 1.2.1)](https://www.khronos.org/registry/SYCL/specs/sycl-1.2.1.pdf)\n",
    "\n",
    "#### Modern C++\n",
    "* [CPPReference](https://en.cppreference.com/w/)\n",
    "* [CPlusPlus](http://www.cplusplus.com/)\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
