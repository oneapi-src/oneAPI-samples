{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd0fbd4-17fa-472b-b8af-61cf67c8e668",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Memory and Atomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671ea02-7296-4d7c-8fc2-55424a991a98",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [Local Memory Usage](#Local-Memory-Usage)\n",
    "- _Code:_ [Local Memory Type and Size](#Local-Memory-Type-and-Size)\n",
    "- [Local Accessors](#Local-Accessors)\n",
    "- [Group Barrier](#Group-Barrier)\n",
    "- _Code:_ [Matrix Multiplication without Local Memory](#Matrix-Multiplication-without-Local-Memory)\n",
    "- _Code:_ [Matrix Multiplication with Local Memory](#Matrix-Multiplication-with-Local-Memory)\n",
    "- [Atomic Operations](#Atomic-Operations)\n",
    "- _Code:_ [Atomic Operations with Buffers](#Atomic-Operations-with-Buffers)\n",
    "- _Code:_ [Atomic Operations with USM](#Atomic-Operations-with-USM)\n",
    "- _Lab Exercise:_ [Atomic Operation](#Lab-Exercise:-Atomic-Operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60efc68-b77c-4e97-993d-5f5772e59b74",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Use local memory to avoid repeated global memory access\n",
    "- Understand the usage of group barriers to synchronize all work-items\n",
    "- Use atomic operation to perform reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2a4fe-c794-43a2-8fa5-13fb9e2f0361",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Memory Usage\n",
    "\n",
    "Often work-items need to share data and communicate with each other. On one hand, all work-items in all work-groups can access global memory, so data sharing and communication can occur through global memory. However, due to its lower bandwidth and higher latency, sharing and communication through global memory is less efficient. On the other hand, work-items in a sub-group executing simultaneously in an execution unit (EU) thread can share data and communicate with each other very efficiently, but the number of work-items in a sub-group is usually small and the scope of data sharing and communication is very limited. \n",
    "\n",
    "Memory with higher bandwidth and lower latency accessible to a bigger scope of work-items is very desirable for data sharing communication among work-items. The shared local memory (SLM) in GPUs is designed for this purpose.\n",
    "\n",
    "To simplify kernel development and accelerate communication between work-items in a work-group, SYCL defines a special local memory space specifically for communication between work-items in a work-group.\n",
    "\n",
    "<img src=\"assets/localmem.png\">\n",
    "\n",
    "Each work-group may access variables in its own local memory space, but cannot access variables in another work-group’s local memory. When a work-group begins, the contents of its local memory are uninitialized, and local memory does not persist after a work-group finishes executing. Because of these properties, local memory may only be used for temporary storage while a work-group is executing.\n",
    "\n",
    "For some devices, such as for many CPU devices, local memory is a software abstraction and is implemented using the same memory subsystems as global memory. On these devices, using local memory is primarily a convenience mechanism for communication. Some compilers may use the memory space information for compiler local memory its own local memory optimizations, but otherwise using local memory for communication will not fundamentally perform better than communication via global memory on these devices.\n",
    "\n",
    "For other devices though, such as many GPU devices, there are dedicated resources for local memory, and on these devices, communicating via local memory will perform better than communicating via global memory.\n",
    "\n",
    "We can use the device query `info::device::local_mem_type` to determine whether an accelerator has dedicated resources for local memory or whether local memory is implemented as a software abstraction of global memory. \n",
    "\n",
    "We can use the device query `info::device::local_mem_size` to determine the size of local memory available for each work-group to access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2a89b-b945-489a-83b7-3c610cc24ce8",
   "metadata": {},
   "source": [
    "### Local Memory Type and Size\n",
    "\n",
    "The code below uses device query to determine the local memory size and type. Inspect code, there are no modifications necessary:\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file.\n",
    "\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a159443-842d-484c-a7ad-6de93857e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/localmem_info.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "int main() {\n",
    "  queue q;\n",
    "\n",
    "  //# Print the device info\n",
    "  std::cout << \"device name   : \" << q.get_device().get_info<info::device::name>() << \"\\n\";\n",
    "  std::cout << \"local_mem_size: \" << q.get_device().get_info<info::device::local_mem_size>() << \"\\n\";\n",
    "\n",
    "  auto local_mem_type = q.get_device().get_info<info::device::local_mem_type>();\n",
    "  if(local_mem_type == info::local_mem_type::local) \n",
    "    std::cout << \"local_mem_type: info::local_mem_type::local\" << \"\\n\";\n",
    "  else if(local_mem_type == info::local_mem_type::global) \n",
    "    std::cout << \"local_mem_type: info::local_mem_type::global\" << \"\\n\";\n",
    "  else if(local_mem_type == info::local_mem_type::none) \n",
    "    std::cout << \"local_mem_type: info::local_mem_type::none\" << \"\\n\";\n",
    " \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bbd72-d910-43ce-8ef7-26967fbbee12",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a447ad5-a382-4d49-b922-2433f8ee98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_localmem_info.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_localmem_info.sh; else ./run_localmem_info.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616663ca-ed92-44ac-a332-12b2596519ea",
   "metadata": {},
   "source": [
    "### Local Accessors\n",
    "\n",
    "A Local Accessor is used to declare local memory for use in an ND-range kernel. Like other accessor objects, a local accessor is constructed within a command group handler.\n",
    "\n",
    "A local accessor is created by specifying a type and a range describing the number of elements of that type. Like other accessors, local accessors may be one-dimensional, two-dimensional, or three dimensional.\n",
    "\n",
    "Below is an example of defining local_accessor `localmem` with type _int_ and _one-dimension_ data\n",
    "\n",
    "```cpp\n",
    "local_accessor<int, 1> localmem(N, h);\n",
    "```\n",
    "\n",
    "Below is an example of defining local_accessor `localmem` with type _float_ and _two-dimension_ data\n",
    "\n",
    "```cpp\n",
    "local_accessor<float, 2> localmem(range<2>(N, N), h);\n",
    "```\n",
    "\n",
    "The local accessor from one work-group can be accessed by all work-items within the work-group. Each work-group can have its own local accessor, work-item from another work-group cannot access this local accessor.\n",
    "\n",
    "### Group Barrier\n",
    "\n",
    "When local accessor data is shared, work-group barriers are often required for work-item synchronization.\n",
    "\n",
    "The `group_barrier` function synchronizes how each work-item views the state of memory. This type of synchronization operation is known as enforcing memory consistency or fencing memory. It ensures that the results of memory operations performed before the barrier are visible to other work-items after the\n",
    "barrier.\n",
    "\n",
    "A `group_barrier` is usually required right after a local accessor is modified by a work-item so that it is synchronized for all work-items before the local accessor can be accessed.\n",
    "\n",
    "Below is an example of how a `group_barrier` function is defined to synchronize across all work-items within the work-group:\n",
    "\n",
    "```cpp\n",
    "group_barrier(item.get_group());\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ca6e1-b25d-441b-adbf-2314568f456f",
   "metadata": {},
   "source": [
    "## Local Memory Usage Example\n",
    "\n",
    "When a computation requires repeated access to global memory data, using a local memory to load data from global memory and then accessing subsequent repeated access from local memory can be more performant.\n",
    "\n",
    "One such example is matrix multiplication, multiplying two 8x8 matrices requires each of 8 rows to multiply with 8 columns, every row and column is accessed 8 times from global memory. \n",
    "\n",
    "<img src=\"assets/naive.PNG\">\n",
    "\n",
    "Using local memory for matrix multiplication can be more performant. Let's look at matrix multiplication without using local memory and using local memory to understand usage of `local accessor` and `group_barrier` concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43b38e-2c4c-4a24-afe7-617d0cf1a1ae",
   "metadata": {},
   "source": [
    "### Matrix Multiplication without Local Memory\n",
    "\n",
    "The code below demonstrates basic matrix multiplication example. Inspect code, there are no modifications necessary:\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file.\n",
    "\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be897d90-2d92-464b-be20-26bdf6cde6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/matrixmul_16x16.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iomanip>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "int main() {\n",
    "    \n",
    "    size_t N = 16;\n",
    "    std::cout << \"MATRIX_SIZE    : \" << N << \"x\" << N << std::endl;\n",
    "\n",
    "    //# Define vectors for matrices\n",
    "    std::vector<float> matrix_a(N*N);\n",
    "    std::vector<float> matrix_b(N*N);\n",
    "    std::vector<float> matrix_c(N*N);\n",
    "    std::vector<float> matrix_d(N*N);\n",
    "    \n",
    "    //# Initialize matrices with values\n",
    "    float v1 = 2.f;\n",
    "    float v2 = 3.f;\n",
    "    for (int i=0; i<N; i++)\n",
    "        for (int j=0; j<N; j++){\n",
    "            matrix_a[i*N+j] = v1++;\n",
    "            matrix_b[i*N+j] = v2++;\n",
    "            matrix_c[i*N+j] = 0.f;\n",
    "            matrix_d[i*N+j] = 0.f;\n",
    "    }\n",
    "    \n",
    "    //# Define queue with default device for offloading computation\n",
    "    queue q;\n",
    "    std::cout << \"Offload Device : \" << q.get_device().get_info<info::device::name>() << std::endl;\n",
    "    \n",
    "    //# Create buffers for matrices\n",
    "    buffer a(matrix_a);\n",
    "    buffer b(matrix_b);\n",
    "    buffer c(matrix_c);\n",
    "\n",
    "    //# Submit command groups to execute on device\n",
    "    q.submit([&](handler &h){\n",
    "        //# Create accessors to copy buffers to the device\n",
    "        accessor A(a, h, read_only);\n",
    "        accessor B(b, h, read_only);\n",
    "        accessor C(c, h, write_only);\n",
    "\n",
    "        //# Define size for ND-range and work-group size\n",
    "        range<2> global_size(N,N);\n",
    "        range<2> work_group_size(N,N);\n",
    "\n",
    "        //# Parallel Compute Matrix Multiplication\n",
    "        h.parallel_for(nd_range<2>{global_size, work_group_size}, [=](nd_item<2> item){\n",
    "            const int i = item.get_global_id(0);\n",
    "            const int j = item.get_global_id(1);\n",
    "\n",
    "            //# matrix multiplication computation from local memory\n",
    "            float temp = 0.f;\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                temp += A[i*N+k] * B[k*N+j];\n",
    "            }\n",
    "            C[i*N+j] = temp;\n",
    "        });\n",
    "    });\n",
    "    host_accessor ha(c, read_only);\n",
    "    \n",
    "    //# Print Output and Verification\n",
    "    auto FAIL = 0;\n",
    "    for (int i=0; i<N; i++){\n",
    "        for (int j=0; j<N; j++){\n",
    "            for(int k=0; k<N; k++){\n",
    "                matrix_d[i*N+j] += matrix_a[i*N+k] * matrix_b[k*N+j];\n",
    "            }\n",
    "            if(matrix_d[i*N+j] != matrix_c[i*N+j]) FAIL = 1;\n",
    "            std::cout << std::setw(6) << matrix_c[i*N+j] << \" \";\n",
    "        }\n",
    "        std::cout << \"\\n\";\n",
    "    }\n",
    "    if(FAIL == 1) std::cout << \"FAIL\\n\"; else std::cout << \"PASS\\n\";\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6d041-c1d5-4039-a673-80c46a086a62",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2de30c-7b28-4f69-8c5d-7afd68347159",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_matrixmul_16x16.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_matrixmul_16x16.sh; else ./run_matrixmul_16x16.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3789b4-e3fc-4cd4-8ac2-77b8c7dbae05",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Matrix Multiplication with Local Memory\n",
    "\n",
    "The code below demonstrates matrix multiplication example making use of local memory. Inspect code, there are no modifications necessary:\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file.\n",
    "\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5d945-4932-465f-b3db-f9d5280d8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/matrixmul_16x16_localmem.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iomanip>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "int main() {\n",
    "    \n",
    "    size_t N = 16;\n",
    "    std::cout << \"MATRIX_SIZE    : \" << N << \"x\" << N << std::endl;\n",
    "\n",
    "    //# Define vectors for matrices\n",
    "    std::vector<float> matrix_a(N*N);\n",
    "    std::vector<float> matrix_b(N*N);\n",
    "    std::vector<float> matrix_c(N*N);\n",
    "    std::vector<float> matrix_d(N*N);\n",
    "    \n",
    "    //# Initialize matrices with values\n",
    "    float v1 = 2.f;\n",
    "    float v2 = 3.f;\n",
    "    for (int i=0; i<N; i++)\n",
    "        for (int j=0; j<N; j++){\n",
    "            matrix_a[i*N+j] = v1++;\n",
    "            matrix_b[i*N+j] = v2++;\n",
    "            matrix_c[i*N+j] = 0.f;\n",
    "            matrix_d[i*N+j] = 0.f;\n",
    "    }\n",
    "    \n",
    "    //# Define queue with default device for offloading computation\n",
    "    queue q;\n",
    "    std::cout << \"Offload Device : \" << q.get_device().get_info<info::device::name>() << std::endl;\n",
    "    \n",
    "    //# Create buffers for matrices\n",
    "    buffer a(matrix_a);\n",
    "    buffer b(matrix_b);\n",
    "    buffer c(matrix_c);\n",
    "\n",
    "    //# Submit command groups to execute on device\n",
    "    q.submit([&](handler &h){\n",
    "        //# Create accessors to copy buffers to the device\n",
    "        accessor A(a, h, read_only);\n",
    "        accessor B(b, h, read_only);\n",
    "        accessor C(c, h, write_only);\n",
    "\n",
    "        //# Define size for ND-range and work-group size\n",
    "        range<2> global_size(N,N);\n",
    "        range<2> work_group_size(N,N);\n",
    "\n",
    "        //# Create local accessors\n",
    "        local_accessor<float, 2> A_local(range<2>(N, N), h);\n",
    "        local_accessor<float, 2> B_local(range<2>(N, N), h);\n",
    "\n",
    "        //# Parallel Compute Matrix Multiplication\n",
    "        h.parallel_for(nd_range<2>{global_size, work_group_size}, [=](nd_item<2> item){\n",
    "            const int i = item.get_global_id(0);\n",
    "            const int j = item.get_global_id(1);\n",
    "            const int x = item.get_local_id(0);\n",
    "            const int y = item.get_local_id(1);\n",
    "\n",
    "            //# copy from global to local memory\n",
    "            A_local[x][y] = A[i * N + j];\n",
    "            B_local[x][y] = B[i * N + j];\n",
    "\n",
    "            //# barrier to sychronize local memory copy across all work items\n",
    "            group_barrier(item.get_group());\n",
    "\n",
    "            //# matrix multiplication computation from local memory\n",
    "            float temp = 0.f;\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                temp += A_local[x][k] * B_local[k][y];\n",
    "            }\n",
    "            C[i*N+j] = temp;\n",
    "        });\n",
    "    });\n",
    "    host_accessor ha(c, read_only);\n",
    "    \n",
    "    //# Print Output and Verification\n",
    "    auto FAIL = 0;\n",
    "    for (int i=0; i<N; i++){\n",
    "        for (int j=0; j<N; j++){\n",
    "            for(int k=0; k<N; k++){\n",
    "                matrix_d[i*N+j] += matrix_a[i*N+k] * matrix_b[k*N+j];\n",
    "            }\n",
    "            if(matrix_d[i*N+j] != matrix_c[i*N+j]) FAIL = 1;\n",
    "            std::cout << std::setw(6) << matrix_c[i*N+j] << \" \";\n",
    "        }\n",
    "        std::cout << \"\\n\";\n",
    "    }\n",
    "    if(FAIL == 1) std::cout << \"FAIL\\n\"; else std::cout << \"PASS\\n\";\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d4525-bd6a-483e-80fc-d5414ea6beef",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb4c03-3538-4571-9522-71433ed41015",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_matrixmul_16x16_localmem.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_matrixmul_16x16_localmem.sh; else ./run_matrixmul_16x16_localmem.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30865b8d-3304-4083-814c-0cbf5147e9e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Atomic Operations\n",
    "\n",
    "Atomic operations enable __concurrent access to a memory location without introducing a data race__. When multiple atomic operations access the same memory, they are guaranteed not to overlap. \n",
    "\n",
    "To understand why atomic operations are necessary, let look at few kernel examples to perform reduction, addition of N number of elements:\n",
    "\n",
    "#### Serial Computation with single_task\n",
    "A simple way to perform reduction is by using a for-loop to add all items in a single_task kernel submission as show below, but it does not take advantage of parallelism in hardware.\n",
    "```cpp\n",
    "     q.single_task([=](){\n",
    "       for(int i=0; i<N; i++){\n",
    "         sum[0] += data[i];\n",
    "       }\n",
    "     });\n",
    "```\n",
    "\n",
    "#### Parallel Computation with parallel_for may encounter race conditions\n",
    "Using parallel_for for kernel submission will enable multiple work-items to execute concurrently but multiple work-item may try to update the same output variable causing __race conditions__.\n",
    "```cpp\n",
    "      q.parallel_for(N, [=](auto i) {\n",
    "        sum[0] += data[i];\n",
    "      });\n",
    "```\n",
    "\n",
    "\n",
    "#### Parallel Computation with atomic operation\n",
    "The code snippet below show how to avoid race conditions when multiple work-items are trying to update the same memory location using atomic operations\n",
    "```cpp\n",
    "      q.parallel_for(N, [=](auto i) {\n",
    "        auto atomic_var = atomic_ref<int, memory_order::relaxed, memory_scope::device, access::address_space::global_space>(sum[0]);\n",
    "\n",
    "        atomic_var.fetch_add(data[i]);\n",
    "      });\n",
    "```\n",
    "\n",
    "### atomic_ref class\n",
    "The `atomic_ref` class above will make sure that the referenced variable will only be accessed atomically for the lifetime of the reference. It also specifies the _data type_, _memory order_ and _memory scope_.\n",
    "\n",
    "```cpp\n",
    "  auto atomic_var = atomic_ref<int, memory_order::relaxed, memory_scope::device, access::address_space::global_space>(result[0]);\n",
    "```\n",
    "\n",
    "\n",
    "#### memory_order\n",
    "By providing the compiler with information about our desired memory order, we can prevent re-ordering optimizations that are incompatible with the intended behavior of our applications.\n",
    "- `memory_order::relaxed`: Read and write operations can be re-ordered before or after the operation with no restrictions. There are no ordering guarantees.\n",
    "- `memory_order::acquire`: Read and write operations appearing after the operation in the program must occur after it.\n",
    "- `memory_order::release`:\n",
    "Read and write operations appearing before the operation in the program must occur before it , and preceding write operations are guaranteed to be visible to other program instances which have been synchronized by a corresponding acquire operation.\n",
    "- `memory_order::acq_rel`: \n",
    "The operation acts as both an acquire and a release. Read and write operations cannot be re-ordered around the operation, and preceding writes must be made visible as previously described for _memory_order::release_.\n",
    "- `memory_order::seq_cst`: \n",
    "The operation acts as an acquire, release, or both depending on whether it is a read, write, or read-modify-write operation, respectively. All operations with this memory order are observed in a sequentially consistent order.\n",
    "\n",
    "#### memory_scope\n",
    "- `memory_scope::work_item`: The memory ordering constraint applies only to the calling work-item. This scope is only useful for image operations, as all other operations within a work-item are already guaranteed to execute in program order.\n",
    "- `memory_scope::work_group`: The memory ordering constraint applies only to work-items in the same work-group as the calling work-item.\n",
    "- `memory_scope::sub_group`: The memory ordering constraint applies only to work-items in the same sub-group as the calling work-item.\n",
    "- `memory_scope::device`: The memory ordering constraint applies only to work-items executing on the same device as the calling work-item.\n",
    "- `memory_scope::system`:  The memory ordering constraint applies to all work-items in the system.\n",
    "\n",
    "### atomic operations\n",
    "\n",
    "Atomic references to objects of integral and floating-point types extend the set of available atomic operations to include arithmetic operations\n",
    "\n",
    "```cpp\n",
    "        // integer and floating point\n",
    "        atomic_var += data[i];         // addition\n",
    "        atomic_var.fetch_add(data[i]); // addition\n",
    "        atomic_var -= data[i];         // subtraction\n",
    "        atomic_var.fetch_sub(data[i]); // subtraction\n",
    "        atomic_var.fetch_max(data[i]); // maximum\n",
    "        atomic_var.fetch_min(data[i]); // minimum\n",
    "        // integer only\n",
    "        atomic_var.fetch_and(data[i]); // bitwise AND\n",
    "        atomic_var.fetch_or(data[i]);  // bitwise OR\n",
    "        atomic_var.fetch_xor(data[i]); // bitwise XOR\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed385f-9808-4012-987e-7343a35e8c2b",
   "metadata": {},
   "source": [
    "### Atomic Operations with Buffers\n",
    "\n",
    "The code below uses atomic operation to perform reduction with buffers memory model. Inspect code, there are no modifications necessary.\n",
    "\n",
    "_[Note that using atomics to do reduction operation is not best approach, but it a easy example to demonstrate atomic operation functionality, for better performance with reduction can be achieved using SYCL reduction kernels]_\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file.\n",
    "\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e7e6a-ddd4-4fe2-9fb5-3b7c1e652d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_atomics_buffer.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "static constexpr size_t N = 1024; // global size\n",
    "\n",
    "int main() {\n",
    "  queue q;\n",
    "  std::cout << \"Device : \" << q.get_device().get_info<info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N);\n",
    "  for (int i = 0; i < N; i++) data[i] = i;\n",
    "  int sum = 0;\n",
    "  {\n",
    "    //# create buffers for data and sum\n",
    "    buffer buf_data(data);\n",
    "    buffer buf_sum(&sum, range(1));\n",
    "\n",
    "    //# Reduction Kernel using atomics \n",
    "    q.submit([&](auto &h) {\n",
    "      accessor data_acc(buf_data, h, sycl::read_only);\n",
    "      accessor sum_acc(buf_sum, h);\n",
    "\n",
    "      h.parallel_for(N, [=](auto i) {\n",
    "        auto sum_atomic = atomic_ref<int, \n",
    "          memory_order::relaxed, \n",
    "          memory_scope::device, \n",
    "          access::address_space::global_space>(sum_acc[0]);\n",
    "        sum_atomic += data_acc[i];\n",
    "      });\n",
    "    });\n",
    "  }\n",
    "  std::cout << \"Sum = \" << sum << \"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd05768-3154-4479-a58d-1075f58f0083",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731ed45-6743-4ec1-bd7b-128477fa542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_reduction_atomics_buffer.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_reduction_atomics_buffer.sh; else ./run_reduction_atomics_buffer.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1775bb0-7e49-4da2-8777-548da487bc1b",
   "metadata": {},
   "source": [
    "### Atomic Operations with USM\n",
    "\n",
    "The code below uses atomic operation to perform reduction with Unified Shared memory. Inspect code, there are no modifications necessary.\n",
    "\n",
    "_[Note that using atomics to do reduction operation is not best approach, but it a easy example to demonstrate atomic operation functionality, for better performance with reduction can be achieved using SYCL reduction kernels]_\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file.\n",
    "\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0639250-e30c-4744-b360-08fb04ee116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_atomics_usm.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "static constexpr size_t N = 1024; // global size\n",
    "\n",
    "int main() {\n",
    "  queue q;\n",
    "  std::cout << \"Device : \" << q.get_device().get_info<info::device::name>() << \"\\n\";\n",
    "\n",
    "  auto data = malloc_shared<int>(N, q);\n",
    "  for (int i = 0; i < N; i++) data[i] = i;\n",
    "  auto sum = malloc_shared<int>(1, q);\n",
    "  sum[0] = 0;\n",
    "\n",
    "  //# Reduction Kernel using atomics \n",
    "  q.parallel_for(N, [=](auto i) {\n",
    "    auto sum_atomic = atomic_ref<int, \n",
    "      memory_order::relaxed, \n",
    "      memory_scope::device, \n",
    "      access::address_space::global_space>(sum[0]);\n",
    "    sum_atomic += data[i];\n",
    "  }).wait();\n",
    "\n",
    "  std::cout << \"Sum = \" << sum[0] << \"\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba49aab-bb7c-46ff-9f66-9c1b2e179988",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b991f0-bd9f-487e-aabe-6e001e43263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_reduction_atomics_usm.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_reduction_atomics_usm.sh; else ./run_reduction_atomics_usm.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9224377-ff8c-45fc-afdd-17c0ae8bf9d0",
   "metadata": {},
   "source": [
    "## Lab Exercise: Atomic Operations\n",
    "\n",
    "Complete the coding exercise below using Atomic Operations:\n",
    "- The code has an array `data` of size `N=1024` elements initialized\n",
    "- We will offload kernel task to find the minimum and maximum values from the `data` array using atomic operations\n",
    "- Create atomic reference for minimum and maximum variables\n",
    "- Create atomic operation in kernel to find minimum and maximum\n",
    "- On the host, compute mid-range, which is average of min and max values\n",
    "\n",
    "1. Edit the code cell below by following the steps and then click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af66e69-45bf-442f-88c7-1478b6365631",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/atomics_lab.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "static constexpr size_t N = 1024; // global size\n",
    "\n",
    "int main() {\n",
    "  queue q;\n",
    "  std::cout << \"Device : \" << q.get_device().get_info<info::device::name>() << \"\\n\";\n",
    "\n",
    "  auto data = malloc_shared<int>(N, q);\n",
    "  for (int i = 0; i < N; i++) data[i] = i;\n",
    "  auto min = malloc_shared<int>(1, q);\n",
    "  auto max = malloc_shared<int>(1, q);\n",
    "  min[0] = 0;\n",
    "  max[0] = 0;\n",
    "\n",
    "  //# Reduction Kernel using atomics \n",
    "  q.parallel_for(N, [=](auto i) {\n",
    "    //# STEP 1: create atomic reference for min and max\n",
    "\n",
    "    //# YOUR CODE GOES HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    //# STEP 2: add atomic operation for min and max computation  \n",
    "\n",
    "    //# YOUR CODE GOES HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "  }).wait();\n",
    "\n",
    "  auto mid = 0.0;\n",
    "  //# STEP 3: Compute mid-range using the min and max \n",
    "\n",
    "  //# YOUR CODE GOES HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "  std::cout << \"Minimum   = \" << min[0] << \"\\n\";\n",
    "  std::cout << \"Maximum   = \" << max[0] << \"\\n\";\n",
    "  std::cout << \"Mid-Range = \" << mid << \"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dd8c8-3eca-4faf-a271-e27b708a242a",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f9117-3ae5-4a6d-845e-c029f7acce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_atomics_lab.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_atomics_lab.sh; else ./run_atomics_lab.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0bb06-3795-4ff8-81b1-9ede6bd6c510",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Summary\n",
    "\n",
    "In this module you learned:\n",
    "* How to setup and use Shared Local Memory in the device\n",
    "* How to use atomic operation when using buffers or USM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
