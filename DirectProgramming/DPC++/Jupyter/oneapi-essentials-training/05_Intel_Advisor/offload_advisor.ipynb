{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel® Advisor - Offload Advisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sections demonstrate how to collect and generate a roofline report using Intel® Advisor, below we will examine our \"offload\" report.\n",
    "\n",
    "##### Sections\n",
    "- [What is Offload Advisor?](#What-is-Offload-Advisor?)\n",
    "- _Analysis:_ [Offload Advisor Analysis](#Offload-Advisor-Analysis)\n",
    "- [Analysis of Top Offload Regions](#Analysis-of-Top-Offload-Regions)\n",
    "- [What Kernels Should Not Be Offloaded?](#What-Kernels-Should-Not-Be-Offloaded?)\n",
    "- [Command line options](#Command-line-options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "The goal of this notebook is to show how Intel® Advisor can help deciding what part of the code should or should not be offloaded on the GPU. At the end of this, you will be able:\n",
    "<ul>\n",
    "    <li>To run Offload Advisor and generate a HTML report</li>\n",
    "    <li>To read and understand the metrics in the report</li>\n",
    "    <li>To get a performance estimation of your application on the target hardware</li>\n",
    "    <li>To decide which loops are good candidate for offload</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Offload Advisor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offload Advisor allows you to collect performance predictor data in addition to the profiling capabilities of Intel® Advisor. View output files containing metrics and performance data such as total speedup, fraction of code accelerated, number of loops and functions offloaded, and a call tree showing offloadable and accelerated regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offload Advisor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below HTML report is <span style=\"color:blue\"><b>live</b></span>, click navigation to see output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intel Advisor Offload report](assets/offload.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Report\n",
    "Select the cell below and click run ▶ to view the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('/bin/echo $(whoami) is running DPCPP_Essentials Module5 -- Intel Advisor - 1 of 2 offload.html')\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='assets/offload.html', width=1024, height=1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Intel® Advisor to increase performance\n",
    "__Intel® Advisor__ is recommended to  __Optimize__ the design for __vectorization and memory__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__\n",
    "\n",
    "__offload Advisor__ can help determine what kernels should be offloaded and can predict the speedup that can be expected.\n",
    "\n",
    "Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA__ to __Data Parallel C++__. Existing __Fortran__ applications can use a __directive style based on OpenMP__. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__.\n",
    "\n",
    "Once you wirte the SYCL code,  __GPU roofline analyis__ helps to develop an optimization strategy and see potential bottlenecks relative to target maximums.\n",
    "\n",
    "Finally the GPU analysis using VTune can help optimize for the target.\n",
    "\n",
    "<img src=\"assets/a1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Advisor - Offload Advisor: Find code that can be profitably offloaded\n",
    "\n",
    "From the below fugure we can clearly observe that the the workload was accelerated by 3.5x. You can see in program metrics that the original workload ran in 18.51s and the accelerated workload ran in 5.45s\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"assets/a4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offload Advisor: Will Offload Increase Performance?\n",
    "\n",
    "From the below figure we can clearly observe the good candidates for offloading and the bad candidates to offload. You can also observe what your workload is bounded by.\n",
    "\n",
    "\n",
    "<img src=\"assets/a5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Top Offload Regions\n",
    "\n",
    "Provides a detailed description of each loop interesting for offload. You can view the Timings (total time, time on the accelerator, speedup), the Offload metrics like the offload taxe and the data transfers, Memory traffic (DRAM, L3, L2, L1) and the trip count. It also highlighst which part of the code should run on the accelerator.\n",
    "  \n",
    "  <img src=\"assets/a6.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Kernels Should Not Be Offloaded?\n",
    "\n",
    "Below explains why Intel Advisor does not recommend a given loop for offload. The possible reason can be dependency issues, that loops are not profitable, or the total time is too small.\n",
    "  \n",
    "  <img src=\"assets/a7.png\">\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Acceleration on Different GPUs\n",
    "\n",
    "Below compares acceleration on Gen9 and Gen11. You can observe from the below picture that its not efficient to offload on Gen 9\n",
    "whereas in Gen11 there is one offload with 98% of code accelerated and by 1.6x.\n",
    "\n",
    "  \n",
    "  <img src=\"assets/a8.png\">\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is the Workload Bounded By?\n",
    "\n",
    "The performance will ultimately have an upper bound based on your hardware’s limitations. There are several limitations that Offload Advisor can indicate but they generally  come down to compute, memory and data transfer. Knowing what your application is bounded by is critical to developing an optimization strategy. In the below example 95% of workload bounded by L3 bandwidth but you may have several bottlenecks.\n",
    "\n",
    "  \n",
    "  <img src=\"assets/a9.png\">\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Tree\n",
    "\n",
    "The program tree offers another view of the proportion of code that can be offloaded to the accelerator\n",
    "\n",
    "![image](assets/programtree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command line options\n",
    "\n",
    "\n",
    "\n",
    "The application runs on a CPU and is actually need not be threaded. For Intel® Offload Advisor, it doesn't matter if your code is already threaded. Advisor will run several analyses on your application to extract several metric such as the number of operations, the number of memory transfers, data dependencies and many more.\n",
    "Remember that our goal here is to decide if some of our loops are good candidates for offload. In this section, we will generate the report assuming that we want to offload our computations on a Gen Graphic (gen9) which is the hardware available on DevCloud.\n",
    "Keep in mind that if you want Advisor to extract as much information as possible, you need to compile your application with debug information (-g with intel compilers).\n",
    "\n",
    "The easiest way to run Offload Advisor is to use the batch mode that consists in running 2 scripts available is the folder $APM ($APM is available when Advisor is sourced).\n",
    "<ul>\n",
    "    <li>collect.py: Used to collect data such as timing, flops, tripcounts and many more</li>\n",
    "    <li>analyze.py: Creating the report</li>\n",
    "</ul>\n",
    "\n",
    "To be more specific, collect.py runs the following analyses:\n",
    "<ul>\n",
    "    <li>survey: Timing your application functions and loops, reading compiler diagnostics</li>\n",
    "    <li>tripcount: With flops and cache simulation to count the number of iterations in the loops as well as the number of operations and memory transfers</li>\n",
    "    <li>dependency: Check if you have data dependency in your loops, preventing it to be good candidates for offloading or vectorization</li>\n",
    "</ul>\n",
    "\n",
    "Offload Advisor is currently run from the command-line as below. Once the run is complete you can view the generated report.html.\n",
    "\n",
    "* Clone official GitHub samples repository\n",
    "     git clone https://github.com/oneapi-src/oneAPI-samples.git\n",
    "        \n",
    "* Go into Project directory to the matrix multiply advisor sample \n",
    "\n",
    "    ``cd oneAPI-samples/Tools/Advisor/matrix_multiply_advisor/``\n",
    "    \n",
    "* Build the application and generate the matrix multiplication binary\n",
    "\n",
    "    ``cmake .``    \n",
    "    ``make``\n",
    "\n",
    "```\n",
    "\n",
    "advixe-python $APM/collect.py advisor_project --config gen9 -- ./matrix.dpcpp\n",
    "advixe-python $APM/analyze.py advisor_project --config gen9 --out-dir ./analyze\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile advisor_offload.sh\n",
    "#!/bin/bash\n",
    "\n",
    "advixe-python $APM/collect.py advisor_project --config gen9 -- ./matrix.dpcpp\n",
    "advixe-python $APM/analyze.py advisor_project --config gen9 --out-dir ./analyze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the HTML report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to generate our HTML report for offloading on gen9. This report will show us:\n",
    "<ul>\n",
    "    <li>What is the expected speedup on Gen9</li>\n",
    "    <li>What will most likely be our bottleneck on Gen9</li>\n",
    "    <li>What are the good candidates for offload</li>\n",
    "    <li>What are the loops that should not be offloaded</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offload Advisor Output Overview\n",
    "\n",
    "<span style=\"color:blue\">report.html</span>: Main report in HTML format\n",
    "\n",
    "<span style=\"color:blue\">report.csv</span> and <span style=\"color:blue\">whole_app_metric.csv</span>: Comma-separated CSV files\n",
    "\n",
    "<span style=\"color:blue\">program_tree.dot:</span> A graphical representation of the call tree showing the offloadable and accelerated regions\n",
    "\n",
    "<span style=\"color:blue\">program_tree.pdf:</span> A graphical representation of the call tree generated if the DOT\\(GraphViz*) utility is installed and a 1:1 conversion from the <span style=\"color:blue\">program_tree.dot</span> file\n",
    "\n",
    "<span style=\"color:blue\">JSON</span> and <span style=\"color:blue\">LOG</span> files that contain data used to generate the HTML report and logs, primarily used for debugging and reporting bugs and issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "  * Ran the Offload Advisor report.\n",
    "  * Analyzed various outputs.\n",
    "  * Learned about additional command line options and how to speed up collection time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html><body><span style=\"color:green\"><h1>Survey</h1></span></body></html>\n",
    "\n",
    "[We would appreciate any feedback you’d care to give, so that we can improve the overall training quality and experience. Thanks! ](https://intel.az1.qualtrics.com/jfe/form/SV_0OZVTLvFGI2e0Id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue to Roofline Analysis\n",
    "[Roofline Analysis](roofline_analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
