{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3b02b3-c95f-4171-851a-4c72c4513756",
   "metadata": {},
   "source": [
    "# SYCL Multi-GPU Programming\n",
    "\n",
    "This Module covers Multi-GPU SYCL Programming and Optimization. It also covers `ONEAPI_DEVICE_SELECTOR` environment variable to filter devices available for SYCL kernel offloading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cae3cd-5944-4da7-848e-53981a63344e",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [Check Available Devices](#Check-Available-Devices)\n",
    "  - [sycl-ls](#sycl-ls)\n",
    "  - [ONEAPI_DEVICE_SELECTOR Environment Variable](#ONEAPI_DEVICE_SELECTOR-Environment-Variable)\n",
    "- [Multi-GPU Device Selection](#Multi-GPU-Device-Selection)\n",
    "  - _Code:_ [Single GPU](#Single-GPU)\n",
    "  - _Code:_ [Multiple GPUs](#Multiple-GPUs)\n",
    "- [Multi-GPU Kernel Submission](#Multi-GPU-Kernel-Submission)\n",
    "- [GPU to GPU Memory Copy](#GPU-to-GPU-Memory-Copy)\n",
    "- [Split computation to multiple GPUs](#Split-computation-to-multiple-GPUs)\n",
    "- [Optimizing Multi-GPU Offload](#Optimizing-Multi-GPU-Offload)\n",
    "- [Filter offload devices with ONEAPI_DEVICE_SELECTOR](#Filter-offload-devices-with-ONEAPI_DEVICE_SELECTOR)\n",
    "- [Tips for Multi-GPU Programming](#Tips-for-Multi-GPU-Programming)\n",
    "  - [Filter by CPU](#Filter-by-CPU)\n",
    "  - [Filter by GPU OpenCL Backend](#Filter-by-GPU-OpenCL-Backend)\n",
    "  - [Filter by GPU Level Zero Backend](#Filter-by-GPU-Level-Zero-Backend)\n",
    "  - [Filter number of GPUs available for offload](#Filter-number-of-GPUs-available-for-offload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403734c4-89e0-4572-9a5b-703acf2e4b18",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Use __sycl-ls__ command line tool to find all available offload devices.\n",
    "* Use __ONEAPI_DEVICE_SELECTOR__ environment variable to enable or disable devices available for offload.\n",
    "* Write SYCL code to find all GPU devices in a system and submit kernels to all concurrently.\n",
    "* Understand the __memory copy latency__ differences between Host to GPU and GPU to GPU.\n",
    "* __Optimize__ Multi-GPU offload SYCL code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5be99c-d57c-4d6a-aab5-148fdcb806e7",
   "metadata": {},
   "source": [
    "## Check Available Devices\n",
    "\n",
    "You can check for available devices that a SYCL kernel can be offloaded to, using `sycl-ls` command line tool and you can filter out the devices that is available for SYCL kernel to offload using the `ONEAPI_DEVICE_SELECTOR` environment variable.\n",
    "\n",
    "### sycl-ls\n",
    "\n",
    "`sycl-ls` command line tool can be used to list all the enabled SYCL enabled devices.\n",
    "\n",
    "Run the following command to check all devices available by default for SYCL kernel offload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7de45-0a83-4fe9-a5dd-03d70ff03611",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sycl-ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0304e73-6cc5-402a-9d9b-a63784c46be4",
   "metadata": {},
   "source": [
    "### ONEAPI_DEVICE_SELECTOR Environment Variable\n",
    "\n",
    "This device selection environment variable can be used to limit the choice of devices available when the SYCL-using application is run. Useful for limiting devices to a certain type (like GPUs or accelerators) or backends (like Level Zero or OpenCL).\n",
    "\n",
    "With no environment variables set to say otherwise, all platforms and devices presently on the machine are available. The default choice will be one of these devices, usually preferring a Level Zero GPU device, if available. The `ONEAPI_DEVICE_SELECTOR` can be used to limit that choice of devices, and to expose GPU sub-devices or sub-sub-devices as individual devices.\n",
    "\n",
    "`ONEAPI_DEVICE_SELECTOR=<backend>:<device>`\n",
    "\n",
    "\n",
    "Try these examples to limit certain devices, followed by `sycl-ls` command:\n",
    "```\n",
    "export ONEAPI_DEVICE_SELECTOR=opencl:cpu\n",
    "export ONEAPI_DEVICE_SELECTOR=opencl:gpu\n",
    "export ONEAPI_DEVICE_SELECTOR=opencl:*\n",
    "export ONEAPI_DEVICE_SELECTOR=level_zero:gpu\n",
    "export ONEAPI_DEVICE_SELECTOR=*:gpu\n",
    "export ONEAPI_DEVICE_SELECTOR=*:*\n",
    "```\n",
    "\n",
    "To reset and enable all devices:\n",
    "```\n",
    "unset ONEAPI_DEVICE_SELECTOR\n",
    "```\n",
    "\n",
    "Try the following 3 commands to print currently enables device and then `unset` and try again, you can also try these commands on a terminal directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84951475-fe82-4b5f-b9b8-a673ea0cf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ONEAPI_DEVICE_SELECTOR=opencl:cpu; sycl-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36040a2a-1d95-4af4-b954-c2eae3aa63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ONEAPI_DEVICE_SELECTOR=opencl:gpu; sycl-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c62892-50bb-43d4-a0a1-0285cfe20db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! unset ONEAPI_DEVICE_SELECTOR; sycl-ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f158c7-d1fc-48da-8430-f4898e9af80f",
   "metadata": {},
   "source": [
    "## Multi-GPU Device Selection\n",
    "\n",
    "To offload a SYCL kernel on specific device, we can either limit available devices using `ONEAPI_DEVICE_SELECTOR` environment variable or you can limit in SYCL code using `sycl::queue` device selector.\n",
    "\n",
    "### Single GPU\n",
    "\n",
    "To submit job to a single GPU, we create `sycl::queue` with `sycl::gpu_selector_v` device selector as shown below:\n",
    "\n",
    "```cpp\n",
    "sycl::queue q(sycl::gpu_selector_v);\n",
    "```\n",
    "\n",
    "The SYCL code below shows GPU device selection: Inspect code, there are no modifications necessary:\r\n",
    "\r\n",
    "Inspect the code cell below and click run ▶ to save the code to file\r\n",
    "Next run ▶ the cell in the Build and Run section below the code to compile and execute the code.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc7f6e-17d7-438c-9a66-e5a78c111170",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/single_gpu.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  // Create a device queue with device selector\n",
    "  sycl::queue q(sycl::gpu_selector_v);\n",
    "\n",
    "  // Print the device name\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb7b0d-9dc5-4727-af51-d3697f125519",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6aef9-3915-4699-8197-38cb1029a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_single_gpu.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a95aa-daba-4e77-bf88-2210d20eb8b2",
   "metadata": {},
   "source": [
    "### Multiple GPUs\n",
    "\n",
    "To find multiple GPU device in the system, `sycl::platform` class is used to query all devices in a system, `sycl::gpu_selector_v` is used to filter only GPU devices, the `get_devices()` method will create a vector of GPU devices found.\n",
    "\n",
    "```cpp\n",
    "auto gpus = sycl::platform(sycl::gpu_selector_v).get_devices();\n",
    "\n",
    "sycl::queue q0(gpus[0]);\n",
    "sycl::queue q1(gpus[1]);\n",
    "```\n",
    "\n",
    "Once we have found all the GPU devices, we create `sycl::queue` for each GPU device and submit job for GPU devices.\n",
    "\n",
    "The SYCL code below shows Multi-GPU device selection: Inspect code, there are no modifications necessary:\n",
    "\n",
    "Inspect the code cell below and click run ▶ to save the code to file\n",
    "Next run ▶ the cell in the Build and Run section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a2a64-38d4-4baf-86a9-222c30dfa067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/multi_gpu.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  // get all GPUs devices into a vector\n",
    "  auto gpus = sycl::platform(sycl::gpu_selector_v).get_devices();\n",
    "\n",
    "  // Print the device names\n",
    "  for(auto gpu : gpus)\n",
    "    std::cout << \"Device: \" << gpu.get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abb653-da93-42b1-afa6-158bae2f0c9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aab7f3-5b11-4d2b-9fb4-242690a9b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f18bc-3f40-444d-8813-d4ecd593e8a8",
   "metadata": {},
   "source": [
    "## Multi-GPU Kernel Submission\n",
    "\n",
    "In the previous section we learned how to find multiple GPUs and create multiple `sycl::queue`.\n",
    "\n",
    "Next we will submit kernel for execution using the `sycl::queue` for different GPUs.\n",
    "```cpp\n",
    "    // submit kernels to 2 devices\n",
    "    q0.parallel_for(N, [=](auto i) {\n",
    "      a0[i] *= 2;\n",
    "    }).wait();\n",
    "    \n",
    "    q1.parallel_for(N, [=](auto i) {\n",
    "      a1[i] *= 3;\n",
    "    }).wait();\n",
    "```\n",
    "Note that the above code will submit kernel to GPU and wait for completion, but since `.wait()` is blocking call on host, the 2 kernels will not execute concurrently on 2 GPUs.\n",
    "\n",
    "To get concurrent execution on GPUs, we have to separate the asynchronous calls and synchronization calls as shown below:\n",
    "\n",
    "```cpp\n",
    "    // submit kernels to 2 devices\n",
    "    q0.parallel_for(N, [=](auto i) {\n",
    "      a0[i] *= 2;\n",
    "    });\n",
    "    q1.parallel_for(N, [=](auto i) {\n",
    "      a1[i] *= 3;\n",
    "    });\n",
    "\n",
    "    // wait for compute complete\n",
    "    q0.wait();\n",
    "    q1.wait();\n",
    "    \n",
    "```\n",
    "\n",
    "The SYCL code below shows Multi-GPU kernel submission, submits 2 different kernels to 2 different GPUs: Inspect code, there are no modifications necessary:\n",
    "\n",
    "Inspect the code cell below and click run ▶ to save the code to file\n",
    "Next run ▶ the cell in the Build and Run section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389c163-f3e5-4cf5-a5fd-cab550779167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/multi_gpu_submit.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  // get all GPUs devices into a vector\n",
    "  auto gpus = sycl::platform(sycl::gpu_selector_v).get_devices();\n",
    "\n",
    "  if(gpus.size() >= 2) {\n",
    "    // Initialize array with values\n",
    "    const int N = 256;\n",
    "    float a[N], b[N];\n",
    "    for (int i = 0; i < N; i++){\n",
    "      a[i] = i;\n",
    "    }\n",
    "      \n",
    "    // Create sycl::queue for each gpu\n",
    "    sycl::queue q0(gpus[0]);\n",
    "    std::cout << \"GPU0: \" << gpus[0].get_info<sycl::info::device::name>() << \"\\n\";\n",
    "    sycl::queue q1(gpus[1]);\n",
    "    std::cout << \"GPU1: \" << gpus[1].get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "    // device mem alloc for each device\n",
    "    auto a0 = sycl::malloc_device<float>(N, q0);\n",
    "    auto a1 = sycl::malloc_device<float>(N, q1);\n",
    "\n",
    "    // memcpy to device alloc\n",
    "    q0.memcpy(a0, a, N * sizeof(float));\n",
    "    q1.memcpy(a1, a, N * sizeof(float));\n",
    "\n",
    "    // wait for copy to complete\n",
    "    q0.wait();\n",
    "    q1.wait();\n",
    "\n",
    "    // submit kernels to 2 devices\n",
    "    q0.parallel_for(N, [=](auto i) {\n",
    "      a0[i] *= 2;\n",
    "    });\n",
    "    q1.parallel_for(N, [=](auto i) {\n",
    "      a1[i] *= 3;\n",
    "    });\n",
    "\n",
    "    // wait for compute complete\n",
    "    q0.wait();\n",
    "    q1.wait();\n",
    "\n",
    "    // copy back result to host\n",
    "    q0.memcpy(a, a0, N * sizeof(float));\n",
    "    q1.memcpy(b, a1, N * sizeof(float));\n",
    "\n",
    "    // wait for copy to complete\n",
    "    q0.wait();\n",
    "    q1.wait();\n",
    "\n",
    "    // print output\n",
    "    for (int i = 0; i < N; i++) std::cout << a[i] << \" \";\n",
    "    std::cout << \"\\n\";\n",
    "    for (int i = 0; i < N; i++) std::cout << b[i] << \" \";\n",
    "    std::cout << \"\\n\";\n",
    "\n",
    "    sycl::free(a0, q0);\n",
    "    sycl::free(a1, q1);\n",
    "  } else {\n",
    "      std::cout << \"Multiple GPUs not available\\n\";\n",
    "  }\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14151a-20d2-475a-8241-803eebd6ca71",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e3828-5c3c-4a30-ada7-2c52b3750787",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu_submit.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710cad8-e7a9-485f-a0ac-a67f8e741e51",
   "metadata": {},
   "source": [
    "## GPU to GPU Memory Copy\n",
    "\n",
    "The code below shows how to copy memory between 2 GPUs, this is especially useful when programming kernels for multi-GPU systems. Memory copy between 2 GPUs is faster that memory copy between Host and GPU.\n",
    "\n",
    "Understanding memory copy latency differences between Host to GPU, GPU-1 to GPU-2 and same GPU copy is key for designing performant GPU Kernel code.\n",
    "\n",
    "```cpp\n",
    "  q0.memcpy(dev0_mem, host_mem, N*sizeof(int));\n",
    "\n",
    "  q0.memcpy(dev1_mem, dev0_mem, N*sizeof(int));\n",
    "\n",
    "  q0.memcpy(dev0_mem2, dev0_mem, N*sizeof(int));\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570f35e-fc14-4907-9f43-e17dbe99577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/multi_gpu_memcpy.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main(){\n",
    "  int N = 1024000000;\n",
    "\n",
    "  // get all GPUs devices into a vector\n",
    "  auto gpus = sycl::platform(sycl::gpu_selector_v).get_devices();\n",
    "\n",
    "  if(gpus.size() > 1){\n",
    "    sycl::queue q0(gpus[0], sycl::property::queue::enable_profiling());\n",
    "    sycl::queue q1(gpus[1], sycl::property::queue::enable_profiling());\n",
    "    std::cout << \"GPU0: \" << q0.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "    std::cout << \"GPU1: \" << q1.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "\tauto host_mem = sycl::malloc_host<int>(N, q0);\n",
    "    for (int i=0;i<N;i++) host_mem[i] = i;\n",
    "        \n",
    "\tauto dev0_mem = sycl::malloc_device<int>(N, q0);\n",
    "\tauto dev0_mem2 = sycl::malloc_device<int>(N, q0);\n",
    "    auto dev1_mem = sycl::malloc_device<int>(N, q1);\n",
    "        \n",
    "\t// host to GPU0 copy\n",
    "    auto event_h2d0 = q0.memcpy(dev0_mem, host_mem, N*sizeof(int));\n",
    "    q0.wait();\n",
    "\n",
    "    // GPU0 to GPU1 copy q0\n",
    "    auto event_d2d_q0 = q0.memcpy(dev1_mem, dev0_mem, N*sizeof(int));\n",
    "    q0.wait();\n",
    "\n",
    "    // GPU0 to GPU0 copy\n",
    "    auto event_d2d_same0 = q0.memcpy(dev0_mem2, dev0_mem, N*sizeof(int));\n",
    "    q0.wait();\n",
    "\n",
    "    std::cout << host_mem[0] << \" ... \" << host_mem[N-1] << \" (1M int, 3.8GB Tx)\\n\"; \n",
    "\n",
    "    // free allocation\n",
    "\tsycl::free(host_mem, q0);\n",
    "\tsycl::free(dev0_mem, q0);\n",
    "\tsycl::free(dev0_mem2, q0);\n",
    "    sycl::free(dev1_mem, q1);\n",
    "\n",
    "    // Print kernel profile times for copy\n",
    "    auto startK = event_h2d0.get_profiling_info<sycl::info::event_profiling::command_start>();\n",
    "    auto endK = event_h2d0.get_profiling_info<sycl::info::event_profiling::command_end>();\n",
    "    std::cout << \"Host to GPU0 Copy [q0]: \" << (endK - startK) / 1e+9 << \" seconds\\n\";\n",
    "    startK = event_d2d_q0.get_profiling_info<sycl::info::event_profiling::command_start>();\n",
    "    endK = event_d2d_q0.get_profiling_info<sycl::info::event_profiling::command_end>();\n",
    "\tstd::cout << \"GPU0 to GPU1 Copy [q0]: \" << (endK - startK) / 1e+9 << \" seconds\\n\";\n",
    "\tstartK = event_d2d_same0.get_profiling_info<sycl::info::event_profiling::command_start>();\n",
    "    endK = event_d2d_same0.get_profiling_info<sycl::info::event_profiling::command_end>();\n",
    "    std::cout << \"GPU0 to GPU0 Copy [q0]: \" << (endK - startK) / 1e+9 << \" seconds\\n\";\n",
    "  } else {\n",
    "    std::cout << \"Multiple GPUs not available\\n\";\n",
    "  }\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a8f11-5ee4-40c4-8f3c-96e5338be03c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dccc4-ec27-44e6-a218-73940a6a6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu_memcpy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb257c-3343-4b7c-aa47-bfa6d6fe8066",
   "metadata": {},
   "source": [
    "## Split computation to multiple GPUs\n",
    "\n",
    "The code below shows how vector add computation is split on available GPUs:\n",
    "- The total workload size is divided by the number of GPUs found on the system\n",
    "- Kernel is submitted to all GPUs and each GPUs computes a fraction of the workload\n",
    "- The result from all GPUs are aggregated to the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e6873-11b4-4f9d-8279-49b7fac7f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/multi_gpu_vadd_split.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "void kernel_compute_vadd(sycl::queue &q, float *a, float *b, float *c, size_t n) {\n",
    "  q.parallel_for(n, [=](auto i) {\n",
    "    c[i] = a[i] + b[i];\n",
    "  });\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  const int N = 1200;\n",
    "\n",
    "  // Define 3 arrays\n",
    "  float *a = static_cast<float *>(malloc(N * sizeof(float)));\n",
    "  float *b = static_cast<float *>(malloc(N * sizeof(float)));\n",
    "  float *c = static_cast<float *>(malloc(N * sizeof(float)));\n",
    "\n",
    "  // Initialize matrices with values\n",
    "  for (int i = 0; i < N; i++){\n",
    "    a[i] = 1;\n",
    "    b[i] = 2;\n",
    "    c[i] = 0;\n",
    "  }\n",
    "    \n",
    "  // get all GPUs devices into a vector\n",
    "  auto gpus = sycl::platform(sycl::gpu_selector_v).get_devices();\n",
    "  int num_devices = gpus.size();\n",
    "\n",
    "  // Create sycl::queue for each gpu\n",
    "  std::vector<sycl::queue> q(num_devices);\n",
    "  for(int i = 0; i < num_devices; i++){\n",
    "    std::cout << \"Device: \" << gpus[i].get_info<sycl::info::device::name>() << \"\\n\";\n",
    "    q.push_back(sycl::queue(gpus[i]));\n",
    "  }\n",
    "\n",
    "  // device mem alloc for vectors a,b,c for each device\n",
    "  float *da[num_devices];\n",
    "  float *db[num_devices];\n",
    "  float *dc[num_devices];\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    da[i] = sycl::malloc_device<float>(N/num_devices, q[i]);\n",
    "    db[i] = sycl::malloc_device<float>(N/num_devices, q[i]);\n",
    "    dc[i] = sycl::malloc_device<float>(N/num_devices, q[i]);\n",
    "  }\n",
    "\n",
    "  // memcpy for matrix and b to device alloc\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    q[i].memcpy(&da[i][0], &a[i*N/num_devices], N/num_devices * sizeof(float));\n",
    "    q[i].memcpy(&db[i][0], &b[i*N/num_devices], N/num_devices * sizeof(float));\n",
    "  }\n",
    "\n",
    "  // wait for copy to complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // submit vector-add kernels to all devices\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    kernel_compute_vadd(q[i], da[i], db[i], dc[i], N/num_devices);\n",
    "\n",
    "  // wait for compute complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // copy back result to host\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].memcpy(&c[i*N/num_devices], &dc[i][0], N/num_devices * sizeof(float));\n",
    "\n",
    "  // wait for copy to complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // print output\n",
    "  for (int i = 0; i < N; i++) std::cout << c[i] << \" \";\n",
    "  std::cout << \"\\n\";\n",
    "\n",
    "  free(a);\n",
    "  free(b);\n",
    "  free(c);\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    sycl::free(da[i], q[i]);\n",
    "    sycl::free(db[i], q[i]);\n",
    "    sycl::free(dc[i], q[i]);\n",
    "  }\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25468ab1-6cc1-4995-9331-2b1beae30790",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac2dff-771e-46cd-843f-659f4db07b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu_vadd_split.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abea02-268d-4a9e-ac23-7d5968a776b8",
   "metadata": {},
   "source": [
    "## Optimizing Multi-GPU Offload\n",
    "\n",
    "Lets look at an example of matrix multiplication kernel submitted to multiple GPUs.\n",
    "\n",
    "The SYCL code uses a for-loop for the number of GPUs to perform operations like:\n",
    "- creating memory allocation\n",
    "- memory copy from host to device\n",
    "- kernel submission\n",
    "- memory copy from device to host\n",
    "\n",
    "All these operations are asynchronously executed, and they run concurrently on GPU.\n",
    "\n",
    "```\n",
    "  // device mem alloc for matrix a,b,c for each device\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    da[i] = sycl::malloc_device<float>(N * N, q[i]);\n",
    "    db[i] = sycl::malloc_device<float>(N * N, q[i]);\n",
    "    dc[i] = sycl::malloc_device<float>(N * N, q[i]);\n",
    "  }\n",
    "\n",
    "  // memcpy for matrix and b to device alloc\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    q[i].memcpy(&da[i][0], &matrix_a[i][0], N * N * sizeof(float));\n",
    "    q[i].memcpy(&db[i][0], &matrix_b[i][0], N * N * sizeof(float));\n",
    "  }\n",
    "\n",
    "  // wait for copy to complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // submit matrix multiply kernels to all devices\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    kernel_compute_mm(q[i], da[i], db[i], dc[i], N, B);\n",
    "```\n",
    "The full SYCL code for performing matrix multiplication on multiple GPUs is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203db4c9-fa5d-4a08-a26d-44e6e071877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/multi_gpu_mm.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <execution>\n",
    "\n",
    "static constexpr size_t N = 5120; // global size\n",
    "static constexpr size_t B = 32;   // WG size\n",
    "\n",
    "void kernel_compute_mm(sycl::queue &q, float *a, float *b, float *c, size_t n, size_t wg) {\n",
    "  q.parallel_for(\n",
    "      sycl::nd_range<2>(sycl::range<2>{n, n}, sycl::range<2>{wg, wg}),\n",
    "      [=](sycl::nd_item<2> item) {\n",
    "        const int i = item.get_global_id(0);\n",
    "        const int j = item.get_global_id(1);\n",
    "        float temp = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "          temp += a[i * N + k] * b[k * N + j];\n",
    "        }\n",
    "        c[i * N + j] = temp;\n",
    "      });\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  auto start = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
    "\n",
    "  // find all GPUs\n",
    "  auto gpus = sycl::platform(sycl::gpu_selector_v).get_devices();\n",
    "  int num_devices = gpus.size();\n",
    "\n",
    "  // Define matrices\n",
    "  float *matrix_a[num_devices];\n",
    "  float *matrix_b[num_devices];\n",
    "  float *matrix_c[num_devices];\n",
    "\n",
    "  float v1 = 2.f;\n",
    "  float v2 = 3.f;\n",
    "  for (int n = 0; n < num_devices; n++) {\n",
    "    matrix_a[n] = static_cast<float *>(malloc(N * N * sizeof(float)));\n",
    "    matrix_b[n] = static_cast<float *>(malloc(N * N * sizeof(float)));\n",
    "    matrix_c[n] = static_cast<float *>(malloc(N * N * sizeof(float)));\n",
    "\n",
    "    // Initialize matrices with values\n",
    "    for (int i = 0; i < N; i++)\n",
    "      for (int j = 0; j < N; j++) {\n",
    "        matrix_a[n][i * N + j] = v1++;\n",
    "        matrix_b[n][i * N + j] = v2++;\n",
    "        matrix_c[n][i * N + j] = 0.f;\n",
    "      }\n",
    "  }\n",
    "\n",
    "  float *da[num_devices];\n",
    "  float *db[num_devices];\n",
    "  float *dc[num_devices];\n",
    "\n",
    "  std::vector<sycl::queue> q(num_devices);\n",
    "  std::vector<int> id;\n",
    "\n",
    "  // create queues for each device\n",
    "  std::cout << \"\\nSubmitting Compute Kernel to GPUs:\\n\";\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    q[i] = sycl::queue(gpus[i]);\n",
    "    id.push_back(i);\n",
    "    std::cout << \"GPU\" << i << \": \" << q[i].get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "  }\n",
    "\n",
    "  // device mem alloc for matrix a,b,c for each device\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    da[i] = sycl::malloc_device<float>(N * N, q[i]);\n",
    "    db[i] = sycl::malloc_device<float>(N * N, q[i]);\n",
    "    dc[i] = sycl::malloc_device<float>(N * N, q[i]);\n",
    "  }\n",
    "\n",
    "  // memcpy for matrix and b to device alloc\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    q[i].memcpy(&da[i][0], &matrix_a[i][0], N * N * sizeof(float));\n",
    "    q[i].memcpy(&db[i][0], &matrix_b[i][0], N * N * sizeof(float));\n",
    "  }\n",
    "\n",
    "  // wait for copy to complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // submit matrix multiply kernels to all devices\n",
    "  /*\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    kernel_compute_mm(q[i], da[i], db[i], dc[i], N, B);\n",
    "  */\n",
    "  std::for_each(std::execution::par, id.begin(), id.end(), [&q, &da, &db, &dc](auto i){\n",
    "    kernel_compute_mm(q[i], da[i], db[i], dc[i], N, B);\n",
    "  });\n",
    "\n",
    "  // wait for compute complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // copy back result to host\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].memcpy(&matrix_c[i][0], &dc[i][0], N * N * sizeof(float));\n",
    "\n",
    "  // wait for copy to complete\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    q[i].wait();\n",
    "\n",
    "  // print first element of result matrix\n",
    "  std::cout << \"\\nMatrix Multiplication Complete\\n\\n\";\n",
    "  for (int i = 0; i < num_devices; i++)\n",
    "    std::cout << \"GPU\" << i << \": matrix_c[0][0]=\" << matrix_c[i][0] << \"\\n\";\n",
    "\n",
    "  for (int i = 0; i < num_devices; i++) {\n",
    "    free(matrix_a[i]);\n",
    "    free(matrix_b[i]);\n",
    "    free(matrix_c[i]);\n",
    "    sycl::free(da[i], q[i]);\n",
    "    sycl::free(db[i], q[i]);\n",
    "    sycl::free(dc[i], q[i]);\n",
    "  }\n",
    "\n",
    "  auto duration = std::chrono::high_resolution_clock::now().time_since_epoch().count() - start;\n",
    "  std::cout << \"\\nCompute Duration: \" << duration / 1e+9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afde543-72b9-41f6-9d35-3ab2603345ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53440368-0b8f-4acd-88f5-3e57d2c5aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu_mm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80edfeb-7d6c-4dae-8991-591dfabddcd7",
   "metadata": {},
   "source": [
    "#### VTune Analysis\n",
    "\n",
    "The CPU and GPU can be profiled using VTune using the command `vtune --collect gpu-hotspots`, from the VTune profiling data, it can be seen that the kernel submission for multiple GPUs has an overhead of kernel `zeModuleCreate` on the host. `zeModuleCreate` is backend API that compiles kernel code for execution on GPU. This does not happen concurrently since a for-loop is used for multiple GPUs, which will stack execution on host. This can be observed the VTune capture:\n",
    "```\n",
    "for (int i = 0; i < num_devices; i++)\n",
    "  kernel_compute_mm(q[i], da[i], db[i], dc[i], N, B);\n",
    "```\n",
    "<img src=\"assets/vtune_mm.png\">\n",
    "\n",
    "The kernel submission for multiple GPUs can instead be called using `std::for_each` with `std::execution::par` policy, which will execute kernel `zeModuleCreate` using separate threads on CPU as shown below, which will overlap kernel `zeModuleCreate` on CPU and improves performance.\n",
    "```\n",
    "std::for_each(std::execution::par, id.begin(), id.end(), [&q, &da, &db, &dc](auto i){\n",
    "  kernel_compute_mm(q[i], da[i], db[i], dc[i], N, B);\n",
    "});\n",
    "```\n",
    "<img src=\"assets/vtune_mm_tbb.png\">\n",
    "\n",
    "Execute the script below to collect VTune data for the code above, you can capture VTune data for both methods of submitting kernel to multiple GPUs by modifying the SYCL code and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b3f41-77e1-4097-9b98-2f2b45d49e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./vtune_multi_gpu_mm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b013b7-9141-40d6-9f98-d84f509e8011",
   "metadata": {},
   "source": [
    "## Tips for Multi-GPU Programming\n",
    "\n",
    "- Check if offloading to multiple GPUs is benificial or not: Calculate total HW threads that can be executed concurrently on one GPU and compare it to the total number of work-items in the kernel workload.\n",
    "- Host to GPU memory copy is more expensive that __GPU to GPU memory copy__. Reduce the memory copy latency by aggregating computation on GPUs before copying back to Host when applicable.\n",
    "- Use __CPU multi-threading__ when launching multiple kernels to multiple GPUs to overlap Kernel Module Creation.\n",
    "- Understand which SYCL calls are __asynchronous__ versus __blocking calls__ to get concurrency on multiple GPUs.\n",
    "- Use __VTune Profiler__ to collect `gpu-hospots` and check for concurrency in thread dispatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566a53d-bb94-449d-bcd9-e9be1a3c294b",
   "metadata": {},
   "source": [
    "## Filter offload devices with ONEAPI_DEVICE_SELECTOR\n",
    "\n",
    "`ONEAPI_DEVICE_SELECTOR` environment variable can be used to filter the available devices for SYCL kernel offload. \n",
    "\n",
    "If SYCL `queue` uses `default_selector`, then the offload device can be controlled using `ONEAPI_DEVICE_SELECTOR` environment variable rather than updating SYCL code and re-compiling. This is useful to do quick performance analysis on different hardware architectures or different hardware vendors.\n",
    "\n",
    "The SYCL code should be __performance portable__ to get accurate consistent results on different hardware architectures or different hardware vendors. For example, different hardware architectures have different Max Works-Group size or Max Local Memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394fe02-5bb4-4aa6-b8b9-758d801effb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build_offload_mm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cd641-8efd-441e-b92d-790d5c563c11",
   "metadata": {},
   "source": [
    "### Filter by CPU\n",
    "Check the script below which sets `ONEAPI_DEVICE_SELECTOR` value to `opencl:cpu` for offload and run the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b858cf0-52ef-41a8-abaa-d26f0dc2ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_offload_mm_filter.sh\n",
    "#!/bin/bash\n",
    "source /opt/intel/oneapi/setvars.sh > /dev/null 2>&1\n",
    "\n",
    "export ONEAPI_DEVICE_SELECTOR=opencl:cpu\n",
    "\n",
    "if [ $? -eq 0 ]; then ./offload_mm.out; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc4b6a-8814-4a14-846d-8ddb30122f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_offload_mm_filter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1e3da-0126-4b58-95e4-1f8a3301ca91",
   "metadata": {},
   "source": [
    "### Filter by GPU OpenCL Backend\n",
    "Check the script below which sets `ONEAPI_DEVICE_SELECTOR` value to `opencl:gpu` for offload and run the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47732b-ac31-4516-b34e-8fdabcd836d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_offload_mm_filter.sh\n",
    "#!/bin/bash\n",
    "source /opt/intel/oneapi/setvars.sh > /dev/null 2>&1\n",
    "\n",
    "export ONEAPI_DEVICE_SELECTOR=opencl:gpu\n",
    "\n",
    "if [ $? -eq 0 ]; then ./offload_mm.out; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210c6f0-88ca-4da0-a6e1-fc981f801b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_offload_mm_filter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab1991-4f86-4a6a-959f-2704bbc24614",
   "metadata": {},
   "source": [
    "### Filter by GPU Level-Zero Backend\n",
    "Check the script below which sets `ONEAPI_DEVICE_SELECTOR` value to `level_zero:gpu` for offload and run the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a098ae4-efab-497a-8cff-0690f306a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_offload_mm_filter.sh\n",
    "#!/bin/bash\n",
    "source /opt/intel/oneapi/setvars.sh > /dev/null 2>&1\n",
    "\n",
    "export ONEAPI_DEVICE_SELECTOR=level_zero:gpu\n",
    "\n",
    "if [ $? -eq 0 ]; then ./offload_mm.out; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47a801-9615-4827-8f84-9e683b5c4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_offload_mm_filter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375ee78-6e63-4324-b63d-f5c65716a716",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filter number of GPUs available for offload\n",
    "\n",
    "The examples below show how to filter number of GPUs available for offload\n",
    "\n",
    "Run the script below to compile the VectorAdd SYCL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d829c9-1349-435d-aba4-2f96a99a29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build_offload_vadd.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847b2f7-22ac-4d60-8441-4f31f56ce06a",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication Kernel on 2 GPUs:\n",
    "Check the script below which sets `ONEAPI_DEVICE_SELECTOR` value to limit GPUs available for offload and run the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef371cf-2fdc-47b6-9600-c9c7e97e0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_multi_gpu_vadd_filter.sh\n",
    "#!/bin/bash\n",
    "source /opt/intel/oneapi/setvars.sh > /dev/null 2>&1\n",
    "\n",
    "export ONEAPI_DEVICE_SELECTOR=\"level_zero:0;level_zero:1\"\n",
    "\n",
    "if [ $? -eq 0 ]; then ./offload_vadd.out; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81a7f3-1dea-4c58-9f0e-86b186ca7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu_vadd_filter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52087d-7099-49bb-add4-febe7f5e5b1d",
   "metadata": {},
   "source": [
    "#### VectorAdd Computation split on 3 GPUs:\n",
    "Check the script below which sets `ONEAPI_DEVICE_SELECTOR` value to limit GPUs available for offload and run the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc1dff-0ef9-4698-90e1-6d1c7554f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_multi_gpu_vadd_filter.sh\n",
    "#!/bin/bash\n",
    "source /opt/intel/oneapi/setvars.sh > /dev/null 2>&1\n",
    "\n",
    "export ONEAPI_DEVICE_SELECTOR=\"level_zero:0;level_zero:1;level_zero:2\"\n",
    "\n",
    "if [ $? -eq 0 ]; then ./offload_vadd.out; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c06322-4796-4fee-b5dd-66355fdb8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_multi_gpu_vadd_filter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734ecb8-d2c2-47fb-8cfc-3908783d396a",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this module you learned:\n",
    "* How to use `sycl-ls` and `ONEAPI_DEVICE_SELECTOR` environment variable to filter offload devices\n",
    "* How to select multiple GPUs and offload a kernel\n",
    "* How to copy memory from GPU-1 to GPU-2\n",
    "* How to optimize Multi-GPU SYCL code for performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
