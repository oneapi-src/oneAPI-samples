{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# C++ SYCL Introduction\n",
    "\n",
    "In this section we cover Introduction to compute offloading to accelerators and introduce SYCL programming starting with C++:\n",
    "- [Why Offload Computation to Accelerators?](#Why-Offload-Computation-to-Accelerators?)\n",
    "- [Simple Computation on CPU](#Simple-Computation-on-CPU)\n",
    "- [C++ SYCL for Offloading Computation on Device](#C++-SYCL-for-Offloading-Computation-on-Device)\n",
    "- [SYCL Libraries for Offloading Computation on Device](#SYCL-Libraries-for-Offloading-Computation-on-Device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d3d97-1614-4bd3-88c8-14925cc46c68",
   "metadata": {},
   "source": [
    "## Why Offload Computation to Accelerators?\n",
    "\n",
    "Large computational problems in High Perforamance Computing run substantially faster on specialized hardware accelerators like a GPU than on a CPU.\n",
    "\n",
    "Accelerators like GPU can run many smaller computations at once by making use of parallelism in the hardware.\n",
    "\n",
    "### Programming Languages for Accelerators\n",
    "\n",
    "| Language | Description\n",
    "|:--|:--\n",
    "| CUDA | CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA for programming to NVIDIA GPUs.\n",
    "| HIP | HIP (Heterogeneous Interface for Portability) is a free and open-source runtime API and kernel language. With it, you can convert an existing CUDA® application into a single C++ code base that can be compiled to run on AMD or NVIDIA GPUs.\n",
    "| OpenMP | OpenMP (Open Multi-Processing) is an application programming interface (API) that supports multi-platform shared-memory multiprocessing programming in C/C++ and Fortran. OpenMP allows loop-level parallelism and also supports function-level parallelism.\n",
    "| OpenACC | OpenACC (Open Accelerators) specification supports C, C++, Fortran programming languages and multiple hardware architectures, OpenACC uses directives to tell the compiler where to parallelize loops, and how to manage data between host and accelerator memories.\n",
    "| OpenCL | OpenCL (Open Computing Language) is an open, royalty-free standard  by the Khronos Group for cross-platform, parallel programming of diverse accelerators. OpenCL provides C-like programming model and is low-level framework that need manual memory management and synchronization. \n",
    "| SYCL | SYCL is a royalty-free open standard developed by the Khronos Group that allows developers to program heterogeneous architectures in standard C++. Supports NVIDIA, AMD and Intel GPUs. SYCL provides C++ like programming with high-level abstractions which make it easier to program.\n",
    "\n",
    "\n",
    "### Accelerating Choice with SYCL\n",
    "SYCL (pronounced ‘sickle’) is a royalty-free, cross-platform abstraction layer that:\n",
    "\n",
    "- Enables code for heterogeneous and offload processors to be written using modern ISO C++ (at least C++ 17).\n",
    "- Provides APIs and abstractions to find devices (e.g. CPUs, GPUs, FPGAs) on which code can be executed, and to manage data resources and code execution on those devices.\n",
    "\n",
    "- Open, standards-based\n",
    "- Multiarchitecture performance \n",
    "- Freedom from vendor lock-in\n",
    "- Comparable performance to native CUDA on Nvidia GPUs\n",
    "- Extension of widely used C++ language\n",
    "- Speed code migration via open source SYCLomatic\n",
    "\n",
    "\n",
    "## Simple Computation on CPU\n",
    "\n",
    "Let’s look at this simple C++ Code\n",
    "- We initialize a data array\n",
    "- We do some computation on each element of the array\n",
    "- Print the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931b73f-7025-4bd5-99ca-191d8e696002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/cpp_compute.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "int main(){\n",
    "\n",
    "    //# initialize some data array\n",
    "    const int N = 16;\n",
    "    float data[N];\n",
    "    for(int i=0;i<N;i++) data[i] = i;\n",
    "\n",
    "    //# computation on CPU\n",
    "    for(int i=0;i<N;i++) data[i] = data[i] * 5;\n",
    "\n",
    "    //# print output\n",
    "    for(int i=0;i<N;i++) std::cout << data[i] << \"\\n\"; \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c58df-3798-4cc0-9a8a-8a9d2ea47e05",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "\n",
    "We will use `icpx` to compile the C++ code\n",
    "\n",
    "```sh\n",
    "icpx lab/cpp_compute.cpp\n",
    "```\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e74bf-fad3-471f-b2a8-70101e817270",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_cpp_compute.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c0505-9455-458a-be4a-6f9ad7a72df3",
   "metadata": {},
   "source": [
    "## C++ SYCL for Offloading Computation on Device\n",
    "\n",
    "The code below show modifications done to the above C++ code using SYCL so that the computation is offloaded to accelerator device.\n",
    "\n",
    "What is SYCL doing here:\n",
    "- Select device for offloading\n",
    "- Allocate memory so that both host and device can access\n",
    "- Submit a kernel task to device for computation and wait for completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c37ebb-15f7-4b8b-ad81-72a0550af75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sycl_compute_offload.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iostream>\n",
    "\n",
    "int main(){\n",
    "    //# select device for offload\n",
    "    sycl::queue q(sycl::gpu_selector_v);\n",
    "    std::cout << \"Offload Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "    //# initialize some data array\n",
    "    const int N = 16;\n",
    "    auto data = sycl::malloc_shared<float>(N, q);\n",
    "    for(int i=0;i<N;i++) data[i] = i;\n",
    "\n",
    "    //# computation on GPU\n",
    "    q.single_task([=](){\n",
    "        for(int i=0;i<N;i++) data[i] = data[i] * 5;\n",
    "    }).wait();\n",
    "\n",
    "    //# print output\n",
    "    for(int i=0;i<N;i++) std::cout << data[i] << \"\\n\"; \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300e9bb-e1e1-438e-9952-4b898e7cb8da",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "\n",
    "We will use `icpx` to compile the C++ SYCL code with `-fsycl` compile flag\n",
    "\n",
    "```sh\n",
    "icpx -fsycl lab/sycl_compute_offload.cpp\n",
    "```\n",
    "\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6be4c6-0805-48c5-99e4-e45583eb49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_sycl_compute_offload.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1884fc1-a6d0-4fae-bae6-894d31a98231",
   "metadata": {},
   "source": [
    "There are three important functionality the SYCL enables:\n",
    "- __Device Selection__ for offloading computation\n",
    "- __Memory Allocation__ so that both host and device can access data\n",
    "- __Submit Compute Task__ to execute on device\n",
    "\n",
    "### SYCL - Device Selection\n",
    "\n",
    "`sycl::queue` is used schedule a task to execute on a device\n",
    "The device can be specified when sycl::queue is constructed\n",
    "\n",
    "| SYCL Device Selector | Description |\n",
    "|---|---|\n",
    "| `sycl::queue q(sycl::gpu_selector_v);`| Select GPU for offloading |\n",
    "| `sycl::queue q(sycl::cpu_selector_v);`| Select CPU for offloading|\n",
    "| `sycl::queue q(sycl::accelerator_selector_v);`|Select other Accelerator for offloading|\n",
    "| `sycl::queue q(sycl::default_selector_v);`|Select default device for offloading, GPU if exists, if not CPU|\n",
    "| `sycl::queue q;` | Same as default selector|\n",
    "| `sycl::queue q(sycl::aspect_selector(aspect::fp64));` | Select a device that supports fp64 |\n",
    "\n",
    "### SYCL - Memory Allocation\n",
    "\n",
    "`sycl::malloc_shared` is used here to allocate memory that can be accessed by both host and device and data movement happens implicitly.\n",
    "\n",
    "There is also `sycl::malloc_device`, which allocates memory on device, which allows more controlled explicit data movement, which is recommended for performance.\n",
    "\n",
    "### SYCL – Submitting Task to Device\n",
    "\n",
    "`q.single_task` is the most basic method to submit a task to execute on device.\n",
    "\n",
    "The kernel execution happens asynchronously, so we have to synchronize with host.\n",
    "`.wait()` method is used to synchronize with host by waiting for task completion.\n",
    "\n",
    "There is also `q.parallel_for`, which allows submitting a task and enables parallel execution on device.\n",
    "\n",
    "The code below shows usage of `parallel_for` to enable parallelism of execution on device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943cba8-e952-4953-bef7-927c6be7d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sycl_compute_offload_parallelism.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iostream>\n",
    "\n",
    "int main(){\n",
    "    //# select device for offload\n",
    "    sycl::queue q(sycl::gpu_selector_v);\n",
    "    std::cout << \"Offload Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "    //# initialize some data array\n",
    "    const int N = 16;\n",
    "    auto data = sycl::malloc_shared<float>(N, q);\n",
    "    for(int i=0;i<N;i++) data[i] = i;\n",
    "\n",
    "    //# parallel computation on GPU\n",
    "    q.parallel_for(N,[=](auto i){\n",
    "        data[i] = data[i] * 5;\n",
    "    }).wait();\n",
    "\n",
    "    //# print output\n",
    "    for(int i=0;i<N;i++) std::cout << data[i] << \"\\n\"; \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bae7f6-36d5-42d8-81e2-a86bc22702d6",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9861bda-3aed-48ed-a667-af0f279f1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_sycl_compute_offload_parallelism.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460298e-db78-4887-83ac-a05ed825c4e1",
   "metadata": {},
   "source": [
    "## SYCL Libraries for Offloading Computation on Device\n",
    "\n",
    "The code below show modifications done to the above C++ code using a SYCL Library (oneDPL or oneAPI Data Parallel C++ Library) so that the computation is offloaded to accelerator device without having to know any knowledge of SYCL programming.\n",
    "\n",
    "The code uses `oneapi::dpl::for_each(...)` library call which will perform the defined functionality on range of data elements, it also defines an execution policy `oneapi::dpl::execution::dpcpp_default`, which will select the default SYCL device used for offloading compution.   \n",
    "\n",
    "There are many more SYCL libraries available that will simplify offloading certain types of  computations to devices. These libraries can be used to quickly get device offloading to work when starting from C/C++ code or if the computation is simple. But using actual SYCL calls to offload computation allows you better optimize the code for performance by programming to device hardware features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4daf8-9cf3-49b1-b514-ef595b44bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/onedpl_compute_offload.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <oneapi/dpl/algorithm>\n",
    "#include <oneapi/dpl/execution>\n",
    "#include <iostream>\n",
    "\n",
    "int main(){\n",
    "    //# initialize some data array\n",
    "    const int N = 16;\n",
    "    std::vector<int> data(N);\n",
    "    for(int i=0;i<N;i++) data[i] = i;\n",
    "\n",
    "    //# parallel computation on GPU using SYCL library (oneDPL)\n",
    "    oneapi::dpl::for_each(oneapi::dpl::execution::dpcpp_default, data.begin(), data.end(), [](int &tmp){ tmp *= 5; });\n",
    "\n",
    "    //# print output\n",
    "    for(int i=0;i<N;i++) std::cout << data[i] << \"\\n\"; \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69f678-ec8b-4ac3-a4fe-7c9446516a3b",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1fc28-6e6f-4f7a-adc1-d5121753a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_onedpl_compute_offload.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77dfc41-94f4-4aea-8284-27474d308655",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lab Exercise: Vector Add\n",
    "\n",
    "Complete the coding excercise below using SYCL Buffer and Accessor concepts:\n",
    "- The code has three arrays initialized on host\n",
    "- There is a for-loop to compute c = a + b\n",
    "- Modify the code using SYCL to offload the computation to a device\n",
    "  - include SYCL header\n",
    "  - create a SYCL queue for offloading\n",
    "  - allocate memory so that both device and host can access\n",
    "  - submit a kernel for computation\n",
    "\n",
    "1. Edit the code cell below by following the steps and then click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a700ba-dfe5-4dc0-b57d-360cb1854e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sycl_vector_add.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "//# STEP 1 : Include header for SYCL\n",
    "//# YOUR CODE GOES HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "int main(){\n",
    "    \n",
    "    //# STEP 2: Create a SYCL queue and device selection for offload\n",
    "    //# YOUR CODE GOES HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    //# initialize some data array\n",
    "    const int N = 16;\n",
    "    \n",
    "    //# STEP 3: Allocate memory so that both host and device can access\n",
    "    //# MODIFY THE CODE BELOW    \n",
    "    float a[N], b[N], c[N];\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for(int i=0;i<N;i++) {\n",
    "        a[i] = 1;\n",
    "        b[i] = 2;\n",
    "        c[i] = 0;\n",
    "    }\n",
    "\n",
    "    \n",
    "    //# STEP 4: Submit computation to Offload device\n",
    "    //# MODIFY THE CODE BELOW      \n",
    "    \n",
    "    //# computation\n",
    "    for(int i=0;i<N;i++) c[i] = a[i] + b[i];\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    //# print output\n",
    "    for(int i=0;i<N;i++) std::cout << c[i] << \"\\n\"; \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6ace9-6f60-4341-bfa0-f08c2e88222d",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click Run ▶ to compile and execute the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3bdeb-4bf4-47ed-934c-1c4fd55bb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./run_sycl_vector_add.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96346910-4705-4cfa-a6a7-5aab154da127",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "C++ with SYCL is open standard for heterogenous computing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
