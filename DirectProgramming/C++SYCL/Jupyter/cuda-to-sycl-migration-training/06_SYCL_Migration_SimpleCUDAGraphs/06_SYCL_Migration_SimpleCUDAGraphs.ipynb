{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8f271f-d562-44f8-bf5f-96310bef377d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SYCL Migration - SimpleCUDAGraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada1808-d73a-4d96-a1bf-9dccdab7747b",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [Introduction](#Introduction)\n",
    "- [Analyze CUDA source](#Analyze-CUDA-source)\n",
    "- [Migrate CUDA source to SYCL source](#Migrate-CUDA-source-to-SYCL-source)\n",
    "- [Analyze, Compile and Run the migrated SYCL source](#Analyze,-Compile-and-Run-the-migrated-SYCL-source)\n",
    "- [Source Code](#Source-Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fbb8d-244b-4f90-9de9-3846df15afc1",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Use SYCLomatic Tool to migrate a simple single source CUDA application\n",
    "* Use various command line options of `SYCLomatic` for CUDA to SYCL migration\n",
    "* Compile and run migrated SYCL code on Intel CPUs and GPUs\n",
    "* Optimize the migrated SYCL code with manual coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e6c1f-95ee-419d-8149-6a7789998ef0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This module will walk you through migrating CUDA code to SYCL code using Intel SYCLomatic Tool\n",
    "\n",
    "#### Requirements\n",
    "1. NVidia CUDA development machine\n",
    "2. Development machine with Intel CPU/GPU or a Intel Developer Cloud account\n",
    "\n",
    "#### Migration Process\n",
    "We will do the following steps in this hands-on workshop:\n",
    "- Analyze CUDA source\n",
    "- Migrate CUDA source to SYCL source\n",
    "- Analyze, Compile and Run the migrated SYCL source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5aa043-0d1f-43df-9a9a-e38f13e4bc86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyze CUDA source\n",
    "\n",
    "The CUDA source for \"SimpleCUDAGraphs\" example is available on [Nvidia Github](https://github.com/NVIDIA/cuda-samples/tree/v11.8/Samples/3_CUDA_Features/simpleCudaGraphs)\n",
    "\n",
    "Pull the entire repository on your CUDA Development machine.\n",
    "\n",
    "```\n",
    "git clone https://github.com/NVIDIA/cuda-samples.git\n",
    "\n",
    "cd cuda-samples/Samples/3_CUDA_Features/simpleCudaGraphs/\n",
    "```\n",
    "\n",
    "The CUDA source demonstrates CUDA Graphs creation, instantiation and launch using Graphs APIs and Stream Capture APIs in the following file.\n",
    "\n",
    "[__simpleCudaGraphs.cu__](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/3_CUDA_Features/simpleCudaGraphs) — host code for:\n",
    "-   The CUDA Graph API is demonstrated in two CUDA functions, cudaGraphsManual() and cudaGraphsUsingStreamCapture()\n",
    "-\tcudaGraphsManual() uses explicit CUDA Graph APIs\n",
    "-\tcudaGraphsUsingStreamCapture() uses stream capture APIs\n",
    "-\tReduction is performed in two CUDA kernels reduce() and reduceFinal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495ba6e-e388-4270-87ee-72a669b1aef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Migrate CUDA source to SYCL source\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: A CUDA development machine is required to accomplish the task in this section </p>\n",
    "\n",
    "Now that we have analyzed the CUDA source, we will migrate the CUDA source into SYCL source using the __SYCLomatic Tool__.\n",
    "\n",
    "In this exercise, we will walk you through step-by-step to migrate the CUDA code.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Make sure you have a __NVIDIA CUDA development machine__ that can __compile and run CUDA code__. The next step is to install the tools for migrating CUDA to SYCL:\n",
    "\n",
    "- Install SYCLomatic Tool on this machine\n",
    "  - go to https://github.com/oneapi-src/SYCLomatic/releases/\n",
    "  - copy link to latest `linux_release.tgz` from assets\n",
    "  - on the CUDA development machine: `mkdir syclomatic; cd syclomatic`\n",
    "  - `wget <link to linux_release.tgz>`\n",
    "  - `tar -xvf linux_release.tgz`\n",
    "  - `export PATH=\"/home/$USER/syclomatic/bin:$PATH\"`\n",
    "  - Verify installation: `c2s --version`\n",
    "- pull the CUDA samples repo to this machine\n",
    "  - `git clone https://github.com/NVIDIA/cuda-samples.git`\n",
    "- Compile and run the `simpleCudaGraphs` sample\n",
    "  - `cd cuda-samples/Samples/3_CUDA_Features/simpleCudaGraphs/`\n",
    "  - `make`\n",
    "\n",
    "\n",
    "### Migrate CUDA source to SYCL source using SYCLomatic\n",
    "\n",
    "On the NVIDIA CUDA Development machine, go to the CUDA source folder and generate a compilation database with the tool `intercept-build`. This creates a JSON file with all the compiler invocations, stores the names of the input files and the compiler options.\n",
    "\n",
    "```\n",
    "make clean\n",
    "intercept-build make\n",
    "```\n",
    "\n",
    "This will create a file named `compile_commands.json` in the sample folder.\n",
    "\n",
    "Next, use the SYCLomatic Tool (c2s) to migrate the code; it will store the result in the migration folder `dpct_output`:\n",
    "\n",
    "```\n",
    "c2s -p compile_commands.json --in-root ../../.. --gen-helper-function --use-experimental-features=logical-group\n",
    "```\n",
    "\n",
    "The `--gen-helper-function` option will copy the SYCLomatic helper header files to output directory.\n",
    "\n",
    "The `-use-experimental-features=logical-group` option is needed since this CUDA example is using CUDA cooperative groups, and SYCLomatic will migrate this using a experimemtal feature currently.\n",
    "\n",
    "The `--in-root` option will specify the path for all the common include files for the CUDA project.\n",
    "\n",
    "This command should migrate the CUDA source to the C++ SYCL source in a folder named `dpct_output` by default, and the folder will have the C++ SYCL source along with any dependencies from the `Common` folder,\n",
    "\n",
    "- `simpleCudaGraphs.dp.cpp`\n",
    "\n",
    "This command may also throw a bunch of warnings about the migration process. The CUDA code that cannot be automatically migrated will have warning comments generated in the migrated source files, which have to be manually migrated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394c88b-9fb6-4160-b320-1e7491fa7139",
   "metadata": {},
   "source": [
    "## Analyze, Compile and Run the migrated SYCL source\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: The tasks in this section should be done on Intel DevCloud or on a system with oneAPI Base toolkit installed.</p>\n",
    "\n",
    "The migrated SYCL code are in the `Samples` folder under the `dpct_output` folder:\n",
    "- `simpleCudaGraphs.dp.cpp`\n",
    "\n",
    "The `dpct_output` folder also has headers files needed for compiling the migrated SYCL code. The `Common` folder has header files with CUDA helper functions which are migrated to SYCL and the `include` folder has header files with SYCLomatic helper functions.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Make sure you have one of the following:\n",
    "- __Development machine with Intel CPU/GPU__ with Intel oneAPI Base Toolkit installed\n",
    "- __Intel Developer Cloud__ account to access the Intel CPUs/GPUs on the cloud\n",
    "\n",
    "### Compiling migrated SYCL code\n",
    "\n",
    "Copy the files mentioned above in `dpct_output` folder on __Nvidia Development Machine__ to __Intel Developer Cloud__\n",
    "\n",
    "To compile the migrated SYCL code we can use the following command:\n",
    "```\n",
    "icpx -fsycl -fsycl-targets=intel_gpu_pvc -I ../../../Common -I ../../../include *.cpp\n",
    "```\n",
    "\n",
    "There may be compile errors based on whether all of the CUDA code was migrated to SYCL or not. The migrated code may also include comments with warning messages, which could help make it easier to fix the errors. These errors have to be manually fixed to get the code to compile.\n",
    "\n",
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code (expect to see errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1df07-401e-4842-95c8-0e753bd41df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_dpct_output.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58a333-ceee-4bea-ba9c-f0982b49449c",
   "metadata": {},
   "source": [
    "### Fixing unmigrated SYCL code\n",
    "\n",
    "The manual migration of CUDA Graph API calls to SYCL can be done using two separate approaches,\n",
    "- The Taskflow programming model which manages a task dependency graph demonstrated in sycl_migrated_option1 \n",
    "- The SYCL Graph extension with command groups SYCL creates an implicit dependency graph of kernel execution at runtime demonstrated in sycl_migrated_option2\n",
    "\n",
    "With Taskflow approach we do not migrate `cudaGraphsUsingStreamCapture()` because CUDA Stream Capture APIs are not yet supported in SYCL through Taskflow.\n",
    "With SYCL Graph approach the method `cudaGraphsManual()` is migrated using Explicit graph building API and the method `cudaGraphsUsingStreamCapture()` is migrated using Queue recording API.\n",
    "\n",
    "The following warnings in the \"DPCT1XXX\" format are generated by the tool to indicate the code has not been migrated by the tool and needs to be manually modified to complete the migration. Below are the manual workarounds, Option 1 for sycl_migrated_option1 and Option 2 for sycl_migrated_option2 respectively.\n",
    "\n",
    "##### 1. DPCT1007: Migration of cudaGraphCreate is not supported.\n",
    "```\n",
    "cudaGraphCreate(&graph, 0);\n",
    "```\n",
    "##### Option 1 (using Taskflow): \n",
    "SYCL doesn’t support migration of CUDA Graphs API yet. We can manually migrate these APIs with the help of [Taskflow](https://github.com/taskflow/taskflow) programming model which supports SYCL. Taskflow introduces a lightweight task graph-based programming model, [tf::syclFlow](https://github.com/taskflow/taskflow/tree/master/taskflow/sycl), for tasking SYCL operations and their dependencies. We must include the header file, taskflow/sycl/syclflow.hpp, for using tf::syclFlow.\n",
    "```\n",
    "#include <taskflow/sycl/syclflow.hpp>\r\n",
    "...\r\n",
    "tf::Taskflow tflow;\r\n",
    "tf::Executor exe;\n",
    "```\n",
    "\n",
    "The above code lines construct a taskflow and an executor. The graph created by the taskflow is executed by an executor.\n",
    "##### Option 2 (using SYCL Graph):\n",
    " SYCL Graph is an addition in `ext::oneapi::experimental` namespace, SYCL command_graph creates an object in the modifiable state for context `syclContext` and device `syclDevice`.\n",
    "\n",
    "```\n",
    "namespace sycl_ext = sycl::ext::oneapi::experimental;\r\n",
    "sycl_ext::command_graph graph(q.get_context(), q.get_device();\n",
    "```\n",
    "##### 2. DPCT1007: Migration of cudaGraphAddMemcpyNode is not supported.\n",
    "```\n",
    "cudaGraphAddMemcpyNode(&memcpyNode, graph, NULL, 0, &memcpyParams);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "The tf::syclFlow provides memcpy method to create a memcpy task that copies untyped data in bytes.\n",
    "\n",
    "```\n",
    "tf::syclTask inputVec_h2d = sf.memcpy(inputVec_d, inputVec_h, sizeof(float) * inputSize) .name(\"inputVec_h2d\");\n",
    "```\n",
    "\n",
    "##### Option 2 (using SYCL Graph):\n",
    "Command graph class includes `add` method which creates an empty node that contains no command. Its intended use is to make a connection point inside a graph between groups of nodes, and can significantly reduce the number of edges, using this we can add memcpy operation as a node.\n",
    "\n",
    "```\n",
    "auto nodecpy = graph.add([&](sycl::handler& h){\r\n",
    " h.memcpy(inputVec_d, inputVec_h, sizeof(float) * inputSize);\r\n",
    "});\n",
    "```\n",
    "\n",
    "##### 3. DPCT1007: Migration of cudaGraphAddMemsetNode is not supported.\n",
    "```\n",
    "cudaGraphAddMemsetNode(&memsetNode, graph, NULL, 0, &memsetParams);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "The tf::syclFlow::memset method creates a memset task that fills untyped data with a byte value.\n",
    "```\n",
    "tf::syclTask outputVec_memset = sf.memset(outputVec_d, 0, numOfBlocks * sizeof(double)) .name(\"outputVecd_memset\");\n",
    "```\n",
    "\n",
    "For more information on memory operations refer [here](https://github.com/taskflow/taskflow/blob/master/taskflow/sycl/syclflow.hpp).\r\n",
    "\n",
    "##### Option 2 (using SYCL Graph):\n",
    "Similar to memcpy node, memset operation can also be included as a node through the command graph add method\n",
    "\n",
    "```\n",
    "auto nodememset1 = graph.add([&](sycl::handler& h){\r\n",
    " h.fill(outputVec_d, 0, numOfBlocks);\r\n",
    "});\n",
    "```\n",
    "\n",
    "##### 4. DPCT1007: Migration of cudaGraphAddKernelNode is not supported.\n",
    "```\n",
    "cudaGraphAddKernelNode(&kernelNode, graph, nodeDependencies.data(),\r\n",
    "                         nodeDependencies.size(), &kernelNodeParams);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "The tf::syclFlow::on creates a task to launch the given command group function object and tf::syclFlow::parallel_for creates a kernel task from a parallel_for method through the handler object associated with a command group. The SYCL runtime schedules command group function objects from an out-of-order queue and constructs a task graph based on submitted events.\n",
    "\n",
    "```\n",
    "tf::syclTask reduce_kernel = sf.on([=] (sycl::handler& cgh){\r\n",
    "  sycl::local_accessor<double, 1> tmp(sycl::range<1>(THREADS_PER_BLOCK), cgh);\r\n",
    "  cgh.parallel_for(sycl::nd_range<3>{sycl::range<3>(1, 1, numOfBlocks) *\r\n",
    "                            sycl::range<3>(1, 1, THREADS_PER_BLOCK), sycl::range<3>(1, 1, THREADS_PER_BLOCK)}, [=](sycl::nd_item<3> item_ct1)[[intel::reqd_sub_group_size(SUB_GRP_SIZE)]]\r\n",
    "                  {\r\n",
    "                    reduce(inputVec_d, outputVec_d, inputSize, numOfBlocks, item_ct1, tmp.get_pointer());\r\n",
    "                  });\r\n",
    "    }).name(\"reduce_kernel\");\n",
    "```\n",
    "\n",
    "##### Option 2 (using SYCL Graph):\n",
    "Kernel operations are also included as a node through the command graph `add` method. These commands are captured into the graph and executed asynchronously when the graph is submitted to a queue. The `property::node::depends_on` property can be passed here with a list of nodes to create dependency edges on.\n",
    "\n",
    "```\n",
    "auto nodek1 = graph.add([&](sycl::handler &cgh) {\r\n",
    "sycl::local_accessor<double, 1> tmp_acc_ct1(\r\n",
    " sycl::range<1>(THREADS_PER_BLOCK), cgh);\r\n",
    "\r\n",
    "cgh.parallel_for(\r\n",
    " sycl::nd_range<3>(sycl::range<3>(1, 1, numOfBlocks) *\r\n",
    "                       sycl::range<3>(1, 1, THREADS_PER_BLOCK),\r\n",
    "                   sycl::range<3>(1, 1, THREADS_PER_BLOCK)),\r\n",
    " [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {\r\n",
    "   reduce(inputVec_d, outputVec_d, inputSize, numOfBlocks, item_ct1,\r\n",
    "          tmp_acc_ct1.get_pointer());\r\n",
    " });\r\n",
    "},  sycl_ext::property::node::depends_on(nodecpy, nodememset1));\n",
    "```\n",
    "\n",
    "##### 5. DPCT1007: Migration of cudaGraphAddHostNode is not supported.\n",
    "```\n",
    "cudaGraphAddHostNode(&hostNode, graph, nodeDependencies.data(),   nodeDependencies.size(), &hostParams);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "The tf::syclFlow doesn’t have a host method to run the callable on the host, instead, we can achieve this by creating a subflow graph since Taskflow supports dynamic tasking and runs the callable on the host.\n",
    "\n",
    "```\n",
    "tf::Task syclHostTask = tflow.emplace([&](){\r\n",
    "  myHostNodeCallback(&hostFnData);\r\n",
    "}).name(\"syclHostTask\");\r\n",
    "syclHostTask.succeed(syclKernelTask);\n",
    "```\n",
    "\n",
    "The task dependencies are established through precede or succeed, here syclHostTask runs after syclKernelTask.\n",
    "\n",
    "##### 6. DPCT1007: Migration of cudaGraphGetNodes is not supported.\n",
    "```\n",
    "cudaGraphGetNodes(graph, nodes, &numNodes);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "CUDA graph nodes are equivalent to SYCL tasks, both tf::Taskflow and tf::syclFlow classes include num_tasks() function to query the total number of tasks.\n",
    "\n",
    "```\n",
    "sf_Task = sf.num_tasks();\n",
    "```\n",
    "\n",
    "##### 7. DPCT1007: Migration of cudaGraphInstantiate is not supported.\n",
    "```\n",
    "cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "SYCL Task graph doesn’t need to be instantiated before executing but needs to establish the task dependencies using precede and succeed.\n",
    "\n",
    "```\n",
    "reduce_kernel.succeed(inputVec_h2d, outputVec_memset).precede(reduceFinal_kernel);\r\n",
    "reduceFinal_kernel.succeed(resultd_memset).precede(result_d2h);\n",
    "```\n",
    "\n",
    "The inputVec_h2d and outputVec_memset tasks run parallelly followed by the reduce_kernel task.\n",
    "\n",
    "##### Option 2 (using SYCL Graph):\n",
    "After all the operations are added as a node the graph is finalized using `finalize()` so that no more nodes can be added and creates an executable graph that can be submitted for execution\n",
    "```\n",
    "auto exec_graph = graph.finalize();\r\n",
    "sycl::queue qexec = sycl::queue{sycl::gpu_selector_v, \r\n",
    " {sycl::ext::intel::property::queue::no_immediate_command_list()}};\n",
    "```\n",
    "\n",
    "##### 8. DPCT1007: Migration of cudaGraphClone is not supported.\n",
    "```\n",
    "cudaGraphClone(&clonedGraph, graph);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "In SYCL, no clone function is available as Taskflow graph objects are move-only. We can use the std::move() function as shown below to achieve functionality.\n",
    "\n",
    "```\n",
    "tf::Taskflow tflow_clone(std::move(tflow));\n",
    "```\n",
    "\n",
    "This will construct a taskflow tflow_clone from moved taskflow tflow, and taskflow tflow becomes empty. For more information refer [here](https://taskflow.github.io/taskflow/classtf_1_1Taskflow.html#afd790de6db6d16ddf4729967c1edebb5).\n",
    "\n",
    "##### 9. DPCT1007: Migration of cudaGraphLaunch is not supported.\n",
    "```\n",
    "for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {\r\n",
    "  cudaGraphLaunch(graphExec, streamForGraph); }\r\n",
    "```\n",
    "##### Option 1 (using Taskflow): \n",
    "A taskflow graph can be run once or multiple times using an executor. run_n() will run the taskflow the number of times specified by the second argument.\n",
    "\n",
    "```\n",
    "exe.run_n(tflow, GRAPH_LAUNCH_ITERATIONS).wait();\n",
    "```\n",
    "\n",
    "##### Option 2 (using SYCL Graph):\n",
    "The graph is submitted in its entirety for execution via `handler::ext_oneapi_graph(graph)`.\n",
    "\n",
    "```\n",
    "for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {\r\n",
    "qexec.submit([&](sycl::handler& cgh) {\r\n",
    " cgh.ext_oneapi_graph(exec_graph);\r\n",
    "}).wait();\n",
    "```\n",
    "\n",
    "##### 10. DPCT1007: Migration of cudaGraphExecDestroy is not supported.\n",
    "```\n",
    "cudaGraphExecDestroy(graphExec);\r\n",
    "cudaGraphDestroy(graph);\n",
    "```\n",
    "\n",
    "##### Option 1 (using Taskflow): \n",
    "tf::Taskflow class has default destructor operators for both tf::executor and tf::taskflow objects created.\n",
    "\n",
    "```\n",
    "~Executor() \r\n",
    "~Taskflow()\n",
    "```\n",
    "\n",
    "To ensure that all the taskflow submissions are completed before calling the destructor, we must use wait() during the execution.\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: The SYCL Task Graph Programming Model, syclFlow, leverages the out-of-order property of the SYCL queue to design a simple and efficient scheduling algorithm using topological sort. SYCL can be slower than CUDA graphs because of execution overheads. Hence we prefer migrating with SYCL Graph Extension.</p>\n",
    "\n",
    "Below is the manual migration using SYCL graph extension for cudaGraphsUsingStreamCapture() method.\n",
    "\n",
    "##### 11. DPCT1027: The call to cudaStreamBeginCapture was replaced with 0 because SYCL currently does not support capture operations on queues.\n",
    "```\n",
    "cudaStreamBeginCapture(stream1, cudaStreamCaptureModeGlobal);\r",
    "(cudaStreamEndCapture(stream1, &grap));\n",
    "```\n",
    "\n",
    "The Queue Recording API (Record & Replay) captures command-groups submitted to a queue and records them in a graph. The command_graph::begin_recording and command_graph::end_recording entry-points return a bool value informing the user whether a related queue state change occurred. All the operation are placed in between these queue-recording APIs.\n",
    "\n",
    "```\n",
    "sycl_ext::command_graph graph(q.get_context(), q.get_device());\r\n",
    " graph.begin_recording(q);\r\n",
    " ..\r\n",
    " graph.end_recording();\n",
    "```\n",
    "\n",
    "##### 12. The memcpy, memset, and kernel operations are placed as a node via `sycl::event` namespace as follows\n",
    "```\n",
    "     sycl::event ememcpy = q.memcpy(inputVec_d, inputVec_h, sizeof(float) * inputSize);\r\n",
    "\r\n",
    "     sycl::event ememset = q.fill(outputVec_d, 0, numOfBlocks);\r\n",
    "\r\n",
    "     sycl::event ek1 = q.submit([&](sycl::handler &cgh) {\r\n",
    "     cgh.depends_on({ememcpy, ememset});\r\n",
    "     sycl::local_accessor<double, 1> tmp_acc_ct1(\r\n",
    "       sycl::range<1>(THREADS_PER_BLOCK), cgh);\r\n",
    "\r\n",
    "     cgh.parallel_for(\r\n",
    "      sycl::nd_range<3>(sycl::range<3>(1, 1, numOfBlocks) *\r\n",
    "                            sycl::range<3>(1, 1, THREADS_PER_BLOCK),\r\n",
    "                        sycl::range<3>(1, 1, THREADS_PER_BLOCK)),\r\n",
    "      [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {\r\n",
    "        reduce(inputVec_d, outputVec_d, inputSize, numOfBlocks, item_ct1,\r\n",
    "               tmp_acc_ct1.get_pointer());\r\n",
    "\n",
    "```\n",
    "\n",
    "##### 13. DPCT1007: Migration of cudaGraphInstantiate is not supported.\n",
    "```\n",
    "   cudaGraphInstantiate(&clonedGraphExec, clonedGraph, NULL, NULL, 0);\n",
    "```\n",
    "\n",
    "Similar to Graph explicit API calls, After all the operations are added as a node the graph is finalized using `finalize()` so that no more nodes can be added and creates an executable graph that can be submitted for execution. \n",
    "```\n",
    "   auto exec_graph = graph.finalize();\r\n",
    "   sycl::queue qexec = sycl::queue{sycl::gpu_selector_v, \r\n",
    "      {sycl::ext::intel::property::queue::no_immediate_command_list()};\n",
    "```\n",
    "\n",
    "##### 14. DPCT1007:Migration of cudaGraphLaunch is not supported.\n",
    "```\n",
    "   cudaGraphLaunch(clonedGraphExec, streamForGraph);\n",
    "```\n",
    "\n",
    "The graph is then submitted for execution via `handler::ext_oneapi_graph(graph)`.\n",
    "\n",
    "```\n",
    "   for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {\r\n",
    "      qexec.submit([&](sycl::handler& cgh) {\r\n",
    "        cgh.ext_oneapi_graph(exec_graph);\r\n",
    "      }).wait();\r\n",
    "```\n",
    "\n",
    "##### 15. CUDA code includes a custom API `findCUDADevice` in helper_cuda file to find the best CUDA Device available.\n",
    "```\n",
    "    findCudaDevice (argc, (const char **) argv);\n",
    "```\n",
    "\n",
    "Since it is a custom API SYCLomatic tool will not act on it and we can either remove it or replace it with the `sycl get_device()` API \n",
    "### Compile and Run the migrated SYCL source\n",
    "\n",
    "Once you have successfully migrated the CUDA source to the SYCL source, verify that the migrated SYCL code is functioning correctly by compiling and running it on the Intel Developer Cloud, which has a variety of Intel CPUs and GPUs available for development.\n",
    "\n",
    "#### Build and Run sycl_migrated_option1\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33bcac-783a-484b-a98a-dd0c405485d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sycl_migrated_option1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55aeac3-d59e-4c7d-9e28-e08d48296b79",
   "metadata": {},
   "source": [
    "#### Build and Run sycl_migrated_option2\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86faf220-3f01-481e-9508-04ca2fcaff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sycl_migrated_option2.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "657b9672-39ae-4193-8af9-103b58667db4",
   "metadata": {},
   "source": [
    "### SYCL Code Migration Analysis\n",
    "\n",
    "When comparing the CUDA code and migrated SYCL code, we can see that there are some 1:1 equivalent calls, which are listed below in the tables.\n",
    "\n",
    "1:1 equivalent mapping for Graph Explicit APIs table:\n",
    "\n",
    "| Functionality| CUDA| SYCL Taskflow| SYCL Graph\n",
    "|-|-|-|-\n",
    "| Header file| `#include <cuda_runtime.h>`| `#include <sycl/sycl.hpp>` <br> `#include <dpct/dpct.hpp>` <br> `#include <taskflow/sycl/syclflow.hpp>`| `#include <sycl/sycl.hpp>` <br> `#include <dpct/dpct.hpp>`\n",
    "| Create Graph| `cudaGraphCreate(&graph, 0);`| `tf::Taskflow tflow; tf::Executor exe;`| `namespace sycl_ext = sycl::ext::oneapi::experimental; sycl_ext::command_graph graph(q.get_context(), q.get_device());`\n",
    "| Add nodes to Graph| `cudaGraphAddKernelNode(&kernelNode, graph, nodeDependencies.data(),nodeDependencies.size(), &kernelNodeParams);`| `tf::syclTask reduce_kernel = sf.on([=] (sycl::handler& cgh){ cgh.parallel_for( … ); }).name(\"reduce_kernel\");`| `auto nodek1 = graph.add([&](sycl::handler &cgh) { cgh.parallel_for( … ); },  sycl_ext::property::node::depends_on(nodecpy, nodememset));`\n",
    "| Finalize Graph| `cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);`| `reduce_kernel.succeed(inputVec_h2d, outputVec_memset).precede(reduceFinal_kernel); reduceFinal_kernel.succeed(resultd_memset).precede(result_d2h);`| `auto exec_graph = graph.finalize(); sycl::queue qexec = sycl::queue{sycl::gpu_selector_v,{sycl::ext::intel::property::queue::no_immediate_command_list()}};`\n",
    "| Submit Graph to Queue| `for (int i = 0; i < 3; i++) { cudaGraphLaunch(graphExec, streamForGraph); }`| `exe.run_n(tflow, GRAPH_LAUNCH_ITERATIONS).wait();`| `for (int i = 0; i < 3; i++) {  qexec.submit([&](sycl::handler& cgh) { cgh.ext_oneapi_graph(exec_graph);  }).wait();}`\n",
    "\n",
    "\n",
    "1:1 equivalent mapping for Graph StreamCapture APIs table:\n",
    "\n",
    "| Functionality| CUDA| SYCL Graph\n",
    "|-|-|-\n",
    "| Create Graph| `cudaGraphCreate(&graph, 0); cudaStreamCreate(&stream1);`| `namespace sycl_ext = sycl::ext::oneapi::experimental; sycl_ext::command_graph graph(q.get_context(), q.get_device());`\n",
    "| Begin Record| `cudaStreamBeginCapture(stream1, cudaStreamCaptureModeGlobal);`| `graph.begin_recording(q);`\n",
    "| Add nodes to Graph| `cudaMemcpyAsync(&result_h, result_d, sizeof(double),cudaMemcpyDefault, stream1);`| `q.submit([&](sycl::handler &cgh) {cgh.depends_on(ek2); cgh.memcpy(&result_h, result_d, sizeof(double)); });`\n",
    "| End Record| `cudaStreamEndCapture(stream1, &graph);`| `graph.end_recording();`\n",
    "| Finalize Graph| `cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);`| `auto exec_graph = graph.finalize(); sycl::queue qexec = sycl::queue{sycl::gpu_selector_v,{sycl::ext::intel::property::queue::no_immediate_command_list()}};`\n",
    "| Submit Graph to Queue| `for (int i = 0; i < 3; i++) { cudaGraphLaunch(graphExec, streamForGraph); }`| `for (int i = 0; i < 3; i++) {  qexec.submit([&](sycl::handler& cgh) { cgh.ext_oneapi_graph(exec_graph);  }).wait();}`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a1da6-16a9-4cca-9e46-12ae431d04c4",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "This section describes the location of the CUDA source and the contents of different SYCL source code directories in this project.\n",
    "\n",
    "| folder name | source code description\n",
    "| --- | ---\n",
    "| [CUDA github](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/3_CUDA_Features/simpleCudaGraphs) | Original CUDA Source used for migration\n",
    "| dpct_output | Contains output of SYCLomatic Tool used to migrate SYCL-compliant code from CUDA code. This SYCL code has some unmigrated code that must be manually fixed to get full functionality. (The code does not functionally work as generated.)\n",
    "| sycl_migrated_option1 | Contains manually migrated SYCL code from CUDA code using Taskflow programming model.\n",
    "| sycl_migrated_option2 | Contains manually migrated SYCL code from CUDA code using SYCL Graph extension.\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: In the first approach(sycl_migrated__option1) we only migrate the cudaGraphsManual() method using Taskflow Programming Model. We do not migrate cudaGraphsUsingStreamCapture() because CUDA Stream Capture APIs are not yet supported in SYCL through Taskflow.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5f463-190f-4ebd-9253-505afa1d5046",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module we have learnt how to migrate simple CUDA source to SYCL source to get functionality using `SYCLomatic` and then analized/optimized the SYCL source by manually coding. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
