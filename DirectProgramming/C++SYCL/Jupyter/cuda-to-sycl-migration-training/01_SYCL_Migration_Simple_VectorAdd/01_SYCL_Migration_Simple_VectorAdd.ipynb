{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726797c2-06bd-4fbe-88b0-734adc409770",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SYCL Migration - Simple VectorAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada1808-d73a-4d96-a1bf-9dccdab7747b",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [Introduction](#Introduction)\n",
    "- [Analyze CUDA Source](#Analyze-CUDA-Source)\n",
    "- [Migrate CUDA source to SYCL source](#Migrate-CUDA-source-to-SYCL-source)\n",
    "- [Analyze, Compile and Run the migrated SYCL source](#Analyze,-Compile-and-Run-the-migrated-SYCL-source)\n",
    "- [Manually Optimize the migrated SYCL source](#Manually-Optimize-the-migrated-SYCL-source)\n",
    "- [Source Code](#Source-Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fbb8d-244b-4f90-9de9-3846df15afc1",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Use SYCLomatic Tool to migrate a simple single source CUDA application\n",
    "* Use various command line options of `SYCLomatic` for CUDA to SYCL migration\n",
    "* Compile and run migrated SYCL code on Intel CPUs and GPUs\n",
    "* Optimize the migrated SYCL code with manual coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e6c1f-95ee-419d-8149-6a7789998ef0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This module will walk you through migrating CUDA code to SYCL code using the SYCLomatic Tool.\n",
    "\n",
    "#### Requirements\n",
    "1. NVidia CUDA development machine\n",
    "2. Development machine with Intel CPU/GPU OR a Intel Developer Cloud account\n",
    "\n",
    "#### Migration Process\n",
    "We will do the following steps in this hands-on workshop:\n",
    "- Analyze CUDA source\n",
    "- Migrate CUDA source to SYCL source\n",
    "- Analyze, Compile and Run the migrated SYCL source\n",
    "- Manually Optimize the migrated SYCL source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5aa043-0d1f-43df-9a9a-e38f13e4bc86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyze CUDA Source\n",
    "\n",
    "Below is simple VectorAdd CUDA source: `vectoradd.cu`\n",
    "\n",
    "```cpp\n",
    "\n",
    "#include <cuda.h>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#define N 16\n",
    "\n",
    "//# kernel code to perform VectorAdd on GPU\n",
    "__global__ void VectorAddKernel(float* A, float* B, float* C)\n",
    "{\n",
    "        C[threadIdx.x] = A[threadIdx.x] + B[threadIdx.x];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "        //# Initialize vectors on host\n",
    "        float A[N] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};\n",
    "        float B[N] = {2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2};\n",
    "        float C[N] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "\n",
    "        //# Allocate memory on device\n",
    "        float *d_A, *d_B, *d_C;\n",
    "        cudaMalloc(&d_A, N*sizeof(float));\n",
    "        cudaMalloc(&d_B, N*sizeof(float));\n",
    "        cudaMalloc(&d_C, N*sizeof(float));\n",
    "\n",
    "        //# copy vector data from host to device\n",
    "        cudaMemcpy(d_A, A, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(d_B, B, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "        //# sumbit task to compute VectorAdd on device\n",
    "        VectorAddKernel<<<1, N>>>(d_A, d_B, d_C);\n",
    "\n",
    "        //# copy result of vector data from device to host\n",
    "        cudaMemcpy(C, d_C, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        //# print result on host\n",
    "        for (int i = 0; i < N; i++) std::cout<< C[i] << \" \";\n",
    "        std::cout << \"\\n\";\n",
    "\n",
    "        //# free allocation on device\n",
    "        cudaFree(d_A);\n",
    "        cudaFree(d_B);\n",
    "        cudaFree(d_C);\n",
    "        return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "The CUDA code above initializes three arrays: `A`, `B` and `C`. The code allocates memory on device for three arrays: `d_A`, `d_B` and `d_C` using the `cudaMalloc` function, and then copies host initialized arrays to device locations using `cudaMemcpy`. The kernel function `VectorAddKernel` is then called to add arrays `d_A` and `d_B` into `d_C`. The results from `d_C` are copied back to host using `cudaMemcpy` to print the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495ba6e-e388-4270-87ee-72a669b1aef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Migrate CUDA source to SYCL source\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: A CUDA development machine is required to accomplish the task in this section </p>\n",
    "\n",
    "Now that we have analyzed the CUDA source, we will migrate the CUDA source into SYCL source using the __SYCLomatic Tool__.\n",
    "\n",
    "For relatively simple projects, the SYCLomatic Tool can be invoked on the user source CUDA code directly. In this exercise, we will walk you through step-by-step to migrate the CUDA code.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Make sure you have a __NVIDIA CUDA development machine__ that can __compile and run CUDA code__. The next step is to install the tools for migrating CUDA to SYCL:\n",
    "\n",
    "- Install SYCLomatic Tool on this machine\n",
    "  - go to https://github.com/oneapi-src/SYCLomatic/releases/\n",
    "  - copy link to latest `linux_release.tgz` from assets\n",
    "  - on the CUDA development machine: `mkdir syclomatic; cd syclomatic`\n",
    "  - `wget <link to linux_release.tgz>`\n",
    "  - `tar -xvf linux_release.tgz`\n",
    "  - `export PATH=\"/home/$USER/syclomatic/bin:$PATH\"`\n",
    "  - Verify installation: `c2s --version`\n",
    "- Create a working directory and copy the above `vectoradd.cu` CUDA source to this machine\n",
    "\n",
    "\n",
    "### Migrate CUDA source to SYCL source using SYCLomatic\n",
    "\n",
    "On the NVIDIA CUDA development machine, run the following command to migrate CUDA code to SYCL code:\n",
    "\n",
    "```\n",
    "c2s vectoradd.cu --gen-helper-function\n",
    "```\n",
    "\n",
    "This command should migrate the CUDA source to SYCL source in a folder named `dpct_output/` by default, and the folder will have the SYCL source with name `vectoradd.dp.cpp`\n",
    "\n",
    "`--gen-helper-function` option will copy the SYCLomatic helper headers filed to output directory\n",
    "\n",
    "Note that when running the tool again for the same source may throw error that `dpct_output` folder is not empty, so make sure to delete the `dpct_output` folder and then try the `c2s vectoradd.cu --gen-helper-function` command.\n",
    "\n",
    "Next we will use the `c2s --out-root` option to specify a custom output directory like shown below:\n",
    "\n",
    "```\n",
    "c2s vectoradd.cu --gen-helper-function --out-root sycl_code\n",
    "```\n",
    "\n",
    "This command should migrate the CUDA source to SYCL source in a folder named `sycl_code`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d03f96-4159-401a-a2e5-97b0548ba477",
   "metadata": {},
   "source": [
    "## Analyze, Compile and Run the migrated SYCL source\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: The tasks in this section should be done on Intel DevCloud or on a system with oneAPI Base toolkit installed.</p>\n",
    "\n",
    "Once you have successfully migrated the CUDA source to the SYCL source, verify that the migrated SYCL code is functioning correctly by compiling and running it on a system with an Intel CPU/GPU. Alternatively, you can do this on the Intel Developer Cloud, which has a variety of Intel CPUs and GPUs available for development.\n",
    "\n",
    "Let's look at the migrated SYCL code:\n",
    "\n",
    "```cpp\n",
    "//==============================================================\n",
    "// Copyright Â© Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <dpct/dpct.hpp>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#define N 16\n",
    "\n",
    "//# kernel code to perform VectorAdd on GPU\n",
    "void VectorAddKernel(float* A, float* B, float* C,\n",
    "                     const sycl::nd_item<3> &item_ct1)\n",
    "{\n",
    "        C[item_ct1.get_local_id(2)] =\n",
    "            A[item_ct1.get_local_id(2)] + B[item_ct1.get_local_id(2)];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "        dpct::device_ext &dev_ct1 = dpct::get_current_device();\n",
    "        sycl::queue &q_ct1 = dev_ct1.default_queue();\n",
    "        std::cout << \"Device: \" << q_ct1.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "        //# Initialize vectors on host\n",
    "        float A[N] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};\n",
    "        float B[N] = {2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2};\n",
    "        float C[N] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "\n",
    "        //# Allocate memory on device\n",
    "        float *d_A, *d_B, *d_C;\n",
    "        d_A = sycl::malloc_device<float>(N, q_ct1);\n",
    "        d_B = sycl::malloc_device<float>(N, q_ct1);\n",
    "        d_C = sycl::malloc_device<float>(N, q_ct1);\n",
    "\n",
    "        //# copy vector data from host to device\n",
    "        q_ct1.memcpy(d_A, A, N * sizeof(float));\n",
    "        q_ct1.memcpy(d_B, B, N * sizeof(float)).wait();\n",
    "\n",
    "        //# sumbit task to compute VectorAdd on device\n",
    "        q_ct1.parallel_for(\n",
    "            sycl::nd_range<3>(sycl::range<3>(1, 1, N), sycl::range<3>(1, 1, N)),\n",
    "            [=](sycl::nd_item<3> item_ct1) {\n",
    "                    VectorAddKernel(d_A, d_B, d_C, item_ct1);\n",
    "            });\n",
    "\n",
    "        //# copy result of vector data from device to host\n",
    "        q_ct1.memcpy(C, d_C, N * sizeof(float)).wait();\n",
    "\n",
    "        //# print result on host\n",
    "        for (int i = 0; i < N; i++) std::cout<< C[i] << \" \";\n",
    "        std::cout << \"\\n\";\n",
    "\n",
    "        //# free allocation on device\n",
    "        sycl::free(d_A, q_ct1);\n",
    "        sycl::free(d_B, q_ct1);\n",
    "        sycl::free(d_C, q_ct1);\n",
    "        return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "The migrated SYCL code can be compiled using the following command in terminal:\n",
    "```sh\n",
    "icpx -fsycl -I include vectoradd.dp.cpp\n",
    "```\n",
    "\n",
    "OR you can compile and run by executing the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90192a10-c652-439f-bdec-05746d0eea62",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run â¶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cee9e-52e2-43e0-9ac4-7748b27562e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_vector_add.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b9672-39ae-4193-8af9-103b58667db4",
   "metadata": {},
   "source": [
    "### SYCL Code Migration Analysis\n",
    "\n",
    "When comparing the CUDA code and migrated SYCL code, we can see that there are some 1:1 equivalent calls, which are listed below in the table:\n",
    "\n",
    "| Functionality|CUDA|SYCL\n",
    "|-|-|-\n",
    "| header file|`#include <cuda.h>`|`#include <CL/sycl.hpp>`\n",
    "| Memory allocation on device| `cudaMalloc(&d_A, N*sizeof(float))`| `d_A = sycl::malloc_device<float>(N, q_ct1)`\n",
    "| Copy memory between host and device| `cudaMemcpy(d_A, A, N*sizeof(float), cudaMemcpyHostToDevice)`| `q.memcpy(d_A, A, N * sizeof(float))`\n",
    " | Free device memory allocation| `cudaFree(d_A)` | `free(d_A, q)`\n",
    "\n",
    "The actual kernel function invocation is different. In CUDA, the kernel function is invoked with the execution configuration syntax `<<<1, N>>>>` as follows, specifying 1 block and N threads:\n",
    "\n",
    "```cpp\n",
    "VectorAddKernel<<<1, N>>>(d_A, d_B, d_C);\n",
    "```\n",
    "\n",
    "In SYCL, the kernel function is invoked using `parallel_for` and specifying `nd_range` with one N work item in one work group, as follows:\n",
    "\n",
    "\n",
    "```cpp\n",
    "q_ct1.parallel_for(\n",
    "            sycl::nd_range<3>(sycl::range<3>(1, 1, N), sycl::range<3>(1, 1, N)),\n",
    "            [=](sycl::nd_item<3> item_ct1) {\n",
    "                    VectorAddKernel(d_A, d_B, d_C, item_ct1);\n",
    "            });\n",
    "```\n",
    "\n",
    "Another difference is that the SYCL requires creating a SYCL queue with a device selector and other optional properties. The queue is used to submit the command group to execute on the device. The creation of a SYCL queue is necessary and is done as follows in the SYCL migrated code using some helper functions:\n",
    "\n",
    "```cpp\n",
    "dpct::device_ext &dev_ct1 = dpct::get_current_device();\n",
    "sycl::queue &q_ct1 = dev_ct1.default_queue();\n",
    "```\n",
    "\n",
    "In CUDA, the equivalent is a CUDA stream; if no stream is created in the CUDA code, a default stream is implicitly created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72a050-8964-42c9-8a83-1d67f9d30fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manually Optimize the migrated SYCL source\n",
    "\n",
    "The SYCLomatic Tool will migrate the CUDA code to the SYCL code to get functionality, but you may have to manually optimize the resulting SYCL code for optimal performance.\n",
    "\n",
    "Now that we have successfully migrated the CUDA code to the SYCL code and executed on an Intel CPU/GPU, letâs look at what manual optimizations we can do.\n",
    "\n",
    "Analyzing the migrated SYCL code, we can see that a SYCL queue is created using the following code:\n",
    "\n",
    "```cpp\n",
    "dpct::device_ext &dev_ct1 = dpct::get_current_device();\n",
    "sycl::queue &q_ct1 = dev_ct1.default_queue();\n",
    "```\n",
    "\n",
    "The above code is creating a SYCL queue using dpct helper functions that can be unwrapped using the `dpct/dpct.hpp` header file.\n",
    "\n",
    "The above code is also creating a SYCL queue with an `in_order` queue property and is doing a default device selection, which is the same as the code below using just SYCL api syntax:\n",
    "\n",
    "```cpp\n",
    "sycl::queue q_ct1{sycl::default_selector_v(), sycl::property::queue::in_order()};\n",
    "```\n",
    "\n",
    "Using an `in_order` queue property will not allow kernels with no dependency to overlap execution. Therefore, we will remove the `in_order` queue property and add event-based dependency between kernels.\n",
    "\n",
    "We can replace the SYCL queue creation with the following code:\n",
    "\n",
    "```cpp\n",
    "sycl::queue q_ct1;\n",
    "```\n",
    "\n",
    "This will create a queue with default device selection and allow kernels to overlap.\n",
    "\n",
    "The next step is to add kernel dependency. From the code above we can enable the two `memcpy` kernel submissions to overlap and then add dependency for the actual kernel that does the vector add. We will also add a dependency to the final `memcpy` kernel to copy back the results.\n",
    "\n",
    "The resulting optimized code will look like this:\n",
    "\n",
    "```cpp\n",
    "//==============================================================\n",
    "// Copyright Â© Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#define N 16\n",
    "\n",
    "//# kernel code to perform VectorAdd on GPU\n",
    "void VectorAddKernel(float* A, float* B, float* C,\n",
    "                     const sycl::nd_item<3> &item_ct1)\n",
    "{\n",
    "        C[item_ct1.get_local_id(2)] =\n",
    "            A[item_ct1.get_local_id(2)] + B[item_ct1.get_local_id(2)];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "        // sycl queue with out of order execution allowed\n",
    "        sycl::queue q_ct1;\n",
    "        std::cout << \"Device: \" << q_ct1.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "        //# Initialize vectors on host\n",
    "        float A[N] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};\n",
    "        float B[N] = {2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2};\n",
    "        float C[N] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "\n",
    "        //# Allocate memory on device\n",
    "        float *d_A, *d_B, *d_C;\n",
    "        d_A = sycl::malloc_device<float>(N, q_ct1);\n",
    "        d_B = sycl::malloc_device<float>(N, q_ct1);\n",
    "        d_C = sycl::malloc_device<float>(N, q_ct1);\n",
    "\n",
    "        //# copy vector data from host to device\n",
    "        auto e1 = q_ct1.memcpy(d_A, A, N * sizeof(float));\n",
    "        auto e2 = q_ct1.memcpy(d_B, B, N * sizeof(float));\n",
    "\n",
    "        //# sumbit task to compute VectorAdd on device\n",
    "        auto e3 = q_ct1.parallel_for(\n",
    "            sycl::nd_range<3>(sycl::range<3>(1, 1, N), sycl::range<3>(1, 1, N)), {e1, e2},\n",
    "            [=](sycl::nd_item<3> item_ct1) {\n",
    "                    VectorAddKernel(d_A, d_B, d_C, item_ct1);\n",
    "            });\n",
    "\n",
    "        //# copy result of vector data from device to host\n",
    "        q_ct1.memcpy(C, d_C, N * sizeof(float), e3).wait();\n",
    "\n",
    "        //# print result on host\n",
    "        for (int i = 0; i < N; i++) std::cout<< C[i] << \" \";\n",
    "        std::cout << \"\\n\";\n",
    "\n",
    "        //# free allocation on device\n",
    "        sycl::free(d_A, q_ct1);\n",
    "        sycl::free(d_B, q_ct1);\n",
    "        sycl::free(d_C, q_ct1);\n",
    "        return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c295c42-592a-4d86-9b97-ed712c51733a",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run â¶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5bc2e-9414-4059-9fd4-de026929be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_vector_add_optimized.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44575ee2-383a-4783-9c03-3986de891cad",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "This section describes the location of the CUDA source and the contents of different SYCL source code directories in this project.\n",
    "\n",
    "| folder name | source code description\n",
    "| --- | ---\n",
    "| cuda | Original CUDA Source used for migration\n",
    "| dpct_output | SYCL migration output from SYCLomatic Tool, compiles without errors\n",
    "| sycl_migrated | SYCL code with code that prints the offload device name added, compiles without errors\n",
    "| sycl_migrated_optimized | SYCL code optimized to allow kernels for execute out-of-order, compiles without errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5f463-190f-4ebd-9253-505afa1d5046",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we learned how to migrate simple CUDA source to SYCL source to get functionality using the `SYCLomatic` Tool, and then analyzed/optimized the SYCL source by manually coding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (IntelÂ® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
