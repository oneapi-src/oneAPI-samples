{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726797c2-06bd-4fbe-88b0-734adc409770",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SYCL Migration - Sorting Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada1808-d73a-4d96-a1bf-9dccdab7747b",
   "metadata": {},
   "source": [
    "##### Sections\n",
    "- [Introduction](#Introduction)\n",
    "- [Analyze CUDA source](#Analyze-CUDA-source)\n",
    "- [Migrate CUDA source to SYCL source](#Migrate-CUDA-source-to-SYCL-source)\n",
    "- [Analyze, Compile and Run the migrated SYCL source](#Analyze,-Compile-and-Run-the-migrated-SYCL-source)\n",
    "- [Source Code](#Source-Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757fbb8d-244b-4f90-9de9-3846df15afc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Learning Objectives\n",
    "* Use SYCLomatic Tool to migrate a simple single source CUDA application\n",
    "* Use various command line options of `SYCLomatic` for CUDA to SYCL migration\n",
    "* Compile and run migrated SYCL code on Intel CPUs and GPUs\n",
    "* Optimize the migrated SYCL code with manual coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e6c1f-95ee-419d-8149-6a7789998ef0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This module will walk you through migrating CUDA code to SYCL code using SYCLomatic Tool\n",
    "\n",
    "#### Requirements\n",
    "1. NVIDIA CUDA development machine\n",
    "2. Development machine with Intel CPU/GPU OR a Intel Developer Cloud account\n",
    "\n",
    "#### Migration Process\n",
    "We will do the following steps in this hands-on workshop:\n",
    "- Analyze CUDA source\n",
    "- Migrate CUDA source to SYCL source\n",
    "- Analyze, Compile and Run the migrated SYCL source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5aa043-0d1f-43df-9a9a-e38f13e4bc86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyze CUDA source\n",
    "\n",
    "The CUDA source for \"Sorting Networks\" example is available on [Nvidia Github](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/2_Concepts_and_Techniques/sortingNetworks)\n",
    "\n",
    "Pull the entire repository on your CUDA Development machine.\n",
    "\n",
    "```\n",
    "git clone https://github.com/NVIDIA/cuda-samples.git\n",
    "\n",
    "cd cuda-samples/Samples/2_Concepts_and_Techniques/sortingNetworks\n",
    "```\n",
    "\n",
    "The CUDA source for the SortingNetworks implementation is in the following files.\n",
    "\n",
    "[__main.cpp__](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/2_Concepts_and_Techniques/sortingNetworks/main.cpp) — host code for:\n",
    "- identify GPU device\n",
    "- memory allocation on GPU\n",
    "- initialization of data on CPU\n",
    "- copy data to GPU memory for computation\n",
    "- kickoff computation on GPU\n",
    "- verify and print results\n",
    "\n",
    "[__OddEvenMergeSort.cu__, __bitonicSort.cu__](https://github.com/NVIDIA/cuda-samples/blob/master/Samples/2_Concepts_and_Techniques/sortingNetworks/OddEvenMergeSort.cu) — kernel code for sort computations that runs on GPU\n",
    "- define kernel\n",
    "- allocate shared local memory\n",
    "- OddEvenMergeSort computation\n",
    "\n",
    "The code also uses some helper functions to capture execution times of kernel which is available in `cuda-samples/Common/` folder in the repo:\n",
    "- `helper_cuda.h` \n",
    "- `helper_timer.h`\n",
    "- `helper_string.h`\n",
    "- `expections.h`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495ba6e-e388-4270-87ee-72a669b1aef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Migrate CUDA source to SYCL source\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: A CUDA development machine is required to accomplish the task in this section </p>\n",
    "\n",
    "Now that we have analyzed the CUDA source, we will next migrate the CUDA source into SYCL source using the __SYCLomatic Tool__.\n",
    "\n",
    "In this exercise, we will walk you though step-by-step to migrate CUDA code.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Make sure you have a __NVIDIA CUDA development machine__ that can __compile and run CUDA code__. The next step is to install the tools for migrating CUDA to SYCL:\n",
    "\n",
    "- Install SYCLomatic Tool on this machine\n",
    "  - go to https://github.com/oneapi-src/SYCLomatic/releases/\n",
    "  - copy link to latest `linux_release.tgz` from assets\n",
    "  - on the CUDA development machine: `mkdir syclomatic; cd syclomatic`\n",
    "  - `wget <link to linux_release.tgz>`\n",
    "  - `tar -xvf linux_release.tgz`\n",
    "  - `export PATH=\"/home/$USER/syclomatic/bin:$PATH\"`\n",
    "  - Verify installation: `c2s --version`\n",
    "- pull the CUDA samples repo to this machine\n",
    "  - `git clone https://github.com/NVIDIA/cuda-samples.git`\n",
    "- Compile and run the `sortingNetworks` sample\n",
    "  - `cd cuda-samples/Samples/2_Concepts_and_Techniques/sortingNetworks`\n",
    "  - `make`\n",
    "\n",
    "\n",
    "### Migrate CUDA source to SYCL source using SYCLomatic\n",
    "\n",
    "On the NVIDIA CUDA Development machine, go to the CUDA source folder and generate a compilation database with the tool `intercept-build`. This creates a JSON file with all the compiler invocations, stores the names of the input files and the compiler options.\n",
    "\n",
    "```\n",
    "make clean\n",
    "intercept-build make\n",
    "```\n",
    "\n",
    "This will create a file named `compile_commands.json` in the sample folder.\n",
    "\n",
    "Next, use the SYCLomatic Tool (c2s) to migrate the code; it will store the result in the migration folder `dpct_output`:\n",
    "\n",
    "```\n",
    "c2s -p compile_commands.json --in-root ../../.. --gen-helper-function\n",
    "```\n",
    "\n",
    "The `--gen-helper-function` option will copy the SYCLomatic helper header files to output directory.\n",
    "\n",
    "The `--in-root` option will specify the path for all the common include files for the CUDA project.\n",
    "\n",
    "This command should migrate the CUDA source to the C++ SYCL source in a folder named `dpct_output` by default, and the folder will have the C++ SYCL source along with any dependencies from the `Common` folder:\n",
    "\n",
    "- `main.cpp.dp.cpp`\n",
    "- `oddEvenMergeSort.dp.cpp`\n",
    "- `bitonicSort.dp.cpp`\n",
    "- `sortingNetworks_validate.cpp`\n",
    "- `sortingNetworks_common.dp.hpp`\n",
    "- `sortingNetworks_common.h`\n",
    "\n",
    "This command may also throw a bunch of warnings about the migration process. The CUDA code that cannot be automatically migrated will have warning comments generated in the migrated source files, which have to be manually migrated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097daea4-2b52-4241-9a45-d79f9095aa09",
   "metadata": {},
   "source": [
    "## Analyze, Compile and Run the migrated SYCL source\n",
    "\n",
    "<p style=\"background-color:#cdc\"> Note: The tasks in this section should be done on Intel DevCloud or on a system with oneAPI Base toolkit installed.</p>\n",
    "\n",
    "\n",
    "The migrated SYCL code are in the `Samples` folder under the `dpct_output` folder:\n",
    "- `main.cpp.dp.cpp`\n",
    "- `oddEvenMergeSort.dp.cpp`\n",
    "- `bitonicSort.dp.cpp`\n",
    "- `sortingNetworks_validate.cpp`\n",
    "- `sortingNetworks_common.dp.hpp`\n",
    "- `sortingNetworks_common.h`\n",
    "\n",
    "The `dpct_output` folder also has headers files needed for compiling the migrated SYCL code. The `Common` folder has header files with CUDA helper functions which are migrated to SYCL and the `include` folder has header files with SYCLomatic helper functions.\n",
    "\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "Make sure you have one of the following:\n",
    "- __Development machine with Intel CPU/GPU__ with Intel oneAPI Base Toolkit installed\n",
    "- __Intel Developer Cloud__ account to access the Intel CPUs/GPUs on the cloud\n",
    "\n",
    "### Compiling migrated SYCL code\n",
    "\n",
    "To compile the migrated SYCL code we can use the following command:\n",
    "```sh\n",
    "icpx -fsycl -I ../../../Common -I ../../../include *.cpp\n",
    "```\n",
    "\n",
    "There may be compile errors based on whether all of the CUDA code was migrated to SYCL or not. The migrated code may also include comments with warning messages, which could help make it easier to fix the errors. These errors have to be manually fixed to get the code to compile.\n",
    "\n",
    "### Fixing unmigrated SYCL code\n",
    "\n",
    "There are other migration warnings that can be looked at to see if any improvements can be made, below are couple examples of warning in `main.cpp.dp.cpp`:\n",
    "\n",
    "```cpp\n",
    "      /*\n",
    "      DPCT1065:1: Consider replacing sycl::nd_item::barrier() with\n",
    "      sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better\n",
    "      performance if there is no access to global memory.\n",
    "      */\n",
    "      item_ct1.barrier();\n",
    "```\n",
    "The above DPCT warning is a suggession to improve performance\n",
    "\n",
    "```cpp\n",
    "    /*\n",
    "    DPCT1049:4: The work-group size passed to the SYCL kernel may exceed the\n",
    "    limit. To get the device limit, query info::device::max_work_group_size.\n",
    "    Adjust the work-group size if needed.\n",
    "    */\n",
    "    q_ct1.submit([&](sycl::handler &cgh) {\n",
    "            sycl::nd_range<3>(\n",
    "                sycl::range<3>(1, 1, (batchSize * arrayLength) / 512) *\n",
    "                    sycl::range<3>(1, 1, 256),\n",
    "                sycl::range<3>(1, 1, 256)),\n",
    "            [=](sycl::nd_item<3> item_ct1) {\n",
    "              oddEvenMergeGlobal(d_DstKey, d_DstVal, d_DstKey, d_DstVal,\n",
    "                                 arrayLength, size, stride, dir, item_ct1);\n",
    "            });\n",
    "```\n",
    "The above DPCT warning suggesting you check the workgroup size limit for the device since it varies from device to device.\n",
    "\n",
    "### Compile and Run the migrated SYCL source\n",
    "\n",
    "Once you have successfully migrated the CUDA source to the SYCL source, verify that the migrated SYCL code is functioning correctly by compiling and running it on the Intel Developer Cloud, which has a variety of Intel CPUs and GPUs available for development.\n",
    "\n",
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cee9e-52e2-43e0-9ac4-7748b27562e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sycl_migrated.sh gen9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b9672-39ae-4193-8af9-103b58667db4",
   "metadata": {},
   "source": [
    "### SYCL Code Migration Analysis\n",
    "\n",
    "When comparing the CUDA code and migrated SYCL code, we can see that there are some 1:1 equivalent calls, which are listed below in the table:\n",
    "\n",
    "| Functionality|CUDA|SYCL\n",
    "|-|-|-\n",
    "| header file|`#include <cuda_runtime.h>`|`#include <CL/sycl.hpp>`\n",
    "| Memory allocation on device| `cudaMalloc((void **)&d_InputKey, N * sizeof(uint))` | `d_InputKey = sycl::malloc_device<uint>(N, q_ct1)`\n",
    "| Copy memory between host and device| `cudaMemcpy(d_InputKey, h_InputKey, N * sizeof(uint), cudaMemcpyHostToDevice)` | `q_ct1.memcpy(d_InputKey, h_InputKey, N * sizeof(uint))`\n",
    "| Free device memory allocation| `cudaFree(d_A)` | `sycl::free(d_A, q_ct1)`\n",
    "| Synchronize host and device | `cudaDeviceSynchronize()` | `dev_ct1.queues_wait_and_throw()`\n",
    "\n",
    "The actual kernel function invocation is different. In CUDA, the kernel function is invoked with the execution configuration syntax `<<<blockCount, threadCount>>>` as follows, specifying blocks and threads:\n",
    "\n",
    "```cpp\n",
    "bitonicSortShared<<<blockCount, threadCount>>>(d_DstKey, d_DstVal, d_SrcKey,\n",
    "                                                   d_SrcVal, arrayLength, dir);\n",
    "```\n",
    "\n",
    "In SYCL, the kernel function is invoked using `parallel_for` and specifying `nd_range` with global size and work group size, as follows:\n",
    "```cpp\n",
    "    q_ct1.submit([&](sycl::handler &cgh) {\n",
    "      sycl::accessor<uint, 1, sycl::access_mode::read_write,\n",
    "                     sycl::access::target::local>\n",
    "          s_key_acc_ct1(sycl::range<1>(1024 /*1024U*/), cgh);\n",
    "      sycl::accessor<uint, 1, sycl::access_mode::read_write,\n",
    "                     sycl::access::target::local>\n",
    "          s_val_acc_ct1(sycl::range<1>(1024 /*1024U*/), cgh);\n",
    "\n",
    "      cgh.parallel_for(sycl::nd_range<3>(sycl::range<3>(1, 1, blockCount) *\n",
    "                                             sycl::range<3>(1, 1, threadCount),\n",
    "                                         sycl::range<3>(1, 1, threadCount)),\n",
    "                       [=](sycl::nd_item<3> item_ct1) {\n",
    "                         bitonicSortShared(d_DstKey, d_DstVal, d_SrcKey,\n",
    "                                           d_SrcVal, arrayLength, dir, item_ct1,\n",
    "                                           s_key_acc_ct1.get_pointer(),\n",
    "                                           s_val_acc_ct1.get_pointer());\n",
    "                       });\n",
    "    });\n",
    "```\n",
    "\n",
    "Another difference is that the SYCL requires creating a SYCL queue with a device selector and other optional properties. The queue is used to submit the command group to execute on the device. The creation of a SYCL queue is necessary and is done as follows in the SYCL migrated code using some helper functions:\n",
    "```cpp\n",
    "dpct::device_ext &dev_ct1 = dpct::get_current_device();\n",
    "sycl::queue &q_ct1 = dev_ct1.default_queue();\n",
    "```\n",
    "In CUDA, the equivalent is a CUDA stream, if no stream is create in CUDA code, a default stream is implicitly created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c9205-febf-4e71-98f2-6caf02edbdd0",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "This section describes the location of the CUDA source and the contents of different SYCL source code directories in this project.\n",
    "\n",
    "| folder name | source code description\n",
    "| --- | ---\n",
    "| [CUDA github](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/2_Concepts_and_Techniques/sortingNetworks) | Original CUDA Source used for migration\n",
    "| dpct_output | SYCL migration output from SYCLomatic Tool, compiles without errors\n",
    "| sycl_migrated | Same as dpct_output, compiles without errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5f463-190f-4ebd-9253-505afa1d5046",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module we have learnt how to migrate simple CUDA source to SYCL source to get functionality using `SYCLomatic` and then analized/optimized the SYCL source by manually coding. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
