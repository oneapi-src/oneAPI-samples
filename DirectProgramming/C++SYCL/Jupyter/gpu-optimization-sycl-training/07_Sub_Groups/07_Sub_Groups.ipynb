{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sub-groups\n",
    "The index space of an ND-Range kernel is divided into work-groups, sub-groups, and work-items. A work-item is the basic unit. A collection of work-items form a sub-group, and a collection of sub-groups form a work-group. The mapping of work-items and work-groups to hardware execution units (EU) is implementation-dependent. All the work-groups run concurrently but may be scheduled to run at different times depending on availability of resources. Work-group execution may or or may not be preempted depending on the capabilities of underlying hardware. Work-items in the same work-group are guaranteed to run concurrently. Work-items in the same sub-group may have additional scheduling guarantees and have access to additional functionality.\n",
    "\n",
    "A sub-group is a collection of contiguous work-items in the global index space that execute in the same EU thread. When the device compiler compiles the kernel, multiple work-items are packed into a sub-group by vectorization so the generated SIMD instruction stream can perform tasks of multiple work-items simultaneously. Properly partitioning work-items into sub-groups can make a big performance difference.\n",
    "\n",
    "In this section we cover performance impact and consideration when writing kernel with Sub-Groups:\n",
    "\n",
    "- [Sub-group Sizes](#Sub-group-Sizes)\n",
    "- [Sub-group Size vs. Maximum Sub-group Size](#Sub-group-Size-vs.-Maximum-Sub-group-Size)\n",
    "- [Vectorization and Memory Access](#Vectorization-and-Memory-Access)\n",
    "- [Data Sharing](#Data-Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69230766-332c-4b99-8abe-38cdd62d1c56",
   "metadata": {},
   "source": [
    "## Sub-group Sizes\n",
    "\n",
    "By default, the compiler selects a sub-group size using device-specific information and a few heuristics. The user can override the compiler’s selection using the kernel attribute `intel::reqd_sub_group_size` to specify the maximum sub-group size. Sometimes, not always, explicitly requesting a sub-group size may help performance.\n",
    "\n",
    "The code below is sub-group example which prints sub-group information for each work-item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a45ce-25c0-4b43-a986-483324e4d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sg_size.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>()<< \"\\n\";\n",
    "\n",
    "  q.submit([&](auto &h) {\n",
    "    sycl::stream out(65536, 256, h);\n",
    "    h.parallel_for(sycl::nd_range<1>(32,32), [=](sycl::nd_item<1> it) {\n",
    "         int groupId = it.get_group(0);\n",
    "         int globalId = it.get_global_linear_id();\n",
    "         auto sg = it.get_sub_group();\n",
    "         int sgSize = sg.get_local_range()[0];\n",
    "         int sgGroupId = sg.get_group_id()[0];\n",
    "         int sgId = sg.get_local_id()[0];\n",
    "\n",
    "         out << \"globalId = \" << sycl::setw(2) << globalId\n",
    "             << \" groupId = \" << groupId\n",
    "             << \" sgGroupId = \" << sgGroupId << \" sgId = \" << sgId\n",
    "             << \" sgSize = \" << sycl::setw(2) << sgSize\n",
    "             << sycl::endl;\n",
    "    });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8435-c445-4ca5-91e7-a72b6acbac46",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e014403-ac0b-43a6-abb0-0431b1449454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./q.sh run_sg_size.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb01f9-de33-43b8-9d08-3eef28675108",
   "metadata": {},
   "source": [
    "Each sub-group in this example has 16 work-items, or the sub-group size is 16. This means each thread simultaneously executes 16 work-items and 32 work-items are executed by two EU threads.\n",
    "\n",
    "The valid sub-group sizes are device dependent. You can query the device to get this information:\n",
    "```cpp\n",
    "  std::cout << \"Sub-group Sizes: \";\n",
    "  for (const auto &s :\n",
    "       q.get_device().get_info<sycl::info::device::sub_group_sizes>()) {\n",
    "    std::cout << s << \" \";\n",
    "  }\n",
    "  std::cout << std::endl;\n",
    "```\n",
    "The valid sub-group sizes supported may be:\n",
    "```\n",
    "Subgroup Sizes: 8 16 32\n",
    "```\n",
    "\n",
    "You can modify and check the output of the above code by overriding the compiler's selection of sub-group size, and set sub-group size to `32` using the kernel attribute `intel::reqd_sub_group_size`:\n",
    "```cpp\n",
    "    h.parallel_for(sycl::nd_range<1>(32,32), [=](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(32)]] {\n",
    "        // Kernel Code\n",
    "    });\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887aa78e-49e3-4e6f-a83c-eebe436123d8",
   "metadata": {},
   "source": [
    "## Sub-group Size vs. Maximum Sub-group Size\n",
    "So far in our examples, the work-group size is divisible by the sub-group size and both the work-group size and the sub-group size (either required by the user or automatically picked by the compiler are powers of two). The sub-group size and maximum sub-group size are the same if the work-group size is divisible by the maximum sub-group size and both sizes are powers of two. But what happens if the work-group size is not divisible by the sub-group size? \n",
    "\n",
    "Consider the following example, the sub-group size is seven, though the maximum sub-group size is still eight! The maximum sub-group size is actually the SIMD width so it does not change, but there are less than eight work-items in the sub-group, so the sub-group size is seven. So be careful when your work-group size is not divisible by the maximum sub-group size. The last sub-group with fewer work-items may need to be specially handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7a5f3-4d86-4149-9595-795147c9f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sg_max_size.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr int N = 15;\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>()<< \"\\n\";\n",
    "\n",
    "  int *data = sycl::malloc_shared<int>(N + N + 2, q);\n",
    "\n",
    "  for (int i = 0; i < N + N + 2; i++) {\n",
    "    data[i] = i;\n",
    "  }\n",
    "\n",
    "  // Snippet begin\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    sycl::stream out(65536, 128, h);\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range<1>(15, 15), [=](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "          int i = it.get_global_linear_id();\n",
    "          auto sg = it.get_sub_group();\n",
    "          int sgSize = sg.get_local_range()[0];\n",
    "          int sgMaxSize = sg.get_max_local_range()[0];\n",
    "          int sId = sg.get_local_id()[0];\n",
    "          int j = data[i];\n",
    "          int k = data[i + sgSize];\n",
    "          out << \"globalId = \" << i << \" sgMaxSize = \" << sgMaxSize\n",
    "              << \" sgSize = \" << sgSize << \" sId = \" << sId << \" j = \" << j\n",
    "              << \" k = \" << k << sycl::endl;\n",
    "        });\n",
    "  });\n",
    "  q.wait();\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75001290-a11a-485f-89f9-57f48d4d687c",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daf304-1c00-4a17-a7d9-cd6c4b556fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sg_max_size.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfb0ec-488f-4c68-89ce-ab2c6b4bab5d",
   "metadata": {},
   "source": [
    "## Vectorization and Memory Access\n",
    "The Intel® graphics device has multiple EUs. Each EU is a multithreaded SIMD processor. The compiler generates SIMD instructions to pack multiple work-items in a sub-group to execute simultaneously in an EU thread. The SIMD width (thus the sub-group size), selected by the compiler is based on device characteristics and heuristics, or requested explicitly by the kernel, and can be 8, 16, or 32.\n",
    "\n",
    "Given a SIMD width, maximizing SIMD lane utilization gives optimal instruction performance. If one or more lanes (or kernel instances or work items) diverge, the thread executes both branch paths before the paths merge later, increasing the dynamic instruction count. SIMD divergence negatively impacts performance. The compiler works to minimize divergence, but it helps to avoid divergence in the source code, if possible.\n",
    "How memory is accessed in work-items affects how memory is accessed in the sub-group or how the SIMD lanes are utilized. Accessing contiguous memory in a work-item is often not optimal. \n",
    "\n",
    "For example: This kernel copies an array of 1024 x 1024 integers to another integer array of the same size. Each work-item copies 16 contiguous integers. However, the reads from data2 are gathered and stores to data are scattered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25109cba-fa2e-4f79-b189-29c36af19468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sg_mem_access_0.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 1024 * 1024;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "  int *data2 = sycl::malloc_shared<int>(N, q);\n",
    "  memset(data2, 0xFF, sizeof(int) * N);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    h.parallel_for(sycl::nd_range(sycl::range{N / 16}, sycl::range{32}),\n",
    "                   [=](sycl::nd_item<1> it) {\n",
    "                     int i = it.get_global_linear_id();\n",
    "                     i = i * 16;\n",
    "                     for (int j = i; j < (i + 16); j++) {\n",
    "                       data[j] = data2[j];\n",
    "                     }\n",
    "                   });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info< sycl::info::event_profiling::command_end>() - e.template get_profiling_info< sycl::info::event_profiling::command_start>())<< \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed64100-dd31-455d-a273-bbf6d8743e5b",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff6b0e-cccb-4a2f-b182-45e46c4c354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sg_mem_access_0.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a3c97-4c63-421c-b8d7-d700bec9b399",
   "metadata": {},
   "source": [
    "It will be more efficient to change the code to read and store contiguous integers in each sub-group instead of each work-item. The example below does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302195bc-9731-4d6f-b8fd-c88fa39021da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sg_mem_access_1.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 1024 * 1024;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "  int *data2 = sycl::malloc_shared<int>(N, q);\n",
    "  memset(data2, 0xFF, sizeof(int) * N);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    h.parallel_for(sycl::nd_range(sycl::range{N / 16}, sycl::range{32}),\n",
    "                   [=](sycl::nd_item<1> it) {\n",
    "                     int i = it.get_global_linear_id();\n",
    "                     sycl::ext::oneapi::sub_group sg = it.get_sub_group();\n",
    "                     int sgSize = sg.get_local_range()[0];\n",
    "                     i = (i / sgSize) * sgSize * 16 + (i % sgSize);\n",
    "                     for (int j = 0; j < sgSize * 16; j += sgSize) {\n",
    "                       data[i + j] = data2[i + j];\n",
    "                     }\n",
    "                   });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) << \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafaacc4-da6f-4644-bcd5-377f9b63074e",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b62d79-c3c4-4efc-98a5-590676109b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sg_mem_access_1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3e8c2-c70e-4538-882d-5635b6c1d38b",
   "metadata": {},
   "source": [
    "Intel® graphics have instructions optimized for memory block loads/stores. So if work-items in a sub-group access a contiguous block of memory, you can use the sub-group block access functions to take advantage of these block load/store instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16cb93-585f-41c7-9368-534c1af2822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sg_mem_access_2.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 1024 * 1024;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "  int *data2 = sycl::malloc_shared<int>(N, q);\n",
    "  memset(data2, 0xFF, sizeof(int) * N);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range(sycl::range{N / 16}, sycl::range{32}), [=\n",
    "    ](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "          sycl::ext::oneapi::sub_group sg = it.get_sub_group();\n",
    "          sycl::vec<int, 8> x;\n",
    "\n",
    "          using global_ptr =\n",
    "              sycl::multi_ptr<int, sycl::access::address_space::global_space>;\n",
    "          int base = (it.get_group(0) * 32 +\n",
    "                      sg.get_group_id()[0] * sg.get_local_range()[0]) *\n",
    "                     16;\n",
    "          x = sg.load<8>(global_ptr(&(data2[base + 0])));\n",
    "          sg.store<8>(global_ptr(&(data[base + 0])), x);\n",
    "          x = sg.load<8>(global_ptr(&(data2[base + 128])));\n",
    "          sg.store<8>(global_ptr(&(data[base + 128])), x);\n",
    "        });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) << \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7ec1b-836a-4175-9c97-d2b464ed24bc",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b794b71-178f-40b3-8c65-918eb9b6bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sg_mem_access_2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103f6e0-efb6-43ea-a7b6-fe1e823c6c52",
   "metadata": {},
   "source": [
    "## Data Sharing\n",
    "Because the work-items in a sub-group execute in the same thread, it is more efficient to share data between work-items, even if the data is private to each work-item. Sharing data in a sub-group is more efficient than sharing data in a work-group using shared local memory, or SLM. One way to share data among work-items in a sub-group is to use shuffle functions.\n",
    "\n",
    "This kernel transposes a 16 x 16 matrix. It looks more complicated than the previous examples, but the idea is simple: a sub-group loads a 16 x 16 sub-matrix, then the sub-matrix is transposed using the sub-group shuffle function `sycl::select_from_group(...)`. There is only one sub-matrix and the sub-matrix is the matrix so only one sub-group is needed. A bigger matrix, say 4096 x 4096, can be transposed using the same technique: each sub-group loads a sub-matrix, then the sub-matrices are transposed using the sub-group shuffle functions. This is left to the reader as an exercise.\n",
    "\n",
    "SYCL has multiple variants of sub-group shuffle functions available. Each variant is optimized for its specific purpose on specific devices. It is always a good idea to use these optimized functions (if they fit your needs) instead of creating your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffe549-5be3-4535-b7ac-22faeecb1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sg_shuffle.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <iomanip>\n",
    "\n",
    "constexpr size_t N = 16;\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<unsigned int> matrix(N * N);\n",
    "  for (int i = 0; i < N * N; ++i) {\n",
    "    matrix[i] = i;\n",
    "  }\n",
    "\n",
    "  std::cout << \"Matrix: \" << std::endl;\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    for (int j = 0; j < N; j++) {\n",
    "      std::cout << std::setw(3) << matrix[i * N + j] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "  }\n",
    "\n",
    "  {\n",
    "    constexpr size_t blockSize = 16;\n",
    "    sycl::buffer<unsigned int, 2> m(matrix.data(), sycl::range<2>(N, N));\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor marr(m, h);\n",
    "      sycl::local_accessor<unsigned int, 2> barr1(sycl::range<2>(blockSize, blockSize), h);\n",
    "      sycl::local_accessor<unsigned int, 2> barr2(sycl::range<2>(blockSize, blockSize), h);\n",
    "\n",
    "      h.parallel_for(\n",
    "          sycl::nd_range<2>(sycl::range<2>(N / blockSize, N),\n",
    "                            sycl::range<2>(1, blockSize)),\n",
    "          [=](sycl::nd_item<2> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "            int gi = it.get_group(0);\n",
    "            int gj = it.get_group(1);\n",
    "\n",
    "            sycl::sub_group sg = it.get_sub_group();\n",
    "            int sgId = sg.get_local_id()[0];\n",
    "\n",
    "            unsigned int bcol[blockSize];\n",
    "            int ai = blockSize * gi;\n",
    "            int aj = blockSize * gj;\n",
    "\n",
    "            for (int k = 0; k < blockSize; k++) {\n",
    "              bcol[k] = sg.load(marr.get_pointer() + (ai + k) * N + aj);\n",
    "            }\n",
    "\n",
    "            unsigned int tcol[blockSize];\n",
    "            for (int n = 0; n < blockSize; n++) {\n",
    "              if (sgId == n) {\n",
    "                for (int k = 0; k < blockSize; k++) {\n",
    "                  tcol[k] = sycl::select_from_group(sg, bcol[n], k);\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "\n",
    "            for (int k = 0; k < blockSize; k++) {\n",
    "              sg.store(marr.get_pointer() + (ai + k) * N + aj, tcol[k]);\n",
    "            }\n",
    "          });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_time = (e.template get_profiling_info< sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"\\nKernel Execution Time: \" << kernel_time * 1e-6 << \" msec\\n\";\n",
    "  }\n",
    "\n",
    "  std::cout << std::endl << \"Transposed Matrix: \" << std::endl;\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    for (int j = 0; j < N; j++) {\n",
    "      std::cout << std::setw(3) << matrix[i * N + j] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c5763-f7e9-4566-8e7d-2a7a395c6033",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31097a-fadc-4a45-9e09-e6b5449ab2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_sg_shuffle.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40749ea0-81c4-447b-a6fc-5c1b8d1ac8d2",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
