{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kernel Reduction\n",
    "Reduction is a common operation in parallel programming where an operator is applied to all elements of an array and a single result is produced. The reduction operator is associative and in some cases commutative. Some examples of reductions are summation, maximum, and minimum. \n",
    "\n",
    "In the next few sections we will look at different implementation of sum reduction in SYCL kernel:\n",
    "- [Reduction using Atomic Operation](#Reduction-using-Atomic-Operation)\n",
    "- [Reduction using Shared Local Memory](#Reduction-using-Shared-Local-Memory)\n",
    "- [Reduction using Sub-Groups](#Reduction-using-Sub-Groups)\n",
    "- [Reduction using SYCL Reduction Kernel](#Reduction-using-SYCL-Reduction-Kernel)\n",
    "\n",
    "Different implementations of reduction operation are provided and discussed here, which may have different performance characteristics depending on the architecture of the accelerator. Another important thing to note is that the time it takes to bring the result of reduction to the host over the PCIe interface (for a discrete GPU) is almost same as actually doing the entire reduction on the device. This shows that one should avoid data transfers between host and device as much as possible or overlap the kernel execution with data transfers.\n",
    "\n",
    "A serial summation reduction is shown below:\n",
    "```cpp\n",
    "  for (int it = 0; it < iter; it++) {\n",
    "    sum = 0;\n",
    "    for (size_t i = 0; i < data_size; ++i) {\n",
    "      sum += data[i];\n",
    "    }\n",
    "  }\n",
    "```\n",
    "The time complexity of reduction is linear with the number of elements. There are several ways this can be parallelized, and care must be taken to ensure that the amount of communication/synchronization is minimized between different processing elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6612c5-2327-4ee1-bff2-e3756072aec4",
   "metadata": {},
   "source": [
    "## Reduction using Atomic Operation\n",
    "A naive way to parallelize this reduction is to use a global variable and let the threads update this variable using an atomic operation, the threads are atomically updating a single memory location and get significant contention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2c4ee-1103-44b3-8003-0077ca821edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_atomics.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr size_t N = (1000 * 1024 * 1024);\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N, 1);\n",
    "  int sum = 0;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "\n",
    "  sycl::buffer<int> buf(data.data(), data.size(), props);\n",
    "  sycl::buffer<int> sum_buf(&sum, 1, props);\n",
    "    \n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    sycl::accessor buf_acc(buf, h, sycl::read_only);\n",
    "    sycl::accessor sum_acc(sum_buf, h, sycl::write_only, sycl::no_init);\n",
    "\n",
    "    h.parallel_for(N, [=](auto index) {\n",
    "      size_t glob_id = index[0];\n",
    "      auto v = sycl::atomic_ref<int, \n",
    "        sycl::memory_order::relaxed, \n",
    "        sycl::memory_scope::device, \n",
    "        sycl::access::address_space::global_space>(sum_acc[0]);\n",
    "      v.fetch_add(buf_acc[glob_id]);\n",
    "    });\n",
    "  });\n",
    "\n",
    "  sycl::host_accessor h_acc(sum_buf);\n",
    "  std::cout << \"Sum = \" << sum << \"\\n\";\n",
    "\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) * 1e-9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a517d-4c05-494b-9f4e-40b2620668ce",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58feae-5624-4743-a7a0-940f18792100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./q.sh run_reduction_atomics.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b78d4-cdb8-4193-833a-025aac8edacc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reduction using Shared Local Memory\n",
    "A further optimization is to block the accesses to the input vector and use the shared local memory to store the intermediate results. This kernel is shown below. In this kernel every work-item operates on a certain number of vector elements, and then one thread in the work-group reduces all these elements to one result by linearly going through the shared memory containing the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2080338-476f-49f4-a11c-468c8054892e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_slm.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr size_t N = (1000 * 1024 * 1024);\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N, 1);\n",
    "  int sum = 0;\n",
    "\n",
    "  int work_group_size = 256;\n",
    "  int log2elements_per_block = 13;\n",
    "  int elements_per_block = (1 << log2elements_per_block); // 8192\n",
    "\n",
    "  int log2workitems_per_block = 8;\n",
    "  int workitems_per_block = (1 << log2workitems_per_block); // 256\n",
    "  int elements_per_work_item = elements_per_block / workitems_per_block;\n",
    "\n",
    "  int mask = ~(~0 << log2workitems_per_block);\n",
    "  int num_work_items = data.size() / elements_per_work_item;\n",
    "  int num_work_groups = num_work_items / work_group_size;\n",
    "  std::cout << \"Num work items = \" << num_work_items << std::endl;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "\n",
    "  sycl::buffer<int> buf(data.data(), data.size(), props);\n",
    "  sycl::buffer<int> sum_buf(&sum, 1, props);\n",
    "  sycl::buffer<int> accum_buf(num_work_groups);\n",
    "    \n",
    "  auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor buf_acc(buf, h, sycl::read_only);\n",
    "      sycl::accessor accum_acc(accum_buf, h, sycl::write_only, sycl::no_init);\n",
    "      sycl::local_accessor<int, 1> scratch(work_group_size, h);\n",
    "      h.parallel_for(sycl::nd_range<1>{num_work_items, work_group_size},\n",
    "                     [=](sycl::nd_item<1> item) {\n",
    "                       size_t glob_id = item.get_global_id(0);\n",
    "                       size_t group_id = item.get_group(0);\n",
    "                       size_t loc_id = item.get_local_id(0);\n",
    "                       int offset = ((glob_id >> log2workitems_per_block)\n",
    "                                     << log2elements_per_block) +\n",
    "                                    (glob_id & mask);\n",
    "                       int sum = 0;\n",
    "                       for (size_t i = 0; i < elements_per_work_item; i++)\n",
    "                         sum +=\n",
    "                             buf_acc[(i << log2workitems_per_block) + offset];\n",
    "                       scratch[loc_id] = sum;\n",
    "                       // Serial Reduction\n",
    "\t\t       sycl::group_barrier(item.get_group());\n",
    "                       if (loc_id == 0) {\n",
    "                         int sum = 0;\n",
    "                         for (int i = 0; i < work_group_size; i++)\n",
    "                           sum += scratch[i];\n",
    "                         accum_acc[group_id] = sum;\n",
    "                       }\n",
    "                     });\n",
    "    });\n",
    "\n",
    "    q.wait();\n",
    "    {\n",
    "      sum = 0;\n",
    "      sycl::host_accessor h_acc(accum_buf);\n",
    "      for (int i = 0; i < num_work_groups; i++)\n",
    "        sum += h_acc[i];\n",
    "    }\n",
    "  std::cout << \"Sum = \" << sum << \"\\n\";\n",
    "\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) * 1e-9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6839cb-b014-4a45-a723-b0715cc1ce63",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb48a5-0470-4b04-a688-ccd7e6860225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./q.sh run_reduction_slm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68cb2b-10d1-424b-bc64-ba9299ad5ca0",
   "metadata": {},
   "source": [
    "In the above code, tree reduction can also be used to reduce the intermediate results from all the work-items in a work-group. In most cases this does not seem to make a big difference in performance.\n",
    "\n",
    "```cpp\n",
    "       // tree reduction\n",
    "       item.barrier(sycl::access::fence_space::local_space);\n",
    "       for (int i = work_group_size / 2; i > 0; i >>= 1) {\n",
    "         item.barrier(sycl::access::fence_space::local_space);\n",
    "         if (loc_id < i)\n",
    "           scratch[loc_id] += scratch[loc_id + i];\n",
    "       }\n",
    "       if (loc_id == 0)\n",
    "         accum_acc[group_id] = scratch[0];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df585ca-76b7-4e73-ad6b-6107a6d15714",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reduction using Sub-Groups\n",
    "This kernel below uses a completely different technique for accessing the memory. It uses sub-group loads to generate the intermediate result in a vector form. This intermediate result is then brought back to the host and the final reduction is performed there. In some cases it may be better to create another kernel to reduce this result in a single work-group, which lets you perform tree reduction through efficient barriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbedf11-3611-46d9-a8b6-11763254aa9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_sg.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr size_t N = (1000 * 1024 * 1024);\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N, 1);\n",
    "  int sum = 0;\n",
    "    \n",
    "  int work_group_size = 256;\n",
    "  int log2elements_per_work_item = 6;\n",
    "  int elements_per_work_item = (1 << log2elements_per_work_item); // 256\n",
    "  int num_work_items = data.size() / elements_per_work_item;\n",
    "  int num_work_groups = num_work_items / work_group_size;\n",
    "\n",
    "  std::cout << \"Num work items = \" << num_work_items << std::endl;\n",
    "  std::cout << \"Num work groups = \" << num_work_groups << std::endl;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "\n",
    "  sycl::buffer<int> buf(data.data(), data.size(), props);\n",
    "  sycl::buffer<int> sum_buf(&sum, 1, props);\n",
    "  sycl::buffer<sycl::vec<int, 8>> accum_buf(num_work_groups);\n",
    "    \n",
    "  auto e = q.submit([&](auto &h) {\n",
    "      const sycl::accessor buf_acc(buf, h);\n",
    "      sycl::accessor accum_acc(accum_buf, h, sycl::write_only, sycl::no_init);\n",
    "      sycl::local_accessor<sycl::vec<int, 8>, 1> scratch(work_group_size, h);\n",
    "      h.parallel_for(\n",
    "          sycl::nd_range<1>{num_work_items, work_group_size}, [=\n",
    "      ](sycl::nd_item<1> item) [[intel::reqd_sub_group_size(16)]] {\n",
    "            size_t glob_id = item.get_global_id(0);\n",
    "            size_t group_id = item.get_group(0);\n",
    "            size_t loc_id = item.get_local_id(0);\n",
    "            sycl::ext::oneapi::sub_group sg = item.get_sub_group();\n",
    "            sycl::vec<int, 8> sum{0, 0, 0, 0, 0, 0, 0, 0};\n",
    "            using global_ptr =\n",
    "                sycl::multi_ptr<int, sycl::access::address_space::global_space>;\n",
    "            int base = (group_id * work_group_size +\n",
    "                        sg.get_group_id()[0] * sg.get_local_range()[0]) *\n",
    "                       elements_per_work_item;\n",
    "            for (size_t i = 0; i < elements_per_work_item / 8; i++)\n",
    "              sum += sg.load<8>(global_ptr(&buf_acc[base + i * 128]));\n",
    "            scratch[loc_id] = sum;\n",
    "            for (int i = work_group_size / 2; i > 0; i >>= 1) {\n",
    "\t    sycl::group_barrier(item.get_group());\n",
    "              if (loc_id < i)\n",
    "                scratch[loc_id] += scratch[loc_id + i];\n",
    "            }\n",
    "            if (loc_id == 0)\n",
    "              accum_acc[group_id] = scratch[0];\n",
    "          });\n",
    "    });\n",
    "\n",
    "    q.wait();\n",
    "    {\n",
    "      sycl::host_accessor h_acc(accum_buf);\n",
    "      sycl::vec<int, 8> res{0, 0, 0, 0, 0, 0, 0, 0};\n",
    "      for (int i = 0; i < num_work_groups; i++)\n",
    "        res += h_acc[i];\n",
    "      sum = 0;\n",
    "      for (int i = 0; i < 8; i++)\n",
    "        sum += res[i];\n",
    "    }\n",
    "  sycl::host_accessor h_acc(sum_buf);\n",
    "  std::cout << \"Sum = \" << sum << \"\\n\";\n",
    "\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) * 1e-9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92035562-028d-4457-9827-08f2da6c41d3",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06bd2f-2f42-4ee7-b7f5-7304cf89977e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./q.sh run_reduction_sg.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45667695-9621-4365-bbb5-d3967c023a98",
   "metadata": {},
   "source": [
    "## Reduction using SYCL Reduction Kernel\n",
    "SYCL also supports built-in reduction operations, and you should use it where it is suitable because its implementation is fine tuned to the underlying architecture. \n",
    "\n",
    "The following kernel shows how to use the built-in reduction operator in the compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f605243-e6f0-433a-ba89-9e70cd7e9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_sycl.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr size_t N = (1000 * 1024 * 1024);\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N, 1);\n",
    "  int sum;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "\n",
    "  sycl::buffer<int> buf(data.data(), data.size(), props);\n",
    "  sycl::buffer<int> sum_buf(&sum, 1, props);\n",
    "    \n",
    "  auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor buf_acc(buf, h, sycl::read_only);\n",
    "      auto sum_reduction = sycl::reduction(sum_buf, h, sycl::plus<>());\n",
    "      h.parallel_for(sycl::nd_range<1>{N, 256}, sum_reduction,\n",
    "                     [=](sycl::nd_item<1> item, auto &sum_wg) {\n",
    "                       int i = item.get_global_id(0);\n",
    "                       sum_wg += buf_acc[i];\n",
    "                     });\n",
    "    });\n",
    "\n",
    "  sycl::host_accessor h_acc(sum_buf);\n",
    "  std::cout << \"Sum = \" << sum << \"\\n\";\n",
    "\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) * 1e-9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19c75e-e54a-40dd-a473-638b602fef75",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66b5a4-515d-489b-b9f6-4e848c95c528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./q.sh run_reduction_sycl.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a953e-ab72-42ea-9b44-66faac020a30",
   "metadata": {},
   "source": [
    "#### Reduction using SYCL Reduction Kernel and blocking\n",
    "The kernel below uses the blocking technique and then the compiler reduction operator to do final reduction. This gives good performance on most of the platforms on which it was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c8de5-7747-4b1e-abae-1e226648c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_sycl_blocks.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "constexpr size_t N = (1000 * 1024 * 1024);\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N, 1);\n",
    "  int sum;\n",
    "\n",
    "  int work_group_size = 256;\n",
    "  int log2elements_per_block = 13;\n",
    "  int elements_per_block = (1 << log2elements_per_block); // 8192\n",
    "\n",
    "  int log2workitems_per_block = 8;\n",
    "  int workitems_per_block = (1 << log2workitems_per_block); // 256\n",
    "  int elements_per_work_item = elements_per_block / workitems_per_block;\n",
    "\n",
    "  int mask = ~(~0 << log2workitems_per_block);\n",
    "  int num_work_items = data.size() / elements_per_work_item;\n",
    "  int num_work_groups = num_work_items / work_group_size;\n",
    "\n",
    "  std::cout << \"Num work items = \" << num_work_items << std::endl;\n",
    "  std::cout << \"Num work groups = \" << num_work_groups << std::endl;\n",
    "  std::cout << \"Elements per item = \" << elements_per_work_item << std::endl;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "\n",
    "  sycl::buffer<int> buf(data.data(), data.size(), props);\n",
    "  sycl::buffer<int> sum_buf(&sum, 1, props);\n",
    "    \n",
    "  auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor buf_acc(buf, h, sycl::read_only);\n",
    "      auto sumr = sycl::reduction(sum_buf, h, sycl::plus<>());\n",
    "      h.parallel_for(sycl::nd_range<1>{num_work_items, work_group_size}, sumr,\n",
    "                     [=](sycl::nd_item<1> item, auto &sumr_arg) {\n",
    "                       size_t glob_id = item.get_global_id(0);\n",
    "                       size_t group_id = item.get_group(0);\n",
    "                       size_t loc_id = item.get_local_id(0);\n",
    "                       int offset = ((glob_id >> log2workitems_per_block)\n",
    "                                     << log2elements_per_block) +\n",
    "                                    (glob_id & mask);\n",
    "                       int sum = 0;\n",
    "                       for (size_t i = 0; i < elements_per_work_item; i++)\n",
    "                         sum +=\n",
    "                             buf_acc[(i << log2workitems_per_block) + offset];\n",
    "                       sumr_arg += sum;\n",
    "                     });\n",
    "    });\n",
    "\n",
    "  sycl::host_accessor h_acc(sum_buf);\n",
    "  std::cout << \"Sum = \" << sum << \"\\n\";\n",
    "\n",
    "  std::cout << \"Kernel time = \" << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>()) * 1e-9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610506df-6c60-43c8-9ddd-23d0f56a09bd",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bceecc-c8bb-46cf-ae85-8d34797d8224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./q.sh run_reduction_sycl_blocks.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32ae16-83fb-4219-bfbc-5fceb004a735",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
