{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kernel Programming\n",
    "\n",
    "In this section we cover performance impact and consideration when writing kernel code SYCL:\n",
    "- [Considerations for Selecting Work-group Size](#Considerations-for-Selecting-Work-group-Size)\n",
    "- [Removing Conditional Checks](#Removing-Conditional-Checks)\n",
    "- [Avoiding Register Spills](#Avoiding-Register-Spills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889b1d3-de70-4f74-8cf4-d67115c5a2af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Considerations for Selecting Work-group Size\n",
    "In SYCL you can select the work-group size for nd_range kernels. The size of work-group has important implications for utilization of the compute resources, vector lanes, and communication among the work-items. The work-items in the same work-group may have access to hardware resources like shared memory and hardware synchronization capabilities that will allow them to run and communicate more efficiently than work-items across work-groups. So in general you should pick the maximum work-group size supported by the accelerator. The maximum work-group size can be queried by the call `device::get_info<sycl::info::device::max_work_group_size>()`.\n",
    "\n",
    "To illustrate the impact of the choice of work-group size, consider the following reduction kernel, which goes through a large vector to add all the elements in it. The function that runs the kernels takes in the work-group-size and sub-group-size as arguments, which lets you run experiments with different values. The performance difference can be seen from the timings reported when the kernel is called with different values for work-group size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6538f-cc8f-420b-ad83-510f68adeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/wg_reduction.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "// Summation of 10M 'one' values\n",
    "constexpr size_t N = (10 * 1024 * 1024);\n",
    "\n",
    "// Number of repetitions\n",
    "constexpr int repetitions = 16;\n",
    "// expected vlaue of sum\n",
    "int sum_expected = N;\n",
    "\n",
    "void init_data(sycl::queue &q, sycl::buffer<int> &buf, int data_size) {\n",
    "  // initialize data on the device\n",
    "  q.submit([&](auto &h) {\n",
    "    sycl::accessor buf_acc(buf, h, sycl::write_only, sycl::no_init);\n",
    "    h.parallel_for(data_size, [=](auto index) { buf_acc[index] = 1; });\n",
    "  });\n",
    "  q.wait();\n",
    "}\n",
    "\n",
    "void check_result(double elapsed, std::string msg, int sum) {\n",
    "  if (sum == sum_expected)\n",
    "    std::cout << \"SUCCESS: Time is \" << elapsed << \"s\" << msg << \"\\n\";\n",
    "  else\n",
    "    std::cout << \"ERROR: Expected \" << sum_expected << \" but got \" << sum\n",
    "              << \"\\n\";\n",
    "}\n",
    "\n",
    "void reduction(sycl::queue &q, std::vector<int> &data, std::vector<int> &flush,\n",
    "               int iter, int vec_size, int work_group_size) {\n",
    "  const size_t data_size = data.size();\n",
    "  const size_t flush_size = flush.size();\n",
    "  int sum = 0;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "  int num_work_items = data_size / work_group_size;\n",
    "  sycl::buffer<int> buf(data.data(), data_size, props);\n",
    "  sycl::buffer<int> flush_buf(flush.data(), flush_size, props);\n",
    "  sycl::buffer<int> sum_buf(&sum, 1, props);\n",
    "\n",
    "  init_data(q, buf, data_size);\n",
    "\n",
    "  double elapsed = 0;\n",
    "  for (int i = 0; i < iter; i++) {\n",
    "    q.submit([&](auto &h) {\n",
    "      sycl::accessor sum_acc(sum_buf, h, sycl::write_only, sycl::no_init);\n",
    "\n",
    "      h.parallel_for(1, [=](auto index) { sum_acc[index] = 0; });\n",
    "    });\n",
    "    // flush the cache\n",
    "    q.submit([&](auto &h) {\n",
    "      sycl::accessor flush_acc(flush_buf, h, sycl::write_only, sycl::no_init);\n",
    "      h.parallel_for(flush_size, [=](auto index) { flush_acc[index] = 1; });\n",
    "    });\n",
    "\n",
    "    auto start = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
    "    // reductionMapToHWVector main begin\n",
    "    q.submit([&](auto &h) {\n",
    "      sycl::accessor buf_acc(buf, h, sycl::read_only);\n",
    "      sycl::local_accessor<int, 1> scratch(work_group_size, h);\n",
    "      sycl::accessor sum_acc(sum_buf, h, sycl::write_only, sycl::no_init);\n",
    "\n",
    "      h.parallel_for(\n",
    "          sycl::nd_range<1>(num_work_items, work_group_size), [=\n",
    "      ](sycl::nd_item<1> item) [[intel::reqd_sub_group_size(16)]] {\n",
    "            auto v = sycl::atomic_ref<\n",
    "                int, sycl::memory_order::relaxed,\n",
    "                sycl::memory_scope::device,\n",
    "                sycl::access::address_space::global_space>(sum_acc[0]);\n",
    "            int sum = 0;\n",
    "            int glob_id = item.get_global_id();\n",
    "            int loc_id = item.get_local_id();\n",
    "            for (int i = glob_id; i < data_size; i += num_work_items)\n",
    "              sum += buf_acc[i];\n",
    "            scratch[loc_id] = sum;\n",
    "\n",
    "            for (int i = work_group_size / 2; i > 0; i >>= 1) {\n",
    "\t    sycl::group_barrier(item.get_group());\n",
    "              if (loc_id < i)\n",
    "                scratch[loc_id] += scratch[loc_id + i];\n",
    "            }\n",
    "\n",
    "            if (loc_id == 0)\n",
    "              v.fetch_add(scratch[0]);\n",
    "          });\n",
    "    });\n",
    "    q.wait();\n",
    "    elapsed += (std::chrono::high_resolution_clock::now().time_since_epoch().count() - start) / 1e+9;\n",
    "    sycl::host_accessor h_acc(sum_buf);\n",
    "    sum = h_acc[0];\n",
    "  }\n",
    "  elapsed = elapsed / iter;\n",
    "  std::string msg = \" with work-groups=\" + std::to_string(work_group_size);\n",
    "  check_result(elapsed, msg, sum);\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q;\n",
    "  std::cout << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> data(N, 1);\n",
    "  std::vector<int> extra(N, 1);\n",
    "\n",
    "  int vec_size = 16;\n",
    "  int work_group_size = vec_size;\n",
    "  reduction(q, data, extra, 16, vec_size, work_group_size);\n",
    "  work_group_size =\n",
    "      q.get_device().get_info<sycl::info::device::max_work_group_size>();\n",
    "  reduction(q, data, extra, 16, vec_size, work_group_size);\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a82c9-dc09-4f16-9cda-e9c1c6a5146a",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a3a99-49c2-4bb4-b3d5-bc3828a547ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_wg_reduction.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24cde92-ba94-4793-9f2f-62e4529db64c",
   "metadata": {},
   "source": [
    "In situations where there are no barriers or atomics used, the work-group size will not impact the performance. To illustrate this, consider the following vec_copy kernel where there are no atomics or barriers.\n",
    "\n",
    "In the code below, the above kernel is called with different work-group sizes. All the above calls to the kernel will have similar run times which indicates that there is no impact of work-group size on performance. The reason for this is that the threads created within a work-group and threads from different work-groups behave in a similar manner from the scheduling and resourcing point of view when there are no barriers or shared memory in the work-groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba93127-9942-40dd-8e23-f110606038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/wg_vec_copy.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "// Copy of 32M 'one' values\n",
    "constexpr size_t N = (32 * 1024 * 1024);\n",
    "\n",
    "// Number of repetitions\n",
    "constexpr int repetitions = 16;\n",
    "\n",
    "void check_result(double elapsed, std::string msg, std::vector<int> &res) {\n",
    "  bool ok = true;\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    if (res[i] != 1) {\n",
    "      ok = false;\n",
    "      std::cout << \"ERROR: Mismatch at \" << i << \"\\n\";\n",
    "    }\n",
    "  }\n",
    "  if (ok)\n",
    "    std::cout << \"SUCCESS: Time \" << msg << \" = \" << elapsed << \"s\\n\";\n",
    "}\n",
    "\n",
    "void vec_copy(sycl::queue &q, std::vector<int> &src, std::vector<int> &dst,\n",
    "              std::vector<int> &flush, int iter, int work_group_size) {\n",
    "  const size_t data_size = src.size();\n",
    "  const size_t flush_size = flush.size();\n",
    "  int sum = 0;\n",
    "\n",
    "  const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n",
    "  int num_work_items = data_size;\n",
    "  double elapsed = 0;\n",
    "  {\n",
    "    sycl::buffer<int> src_buf(src.data(), data_size, props);\n",
    "    sycl::buffer<int> dst_buf(dst.data(), data_size, props);\n",
    "    sycl::buffer<int> flush_buf(flush.data(), flush_size, props);\n",
    "\n",
    "    for (int i = 0; i < iter; i++) {\n",
    "      // flush the cache\n",
    "      q.submit([&](auto &h) {\n",
    "        sycl::accessor flush_acc(flush_buf, h, sycl::write_only, sycl::no_init);\n",
    "        h.parallel_for(flush_size, [=](auto index) { flush_acc[index] = 1; });\n",
    "      });\n",
    "\n",
    "      auto start = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
    "      q.submit([&](auto &h) {\n",
    "        sycl::accessor src_acc(src_buf, h, sycl::read_only);\n",
    "        sycl::accessor dst_acc(dst_buf, h, sycl::write_only, sycl::no_init);\n",
    "\n",
    "        h.parallel_for(\n",
    "            sycl::nd_range<1>(num_work_items, work_group_size), [=\n",
    "        ](sycl::nd_item<1> item) [[intel::reqd_sub_group_size(16)]] {\n",
    "              int glob_id = item.get_global_id();\n",
    "              dst_acc[glob_id] = src_acc[glob_id];\n",
    "            });\n",
    "      });\n",
    "      q.wait();\n",
    "      elapsed += (std::chrono::high_resolution_clock::now().time_since_epoch().count() - start) / 1e+9;\n",
    "    }\n",
    "  }\n",
    "  elapsed = elapsed / iter;\n",
    "  std::string msg = \"with work-group-size=\" + std::to_string(work_group_size);\n",
    "  check_result(elapsed, msg, dst);\n",
    "} // vec_copy end\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  sycl::queue q;\n",
    "  std::cout << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> src(N, 1);\n",
    "  std::vector<int> dst(N, 0);\n",
    "  std::vector<int> extra(N, 1);\n",
    "\n",
    "  // call begin\n",
    "  int vec_size = 16;\n",
    "  int work_group_size = vec_size;\n",
    "  vec_copy(q, src, dst, extra, 16, work_group_size);\n",
    "  work_group_size = 2 * vec_size;\n",
    "  vec_copy(q, src, dst, extra, 16, work_group_size);\n",
    "  work_group_size = 4 * vec_size;\n",
    "  vec_copy(q, src, dst, extra, 16, work_group_size);\n",
    "  work_group_size = 8 * vec_size;\n",
    "  vec_copy(q, src, dst, extra, 16, work_group_size);\n",
    "  work_group_size = 16 * vec_size;\n",
    "  vec_copy(q, src, dst, extra, 16, work_group_size);\n",
    "  // call end\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fce3c-1aac-4233-8246-d7adb7df3133",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791073b0-7e9d-4d13-b7d4-0efc7d259aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_wg_vec_copy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7948b-9382-4f94-a1d9-f98837952b2f",
   "metadata": {},
   "source": [
    "## Removing Conditional Checks\n",
    "In Sub-groups, we learned that SIMD divergence can negatively affect performance. If all work items in a sub-group execute the same instruction, the SIMD lanes are maximally utilized. If one or more work items take a divergent path, then both paths have to be executed before they merge.\n",
    "\n",
    "Divergence is caused by conditional checks, though not all conditional checks cause divergence. Some conditional checks, even when they do not cause SIMD divergence, can still be performance hazards. In general, removing conditional checks can help performance.\n",
    "\n",
    "Look at the convolution example from Shared Local Memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d24152-74ca-44df-9bb6-7cf252d7d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/convolution_global.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr size_t N = 8192 * 8192;\n",
    "  constexpr size_t M = 257;\n",
    "\n",
    "  std::vector<int> input(N);\n",
    "  std::vector<int> output(N);\n",
    "  std::vector<int> kernel(M);\n",
    "\n",
    "  srand(2009);\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    input[i] = rand();\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < M; ++i) {\n",
    "    kernel[i] = rand();\n",
    "  }\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  {\n",
    "    sycl::buffer<int> ibuf(input.data(), N);\n",
    "    sycl::buffer<int> obuf(output.data(), N);\n",
    "    sycl::buffer<int> kbuf(kernel.data(), M);\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor iacc(ibuf, h, sycl::read_only);\n",
    "      sycl::accessor oacc(obuf, h);\n",
    "      sycl::accessor kacc(kbuf, h, sycl::read_only);\n",
    "\n",
    "      h.parallel_for(sycl::nd_range<1>(N, 256), [=](sycl::nd_item<1> it) {\n",
    "           int i = it.get_global_linear_id();\n",
    "           int group = it.get_group()[0];\n",
    "           int gSize = it.get_local_range()[0];\n",
    "\n",
    "           int t = 0;\n",
    "\n",
    "           if ((group == 0) || (group == N / gSize - 1)) {\n",
    "             if (i < M / 2) {\n",
    "               for (int j = M / 2 - i, k = 0; j < M; j++, k++) {\n",
    "                 t += iacc[k] * kacc[j];\n",
    "               }\n",
    "             } else {\n",
    "               if (i + M / 2 >= N) {\n",
    "                 for (int j = 0, k = i - M / 2; j < M / 2 + N - i;\n",
    "                      j++, k++) {\n",
    "                   t += iacc[k] * kacc[j];\n",
    "                 }\n",
    "               } else {\n",
    "                 for (int j = 0, k = i - M / 2; j < M; j++, k++) {\n",
    "                   t += iacc[k] * kacc[j];\n",
    "                 }\n",
    "               }\n",
    "             }\n",
    "           } else {\n",
    "             for (int j = 0, k = i - M / 2; j < M; j++, k++) {\n",
    "               t += iacc[k] * kacc[j];\n",
    "             }\n",
    "           }\n",
    "\n",
    "           oacc[i] = t;\n",
    "         });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_ns = (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6 << \" msec\\n\";\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591a037-25f7-43fc-a291-f85dd99f30ac",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf8c2e-69d0-4e92-9cc1-391cafa65388",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_convolution_global.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0d90e-7342-4d8f-b5f5-e54713ce3c6a",
   "metadata": {},
   "source": [
    "### Padding Buffers to Remove Conditional Checks\n",
    "\n",
    "The nested if-then-else conditional checks are necessary to take care of the first and last 128 elements in the input so indexing will not run out of bounds. If we pad enough 0s before and after the input array, these conditional checks can be safely removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21a97d-14d9-4999-abe0-642df1930f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/convolution_global_conditionals.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr size_t N = 8192 * 8192;\n",
    "  constexpr size_t M = 257;\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  std::vector<int> input(N + M / 2 + M / 2);\n",
    "  std::vector<int> output(N);\n",
    "  std::vector<int> kernel(M);\n",
    "\n",
    "  srand(2009);\n",
    "  for (int i = M / 2; i < N + M / 2; ++i) {\n",
    "    input[i] = rand();\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < M / 2; ++i) {\n",
    "    input[i] = 0;\n",
    "    input[i + N + M / 2] = 0;\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < M; ++i) {\n",
    "    kernel[i] = rand();\n",
    "  }\n",
    "\n",
    "  {\n",
    "    sycl::buffer<int> ibuf(input.data(), N + M / 2 + M / 2);\n",
    "    sycl::buffer<int> obuf(output.data(), N);\n",
    "    sycl::buffer<int> kbuf(kernel.data(), M);\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor iacc(ibuf, h, sycl::read_only);\n",
    "      sycl::accessor oacc(obuf, h);\n",
    "      sycl::accessor kacc(kbuf, h, sycl::read_only);\n",
    "\n",
    "      h.parallel_for(sycl::nd_range<1>(N, 256), [=](sycl::nd_item<1> it) {\n",
    "           int i = it.get_global_linear_id();\n",
    "           int t = 0;\n",
    "\n",
    "           for (int j = 0; j < M; j++) {\n",
    "             t += iacc[i + j] * kacc[j];\n",
    "           }\n",
    "\n",
    "           oacc[i] = t;\n",
    "         });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_ns = (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6 << \" msec\\n\";\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ec9a4-a823-4f9a-a794-2e973a049b67",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94cf11-08ca-48b5-9ab2-42483683fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_convolution_global_conditionals.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefd584-d329-45f9-8a79-2b837f1e13bf",
   "metadata": {},
   "source": [
    "### Replacing Conditional Checks with Relational Functions\n",
    "Another way to remove conditional checks is to replace them with relational functions, especially built-in relational functions. It is strongly recommended to use a built-in function if one is available. SYCL provides a rich set of built-in relational functions like `select()`, `min()`, `max()`. In many cases you can use these functions to replace conditional checks and achieve better performance.\n",
    "\n",
    "Consider the convolution example again. The if-then-else conditional checks can be replaced with built-in functions `min()` and `max()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4da1c-4c51-497c-ab0b-881bc333a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/convolution_global_conditionals_minmax.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr size_t N = 8192 * 8192;\n",
    "  constexpr size_t M = 257;\n",
    "\n",
    "  std::vector<int> input(N);\n",
    "  std::vector<int> output(N);\n",
    "  std::vector<int> kernel(M);\n",
    "\n",
    "  srand(2009);\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    input[i] = rand();\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < M; ++i) {\n",
    "    kernel[i] = rand();\n",
    "  }\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  {\n",
    "    sycl::buffer<int> ibuf(input.data(), N);\n",
    "    sycl::buffer<int> obuf(output.data(), N);\n",
    "    sycl::buffer<int> kbuf(kernel.data(), M);\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor iacc(ibuf, h, sycl::read_only);\n",
    "      sycl::accessor oacc(obuf, h);\n",
    "      sycl::accessor kacc(kbuf, h, sycl::read_only);\n",
    "\n",
    "      h.parallel_for(sycl::nd_range<1>(N, 256), [=](sycl::nd_item<1> it) {\n",
    "           int i = it.get_global_linear_id();\n",
    "           int t = 0;\n",
    "           int startj = sycl::max<int>(M / 2 - i, 0);\n",
    "           int endj = sycl::min<int>(M / 2 + N - i, M);\n",
    "           int startk = sycl::max<int>(i - M / 2, 0);\n",
    "           for (int j = startj, k = startk; j < endj; j++, k++) {\n",
    "             t += iacc[k] * kacc[j];\n",
    "           }\n",
    "           oacc[i] = t;\n",
    "         });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_ns = (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6 << \" msec\\n\";\n",
    "\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126af19-ad54-4f3e-bb70-888dfbd0ff9c",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118abfc-fb24-4704-81f0-16670c0c022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_convolution_global_conditionals_minmax.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc5eea-44ff-4c20-82bd-1ccc9fecc955",
   "metadata": {},
   "source": [
    "## Avoiding Register Spills\n",
    "### Registers and Performance\n",
    "It is well known that the register is the fastest storage in the memory hierarchy. Keeping data in registers as long as possible is critical to performance. On the other hand, register space is limited and much smaller than memory space. The current generation of Intel® GPUs, for example, has 128 general-purpose registers, each 32 bytes wide by default for each EU thread. Though the compiler aims to assign as many variables to registers as possible, the limited number of registers can be allocated only to a small set of variables at any point during execution. A given register can hold different variables at different times because different sets of variables are needed at different times. If there are not enough registers to hold all the variables, register can spill, or some variables currently in the registers can be moved to memory to make room for other variables.\n",
    "\n",
    "In SYCL, the compiler allocates registers to private variables in work items. Multiple work items in a sub-group are packed into one EU thread. By default, the compiler uses register pressure as one of the heuristics to choose SIMD width or sub-group size. High register pressures can result in smaller sub-group size (for example 8 instead of 16) if a sub-group size is not explicitly requested. It can also cause register spilling or cause certain variables not to be promoted to registers.\n",
    "\n",
    "The hardware may not be fully utilized if sub-group size or SIMD width is not the maximum the hardware supports. Register spilling can cause significant performance degradation, especially when spills occur inside hot loops. When variables are not promoted to registers, accesses to these variables incur significant increase of memory traffic.\n",
    "\n",
    "Though the compiler uses intelligent algorithms to avoid or minimize register spills, optimizations by developers can help the compiler to do a better job and often make a big performance difference.\n",
    "### Optimization Techniques\n",
    "The following techniques can reduce register pressure:\n",
    "- Keep live ranges of private variables as short as possible.\n",
    "  Though the compiler schedules instructions and optimizes the distances, in some cases moving the loading and using the same variable closer or removing certain dependencies in the source can help the compiler do a better job.\n",
    "- Avoid excessive loop unrolling.\n",
    "  Loop unrolling exposes opportunities for instruction scheduling optimization by the compiler and thus can improve performance. However, temporary variables introduced by unrolling may increase pressure on register allocation and cause register spilling. It is always a good idea to compare the performance with and without loop unrolling and different times of unrolls to decide if a loop should be unrolled or how many times to unroll it.\n",
    "- Prefer USM pointers.\n",
    "  A buffer accessor takes more space than a USM pointer. If you can choose between USM pointers and buffer accessors, choose USM pointers.\n",
    "- Recompute cheap-to-compute values on-demand that otherwise would be held in registers for a long time.\n",
    "- Avoid big arrays or large structures, or break an array of big structures into multiple arrays of small structures.\n",
    "  For example, an array of `sycl::float4`:\n",
    "  `sycl::float4 v[8];`\n",
    "  can be broken into 4 arrays of float:\n",
    "  `float x[8]; float y[8]; float z[8]; float w[8];`\n",
    "  All or part of the 4 arrays of float have a better chance to be allocated in registers than the array of `sycl::float4`.\n",
    "- Break a large loop into multiple small loops to reduce the number of simultaneously live variables.\n",
    "- Choose smaller data types if possible.\n",
    "- Do not declare private variables as volatile.\n",
    "- Share registers in a sub-group.\n",
    "- Use shared local memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5504291-fca2-408b-8605-c664d0fb9c3e",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
