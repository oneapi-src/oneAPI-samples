{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implicit and Explicit Scaling for Multi-Stack Architecture\n",
    "\n",
    "In this section we cover how Multi-Stack Architecture can be programmed using Implicit Scaling and Explicit Scaling\n",
    "\n",
    "- [Multi-Stack Architecture](#Multi-Stack-Architecture)\n",
    "- [Implicit Scaling](#Implicit-Scaling)\n",
    "  - [Performance Expectations](#Performance-Expectations)\n",
    "  - [Work Scheduling and Memory Distribution](#Work-Scheduling-and-Memory-Distribution)\n",
    "  - [Programming Principles](#Programming-Principles)\n",
    "- [Explicit Scaling](#Explicit-Scaling)\n",
    "  - [Creating Sub-Devices](#Creating-Sub-Devices)\n",
    "  - [Context](#Context)\n",
    "  - [Explicit Scaling Example](#Explicit-Scaling-Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423668dd-a32b-4896-b256-89dcfcc2bf00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multi-Stack Architecture\n",
    "\n",
    "Intel Data Center GPU MAX series use Multi-Stack Architecture with 1 or 2 Stack. Each Stack is capable of functioning as an independent GPU entity. The Stack can execute workloads on its own.\n",
    "\n",
    "For general applications, the multi-stack GPU is represented as a single GPU device. Applications do not care that internally GPU is constructed out of smaller Stacks, which simplifies the programming model and allows existing applications to run without any code changes. Intel GPU driver, SYCL and OpenMP parallel language runtimes work together to automatically dispatch the workloads across the stacks.\n",
    "\n",
    "The figures below show a single Stack and 2-Stack GPU schematic:\n",
    "\n",
    "<img src=\"assets/1-tile-architecture.png\">\n",
    "\n",
    "<img src=\"assets/2-tile-architecture.png\">\n",
    "\n",
    "Stack are connected with fast interconnect that allows efficient communication between stacks using High Bandwidth Memory (HBM).\n",
    "\n",
    "Any Stack is capable of reading and writing to any HBM memory. For example, Stack 0 may read the local HBM memory of Stack 1. In this case, the interconnect between Stack 0 and Stack 1 is used for communication.\n",
    "\n",
    "Stack 0 is connected to the PCI, but any Stack can read and write system memory. The same inter-Stack interconnects are used to transfer the data. Hence, Stack 0 has the shortest path to system memory among all the Stack.\n",
    "\n",
    "Reading and writing to system memory do not require CPU involvement, GPU can perform DMA (Direct Memory Access) over PCI to system memory.\n",
    "\n",
    "Because access to a Stack's local HBM does not involve inter-Stack interconnect, it is more efficient than cross-Stack HBM access, with lower latency and lower inter-Stack bandwidth consumption. Advanced developers can take advantage of memory locality to achieve higher performance.\n",
    "\n",
    "To properly utilize multi-stack GPU, we introduced two application programming modes:\n",
    "\n",
    "#### Implicit scaling mode\n",
    "Driver and language runtimes are responsible for work distribution and multi-stack memory placement. Application sees the GPU as one monolithic device and does not care about multi-stack architecture.\n",
    "\n",
    "#### Explicit scaling mode\n",
    "User is responsible for work distribution and mutli-stack memory placement. Driver and language runtimes provide tools that expose each Stack as a separate subdevice that can be programmed independently of all the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d4cf1-b663-45b7-979a-c33b8dea834d",
   "metadata": {},
   "source": [
    "## Implicit Scaling\n",
    "\n",
    "In Implicit Scaling Mode, Driver and language runtimes are responsible for work distribution and multi-stack memory placement. Application sees the GPU as one monolithic device and does not care about multi-stack architecture.\n",
    "\n",
    "Implicit scaling can be enabled by exporting this environment variable:\n",
    "\n",
    "```\n",
    "export EnableImplicitScaling=1\n",
    "```\n",
    "\n",
    "This environment variable changes the meaning of a device to root-device. No change in application code is required. A kernel submitted to device will utilize all stacks. Similarly, memory allocation on device will span across all stacks.\n",
    "\n",
    "A root-device is built using multiple sub-devices, also known as stacks. These stacks form a shared memory space which allows to treat a root-device as a monolithic device without the requirement of explicit communication between stacks. This section covers multi-stack programming principles using implicit scaling. When using implicit scaling, the root-device driver is responsible for distributing work to all stacks when application code launches a kernel.\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "Implicit scaling exposes resources of all stacks to a single kernel launch. For root-device with 2 stacks, a kernel has access to 2x compute peak, 2x memory bandwidth and 2x memory capacity. In the ideal case, workload performance increases by 2x. However, cache size and cache bandwidth are increased by 2x as well which can lead to better-than-linear scaling if workload fits in increased cache capacity.\n",
    "\n",
    "Each stack is equivalent to a NUMA domain and therefore memory access pattern and memory allocation are a crucial part to achieve optimal implicit scaling performance. Workloads with a concept of locality are expected to work best with this programming model as cross-stack memory accesses are naturally minimized. Note that compute bound kernels are not impacted by NUMA domains, thus are expected to easily scale to multiple stacks with implicit scaling.\n",
    "\n",
    "MPI applications are more efficient with implicit scaling compared to an explicit scaling approach. A single rank can utilize the entire root-device which eliminates explicit synchronization and communication between stacks. Implicit scaling automatically overlaps local memory accesses and cross-stack memory accesses in a single kernel launch.\n",
    "\n",
    "Implicit scaling improves kernel execution time only. Serial bottlenecks will not speed up. Applications will observe no speed-up with implicit scaling if large serial bottleneck is present. Common serial bottlenecks are:\n",
    "\n",
    "- high CPU usage\n",
    "- kernel launch latency\n",
    "- PCIe transfers\n",
    "\n",
    "These will become more pronounced as kernel execution time reduces. Note that only stack-0 has PCIe connection to the host.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c12b2d-f756-4fc7-b1a3-e2e155164be6",
   "metadata": {},
   "source": [
    "### Work Scheduling and Memory Distribution\n",
    "\n",
    "#### Memory Coloring\n",
    "Any allocation in SYCL that corresponds to a shared or device allocation is colored across all stacks, meaning that allocation is divided in number-of-stacks chunks and distributed round-robin between stacks. Consider this root-device allocation:\n",
    "```cpp\n",
    "int *a = sycl::malloc_device<int>(N, q);\n",
    "```\n",
    "For a 2-stack root-device, the first half, (elements a[0] to a[N/2-1]), is physically allocated on stack-0. The remaining half, (elements a[N/2] to a[N-1]), is located on stack-1. In the future, we will introduce memory allocation APIs that allow user-defined memory coloring.\n",
    "\n",
    "<img src=\"assets/2-tile-scaling.png\">\n",
    "\n",
    "__Note:__\n",
    "- Memory coloring described above is applied at page size granularity. \n",
    "  - An allocation containing three pages has two pages resident on stack-0.\n",
    "  - Allocations smaller or equal than page size are resident on stack-0 only.\n",
    "- Using a memory pool that is based on a single allocation will break memory coloring logic. It is recommended that applications create one allocation per object to allow that object data is distributed to all stacks.\n",
    "\n",
    "\n",
    "#### Static Partitioning\n",
    "Scheduling of work-groups to stacks is deterministic and referred to as static partitioning. The partitioning follows a simple rule: the slowest moving dimension is divided in number-of-stacks chunks and distributed round-robin between stacks. \n",
    "\n",
    "Let's look at 1-dimensional kernel launch on root-device:\n",
    "```cpp\n",
    "q.parallel_for(N, [=](auto i) {\n",
    "    //\n",
    "});\n",
    "```\n",
    "Since there is only a single dimension it is automatically slowest dimension and partitioned between stacks by driver. For a 2-stack root-device, iterations 0 to N/2-1 are scheduled to stack-0. The remaining iterations N/2 to N-1 are executed on stack-1.\n",
    "\n",
    "Let's look at 3-dimensional kernel launch on root-device:\n",
    "\n",
    "```cpp\n",
    "range<3> global{nz, ny, nx};\n",
    "range<3> local{1, 1, 16};\n",
    "\n",
    "cgh.parallel_for(nd_range<3>(global, local), [=](nd_item<3> item) {\n",
    "    //\n",
    "});\n",
    "```\n",
    "\n",
    "The slowest dimension is z and partitioned between stacks, i.e. for 2-stack root-device, all iterations from z=0 to z=nz/2-1 are executed on stack-0. The remaining iterations with z=nz/2 to z=nz-1 are scheduled to stack-1.\n",
    "\n",
    "In case slowest moving dimension can't be divided evenly between stacks and creates an remainder imbalance larger than 5%, driver will partition next dimension if it leads to less load imbalance. This impacts kernels with odd dimensions smaller than 19 only. Examples for different kernel launches can be seen in below table (assuming local range {1,1,16}):\n",
    "\n",
    "Work group partition to stacks:\n",
    "\n",
    "|nz|ny|nx|Partitioned Dimension\n",
    "|---|---|---|---\n",
    "|512|512|512|z\n",
    "|21|512|512|z\n",
    "|19|512|512|y\n",
    "|18|512|512|z\n",
    "|19|19|512|x\n",
    "\n",
    "In case of multi-dimensional local range in SYCL, the partitioned dimension can change. For example, for global range {38,512,512} with local range {2,1,8} driver would partition y-dimension while for local range {1,1,16} driver would partition z-dimension. OpenMP can only have a 1-dimensional local range which is created from inner most loop and thus does not impact static partitioning heuristics. OpenMP kernels created with collapse level larger than 3 correspond to 1-dimensional kernel with all for loops linearized. The linearized loop will be portioned following 1D kernel launch heuristics.\n",
    "\n",
    "__Note:__\n",
    "- Static partitioning happens at work-group granularity.\n",
    "  - This implies that all work-items in a work-group are scheduled to same stack.\n",
    "- A kernel with a single work-group is resident on stack-0 only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c715263-c5cf-41d2-b884-e66ba93deccc",
   "metadata": {},
   "source": [
    "### Programming Principles\n",
    "To achieve good performance with implicit scaling, cross-stack memory accesses must be minimized but it is not required to eliminate all cross-stack accesses. A certain amount of cross-stack traffic can be handled by stack-to-stack interconnect if performed concurrently with local memory accesses. For memory bandwidth bound workload the amount of acceptable cross-stack accesses is determined by ratio of local memory bandwidth and cross-stack bandwidth (see Cross-stack Traffic).\n",
    "\n",
    "The following principles should be embraced by workloads that use implicit scaling:\n",
    "\n",
    "- Kernel must have enough work-items to utilize both stacks.\n",
    "  - The minimal number of work-items needed to utilize both stacks is: <number of VEs> * <hardware-threads per VE> * <SIMD width>.\n",
    "  - 2-stack GPU with 1024 VE and SIMD32 requires at least 262,144 work-items.\n",
    "- Device time must dominate runtime to observe whole application scaling.\n",
    "- Minimize cross-stack memory accesses by exploiting locality in algorithm.\n",
    "- Slowest moving dimension should be large to avoid stack load imbalance.\n",
    "- Cross-stack memory accesses and local memory accesses should be interleaved.\n",
    "- Avoid stride-1 memory access in slowest moving dimension for 2D and 3D kernel launches.\n",
    "- If memory access pattern changes dynamically over time, a sorting step every nth iteration should be performed to minimize cross-stack memory accesses.\n",
    "- Don't use a memory pool based on a single allocation.\n",
    " \n",
    "Many applications naturally have a concept of locality. These applications are expected to be a good fit for using implicit scaling due to low cross-stack traffic. To illustrate this concept, let's use a stencil kernel as an example. A stencil operates on a grid which can be divided into blocks where majority of stencil computations within a block use stack local data. Only stencil operations that are at border of the block require data from another block, i.e. on another stack. The amount of these cross-stack/cross-border accesses are suppressed by halo to local volume ratio. This concept is illustrated below:\n",
    "\n",
    "<img src=\"assets/2-tile-stencil.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f9120-7c7e-49bb-a88c-906122af38b0",
   "metadata": {},
   "source": [
    "## Explicit Scaling\n",
    "\n",
    "In Explicit Scaling Mode, User is responsible for work distribution and mutli-stack memory placement. Driver and language runtimes provide tools that expose each stack as a separate subdevice that can be programmed independently of all the others.\n",
    "\n",
    "### Creating Sub-Devices\n",
    "\n",
    "In this section we will learn how to create sub-device in SYCL that represent each stack in a multi-stack GPU device.\n",
    "\n",
    "#### Root-device\n",
    "Represents a multi-stack GPU device, containing multiple stacks.\n",
    "\n",
    "#### Sub-Device\n",
    "Represents a stack in multi-stack GPU device. The root-device in such cases can be partitioned to sub-devices, each subdevice corresponding to a physical stack.\n",
    "\n",
    "```\n",
    "vector<device> SubDevices = RootDevice.create_sub_devices<\n",
    "      sycl::info::partition_property::partition_by_affinity_domain>(\n",
    "      sycl::info::partition_affinity_domain::numa);\n",
    "```\n",
    "\n",
    "#### Query for individual stacks in Multi-stack device\n",
    "The example below shows how to query for individual stacks (sub-devices) in Multi-stack device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32f056b-1457-4661-acc4-cfd467f42245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab/sub_device.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/sub_device.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main(){\n",
    "  sycl::queue q;\n",
    "  sycl::device RootDevice =  q.get_device();\n",
    "  std::cout << \"Device: \" << RootDevice.get_info<sycl::info::device::name>() << \"\\n\";\n",
    "  std::cout << \"-EUs  : \" << RootDevice.get_info<sycl::info::device::max_compute_units>() << \"\\n\\n\";\n",
    "\n",
    "  //# Check if GPU can be partitioned (stacks/Stack)\n",
    "  auto partitions = RootDevice.get_info<sycl::info::device::partition_max_sub_devices>();\n",
    "  if(partitions > 0){\n",
    "    std::cout << \"-partition_max_sub_devices: \" << partitions << \"\\n\\n\";\n",
    "    std::vector<sycl::device> SubDevices = RootDevice.create_sub_devices<\n",
    "                  sycl::info::partition_property::partition_by_affinity_domain>(\n",
    "                                                  sycl::info::partition_affinity_domain::numa);\n",
    "    for (auto &SubDevice : SubDevices) {\n",
    "      std::cout << \"Sub-Device: \" << SubDevice.get_info<sycl::info::device::name>() << \"\\n\";\n",
    "      std::cout << \"-EUs      : \" << SubDevice.get_info<sycl::info::device::max_compute_units>() << \"\\n\";\n",
    "    }  \n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89235c1f-b98c-4d45-8d0c-c58ac6dac5bb",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64eee276-8e3a-4d09-933c-c0e62586e4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"gen9\" node\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2211950.v-qsvr-1           ...ub-singleuser u49991          00:00:46 R jupyterhub     \n",
      "2211995.v-qsvr-1           ...sub_device.sh u49991                 0 Q batch          \n",
      "\n",
      "Waiting for Output ██████████████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Thu 23 Feb 2023 09:28:01 AM PST\n",
      "#    Job ID:           2211995.v-qsvr-1.aidevcloud\n",
      "#      User:           u49991\n",
      "# Resources:           cput=75:00:00,neednodes=1:gen9:ppn=2,nodes=1:gen9:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "Device: Intel(R) UHD Graphics P630 [0x3e96]\n",
      "-EUs  : 24\n",
      "\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2211995.v-qsvr-1.aidevcloud\n",
      "# Date: Thu 23 Feb 2023 09:28:14 AM PST\n",
      "########################################################################\n",
      "\n",
      "/glob/supplementary-software/versions/vector-add/vector-add-buffers: error while loading shared libraries: libsycl.so.5: cannot open shared object file: No such file or directory\n",
      "Job Completed in 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_sub_device.sh pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e21a39-af70-4c2f-a2f0-eab04c96ff2f",
   "metadata": {},
   "source": [
    "### Context\n",
    "Contexts are used for resources isolation and sharing. A SYCL context may consist of one or multiple devices. Both root-devices and sub-devices can be within single context, but they all should be of the same SYCL platform. A SYCL program created against a context with multiple devices will be built to each of the root-devices in the context. For context that consists of multiple sub-devices of the same root-device only single build (to that root-device) is needed.\n",
    "\n",
    "#### Unified shared memory\n",
    "Memory allocated against a root-device is accessible by all of its sub-devices (stacks). So if you are operating on a context with multiple sub-devices of the same root-device, then you can use malloc_device on that root-device instead of using the slower malloc_host. Remember that if using malloc_device you'd need an explicit copy out to the host if it necessary to see data there.\n",
    "\n",
    "#### Buffer\n",
    "SYCL buffers are also created against a context and are mapped to the Level-Zero USM allocation discussed above. Current mapping is as follows:\n",
    "\n",
    "For an integrated device, the allocations are made on the host, and are accessible by the host and the device without any copying.\n",
    "\n",
    "Memory buffers for context with sub-devices of the same root-device (possibly including the root-device itself) are allocated on that root-device. Thus they are readily accessible by all the devices in such context. The synchronization with the host is performed by SYCL RT with map/unmap doing implicit copies when necessary.\n",
    "\n",
    "Memory buffers for context with devices from different root-devices in it are allocated on host (thus made accessible to all devices).\n",
    "\n",
    "#### Queue\n",
    "SYCL queue is always attached to a single device in a possibly multi-device context. In order of most performant to least performant, here are some typical scenarios:\n",
    "\n",
    "##### Context associated with single sub-device\n",
    "Creating a context with a single sub-device in it and the queue is attached to that sub-device (stack), in this scheme, the execution/visibility is limited to the single sub-device only, and expected to offer the best performance per stack. See a code example:\n",
    "```cpp\n",
    "  vector<sycl::device> SubDevices = ...;\n",
    "  for (auto &D : SubDevices) {\n",
    "    // Each queue is in its own context, no data sharing across them.\n",
    "    auto Q = sycl::queue(D);\n",
    "    Q.submit([&](sycl::handler &cgh) { ... });\n",
    "  }\n",
    "```\n",
    "\n",
    "##### Context associated with multiple sub-devices\n",
    "Creating a context with multiple sub-devices (multiple stacks) of the same root-device, in this scheme, queues are to be attached to the sub-devices effectively implementing \"explicit scaling\". In this scheme, the root-device should not be passed to such context for better performance. See a code example below:\n",
    "```cpp\n",
    "  vector<sycl::device> SubDevices = ...;\n",
    "  auto C = sycl::context(SubDevices);\n",
    "  for (auto &D : SubDevices) {\n",
    "    // All queues share the same context, data can be shared across\n",
    "    // queues.\n",
    "    auto Q = sycl::queue(C, D);\n",
    "    Q.submit([&](sycl::handler &cgh) { ... });\n",
    "  }\n",
    "```\n",
    "\n",
    "##### Context associated with root device\n",
    "Creating a context with a single root-device in it and the queue is attached to that root-device. In this scheme, the work will be automatically distributed across all sub-devices/stacks via \"implicit scaling\" by the GPU driver, which is the most simple way to enable multi-stack hardware but doesn't offer the possibility to target specific stacks. See a code example below:\n",
    "```cpp\n",
    "  // The queue is attached to the root-device, driver distributes to\n",
    "  // sub - devices, if any.\n",
    "  auto D = sycl::device(sycl::gpu_selector_v);\n",
    "  auto Q = sycl::queue(D);\n",
    "  Q.submit([&](sycl::handler &cgh) { ... });\n",
    "```\n",
    "\n",
    "##### Context associated with multiple root devices\n",
    "Creating Contexts with multiple root-devices (multi-card). In this scheme, the most unrestrictive context with queues attached to different root-devices, which offers most sharing possibilities at the cost of slow access through host memory or explicit copies needed. See a code example:\n",
    "```cpp\n",
    "  auto P = sycl::platform(sycl::gpu_selector_v);\n",
    "  auto RootDevices = P.get_devices();\n",
    "  auto C = sycl::context(RootDevices);\n",
    "  for (auto &D : RootDevices) {\n",
    "    // Context has multiple root-devices, data can be shared across\n",
    "    // multi - card(requires explict copying)\n",
    "    auto Q = queue(C, D);\n",
    "    Q.submit([&](sycl::handler &cgh) { ... });\n",
    "  }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ef168-36d1-43a5-aa07-47ffbbaacafc",
   "metadata": {},
   "source": [
    "### Explicit Scaling Example\n",
    "\n",
    "The example below shows vector addition that is explicitly scaled to execute on multi-stack GPU:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dc0254-0e6f-44c7-96cc-8d1e1954dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lab/vectoradd_explicit_scaling.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/vectoradd_explicit_scaling.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <CL/sycl.hpp>\n",
    "#include <algorithm>\n",
    "#include <cassert>\n",
    "#include <cfloat>\n",
    "#include <iostream>\n",
    "#include <string>\n",
    "namespace sycl;\n",
    "\n",
    "constexpr int num_runs = 10;\n",
    "constexpr size_t scalar = 3;\n",
    "\n",
    "cl_ulong triad(size_t array_size) {\n",
    "\n",
    "  cl_ulong min_time_ns0 = DBL_MAX;\n",
    "  cl_ulong min_time_ns1 = DBL_MAX;\n",
    "\n",
    "  device dev = device(gpu_selector());\n",
    "\n",
    "  std::vector<device> subdev = {};\n",
    "  subdev = dev.create_sub_devices<sycl::info::partition_property::\n",
    "    partition_by_affinity_domain>(sycl::info::partition_affinity_domain::numa);\n",
    "\n",
    "  queue q[2] = {queue(subdev[0], property::queue::enable_profiling{}),\n",
    "    queue(subdev[1], property::queue::enable_profiling{})};\n",
    "\n",
    "  std::cout << \"Running on device: \" <<\n",
    "    q[0].get_device().get_info<info::device::name>() << \"\\n\";\n",
    "  std::cout << \"Running on device: \" <<\n",
    "    q[1].get_device().get_info<info::device::name>() << \"\\n\";\n",
    "\n",
    "  double *A0 = malloc_shared<double>(array_size/2 * sizeof(double), q[0]);\n",
    "  double *B0 = malloc_shared<double>(array_size/2 * sizeof(double), q[0]);\n",
    "  double *C0 = malloc_shared<double>(array_size/2 * sizeof(double), q[0]);\n",
    "\n",
    "  double *A1 = malloc_shared<double>(array_size/2 * sizeof(double), q[1]);\n",
    "  double *B1 = malloc_shared<double>(array_size/2 * sizeof(double), q[1]);\n",
    "  double *C1 = malloc_shared<double>(array_size/2 * sizeof(double), q[1]);\n",
    "\n",
    "  for ( int i = 0; i < array_size/2; i++) {\n",
    "     A0[i]= 1.0; B0[i]= 2.0; C0[i]= 0.0;\n",
    "     A1[i]= 1.0; B1[i]= 2.0; C1[i]= 0.0;\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i< num_runs; i++) {\n",
    "    auto q0_event = q[0].submit([&](handler& h) {\n",
    "        h.parallel_for(array_size/2, [=](id<1> idx) {\n",
    "            C0[idx] = A0[idx] + B0[idx] * scalar;\n",
    "            });\n",
    "        });\n",
    "\n",
    "    auto q1_event = q[1].submit([&](handler& h) {\n",
    "        h.parallel_for(array_size/2, [=](id<1> idx) {\n",
    "            C1[idx] = A1[idx] + B1[idx] * scalar;\n",
    "            });\n",
    "        });\n",
    "\n",
    "    q[0].wait();\n",
    "    q[1].wait();\n",
    "\n",
    "    cl_ulong exec_time_ns0 =\n",
    "      q0_event.get_profiling_info<info::event_profiling::command_end>() -\n",
    "      q0_event.get_profiling_info<info::event_profiling::command_start>();\n",
    "\n",
    "    std::cout << \"stack-0 Execution time (iteration \" << i << \") [sec]: \"\n",
    "      << (double)exec_time_ns0 * 1.0E-9 << \"\\n\";\n",
    "    min_time_ns0 = std::min(min_time_ns0, exec_time_ns0);\n",
    "\n",
    "    cl_ulong exec_time_ns1 =\n",
    "      q1_event.get_profiling_info<info::event_profiling::command_end>() -\n",
    "      q1_event.get_profiling_info<info::event_profiling::command_start>();\n",
    "\n",
    "    std::cout << \"stack-1 Execution time (iteration \" << i << \") [sec]: \"\n",
    "      << (double)exec_time_ns1 * 1.0E-9 << \"\\n\";\n",
    "    min_time_ns1 = std::min(min_time_ns1, exec_time_ns1);\n",
    "  }\n",
    "\n",
    "  // Check correctness\n",
    "  bool error = false;\n",
    "  for ( int i = 0; i < array_size/2; i++) {\n",
    "    if ((C0[i] != A0[i] + scalar * B0[i]) || (C1[i] != A1[i] + scalar * B1[i])) {\n",
    "      std::cout << \"\\nResult incorrect (element \" << i << \" is \" << C0[i] << \")!\\n\";\n",
    "      error = true;\n",
    "    }\n",
    "  }\n",
    "\n",
    "  sycl::free(A0, q[0]);\n",
    "  sycl::free(B0, q[0]);\n",
    "  sycl::free(C0, q[0]);\n",
    "\n",
    "  sycl::free(A1, q[1]);\n",
    "  sycl::free(B1, q[1]);\n",
    "  sycl::free(C1, q[1]);\n",
    "\n",
    "  if (error) return -1;\n",
    "\n",
    "  std::cout << \"Results are correct!\\n\\n\";\n",
    "  return std::max(min_time_ns0, min_time_ns1);\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  size_t array_size;\n",
    "  if (argc > 1 ) {\n",
    "    array_size =  std::stoi(argv[1]);\n",
    "  }\n",
    "  else {\n",
    "    std::cout << \"Run as ./<progname> <arraysize in elements>\\n\";\n",
    "    return 1;\n",
    "  }\n",
    "  std::cout << \"Running with stream size of \" << array_size\n",
    "    << \" elements (\" << (array_size * sizeof(double))/(double)1024/1024 << \"MB)\\n\";\n",
    "\n",
    "  cl_ulong min_time = triad(array_size);\n",
    "\n",
    "  if (min_time == -1) return 1;\n",
    "  size_t triad_bytes = 3 * sizeof(double) * array_size;\n",
    "  std::cout << \"Triad Bytes: \" << triad_bytes << \"\\n\";\n",
    "  std::cout << \"Time in sec (fastest run): \" << min_time * 1.0E-9 << \"\\n\";\n",
    "  double triad_bandwidth = 1.0E-09 * triad_bytes/(min_time*1.0E-9);\n",
    "  std::cout << \"Bandwidth of fastest run in GB/s: \" << triad_bandwidth << \"\\n\";\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37baaaa2-f459-40fe-bc49-aea9a6e74422",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc20331e-4cff-4964-a9a2-ebd878663afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has been submitted to Intel(R) DevCloud and will execute soon.\n",
      "\n",
      "Executing on \"pvc\" node\n",
      "\n",
      "qsub: submit error (Job exceeds queue resource limits MSG=cannot locate feasible nodes (nodes file is empty, all systems are busy, or no nodes have the requested feature))\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2207862.v-qsvr-1           ...ub-singleuser u49991          00:00:29 R jupyterhub     \n",
      "\n",
      "Waiting for Output ██████████████████████████████████████████████████████████████████████\n",
      "\n",
      "TimeOut 60 seconds: Job is still queued for execution, check for output file later (run_sub_device.sh.o)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! ./q.sh run_vectoradd_explicit_scaling.sh pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd369e5b-b026-4259-8348-c49e6610664a",
   "metadata": {},
   "source": [
    "### Explicit Scaling Summary\n",
    "Performance tuning for a multi-stack dGPU imposes a tedious process given the parallelism granularity is at a finer level. However, the fundamentals are similar to CPU performance tuning. To understand performance scaling dominator, pay attention to:\n",
    "\n",
    "- VE utilization efficiency - how kernels utilize the execution resources of different stacks\n",
    "- Data placement - how allocations are spread across the HBM of different stacks\n",
    "- Thread-data affinity: where data \"located\" and how they are accessed in the system\n",
    "\n",
    "In addition, there are several critical programming model concepts for application developers to keep in mind in order to select their favorite scaling scheme for productivity, portability and performance.\n",
    "\n",
    "- Sub-devices (numa_domains) and Sub-sub-devices (subnuma_domains)\n",
    "- Implicit and explicit scaling\n",
    "- Contexts and queues\n",
    "- Environment variables and program language APIs or constructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818c465-2127-4dee-b06d-ab3e5b29f464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
