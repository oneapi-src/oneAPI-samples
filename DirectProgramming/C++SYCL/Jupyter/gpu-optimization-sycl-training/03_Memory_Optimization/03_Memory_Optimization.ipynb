{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Memory Optimization\n",
    "Accelerators have access to a rich memory hierarchy. Utilizing the right level in the hierarchy is critical to getting the best performance.\n",
    "\n",
    "In this section we cover Performance Impact and Consideration when choosing a memory model in SYCL:\n",
    "- [USM vs Buffers](#USM-vs-Buffers)\n",
    "- [Performance Impact of Buffers](#Performance-Impact-of-Buffers)\n",
    "- [Performance Impact of USM Shared Allocations](#Performance-Impact-of-USM-Shared-Allocations)\n",
    "- [Performance Impact of USM Device Allocations](#Performance-Impact-of-USM-Device-Allocations)\n",
    "\n",
    "We will also look at Memory Optimizations and Considerations when using Buffer and USM in detail:\n",
    "- [Memory Optimization: Buffers](031_Memory_Optimization_Buffers.ipynb)\n",
    "- [Memory Optimization: Unified Shared memory](032_Memory_Optimization_USM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d3d97-1614-4bd3-88c8-14925cc46c68",
   "metadata": {},
   "source": [
    "## USM vs Buffers\n",
    "SYCL offers several choices for managing memory on the device. This section discusses the performance tradeoffs, briefly introducing the concepts. For an in-depth explanation, see Data Parallel C++.\n",
    "\n",
    "As with other language features, the specification defines the behavior but not the implementation, so performance characteristics can change between software versions and devices. This guide provide best practices.\n",
    "\n",
    "#### Buffers\n",
    "A buffer is a container for data that can be accessed from a device and the host. The SYCL runtime manages memory by providing APIs for allocating, reading, and writing memory. The runtime is responsible for moving data between host and device, and synchronizing access to the data.\n",
    "\n",
    "#### Unified Shared Memory (USM)\n",
    "USM allows reading and writing of data with conventional pointers, in contrast to buffers where access to data is exclusively by API. USM has two commonly-used variants. Device allocations can only be accessed from the device and therefore require explicit movement of data between host and device. Shared allocations can be referenced from device or host, with the runtime automatically moving memory.\n",
    "\n",
    "We illustrate the tradeoffs between choices by showing the same example program written with the three models. To highlight the issues, we use a program where a GPU and the host cooperatively compute, and therefore need to ship data back and forth.\n",
    "\n",
    "## Performance Impact of Buffers vs USM\n",
    "\n",
    "We illustrate the tradeoffs between choices by showing the same example program written with the three models. To highlight the issues, we use a program where a GPU and the host cooperatively compute, and therefore need to ship data back and forth.\n",
    "\n",
    "In the next sections we will look at Performance Impact of:\n",
    "- Buffers\n",
    "- USM Shared Allocation (Implicit Data Movement)\n",
    "- USM Device Allocation (Explicit Data Movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397ad32-6a8c-4206-8451-f2920f25ec30",
   "metadata": {},
   "source": [
    "### Performance Impact of Buffers\n",
    "\n",
    "Below, we show computation using buffers to manage data. A `buffer` is created and `parallel_for` executes the kernel. The kernel uses the `device_data` accessor to read and write data in `buffer_data`.\n",
    "\n",
    "Note that the code does not specify the location of data. An `accessor` indicates when and where the data is needed, and the SYCL runtime moves the data to the device (if necessary) and then launches the kernel. The `host_accessor` indicates that the data will be read/written on the host. Since the kernel is also read/writing `buffer_data`, the `host_accessor` constructor waits for the kernel to complete and moves data to the host to perform the read/write. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f30b7-8a94-4245-8e7b-af9f8da8c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/buffers.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr int N = 16;\n",
    "  std::vector<int> host_data(N, 10);\n",
    "\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device : \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  //# Modify data array on device\n",
    "  sycl::buffer buffer_data(host_data);\n",
    "  q.submit([&](sycl::handler& h) {\n",
    "    sycl::accessor device_data(buffer_data, h);\n",
    "    h.parallel_for(N, [=](auto i) { device_data[i] += 1; });\n",
    "  });\n",
    "  sycl::host_accessor ha(buffer_data, sycl::read_only);\n",
    "\n",
    "  //# print output\n",
    "  for (int i = 0; i < N; i++) std::cout << ha[i] << \" \";std::cout << \"\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2e938-54c5-4202-b42b-3cd6a5a75266",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aab395-618f-401e-9184-ee5239dd58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_buffers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bff133-c7a2-4011-8068-74a61b72a3e5",
   "metadata": {},
   "source": [
    "#### Performance Considerations - Buffers\n",
    "\n",
    "In the above code, the data access `device_data[i]` appear to be simple array references, but they are implemented by the SYCL runtime with C++ operator overloading. The efficiency of accessor array references depends on the implementation. In practice, device code pays no overhead for overloading compared to direct memory references. The runtime does not know in advance which part of the buffer is accessed, so it must ensure all the data is on the device before the kernel begins. This is true today, but may change over time.\n",
    "\n",
    "The same is not currently true for the `host_accessor`. The runtime does not move all the data to the host. The array references are implemented with more complex code and are significantly slower than native C++ array references. While it is acceptable to reference a small amount of data, computationally intensive algorithms using `host_accessor` pay a large performance penalty and should be avoided.\n",
    "\n",
    "Another issue is concurrency. A `host_accessor` can block kernels that reference the same buffer from launching, even if the accessor is not actively being used to read/write data. Limit the scope that contains the `host_accessor` to the minimum possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c99cb-38e1-4e5b-ad08-dcbab0741fc4",
   "metadata": {},
   "source": [
    "### Performance Impact of USM Shared Allocations\n",
    "\n",
    "Next we show the same algorithm implemented with shared allocations. Data is allocated using `sycl::malloc_shared`. Accessors are not needed because USM-allocated data can be referenced with conventional allows pointers. Therefore, the array references can be implemented with simple indexing. The `parallel_for` ends with a wait to ensure the kernel finishes before the host accesses data. Similar to buffers, the SYCL runtime ensures that all the data is resident on the device before launching a kernel. And like buffers, shared allocations are not copied to the host unless it is referenced. The first time the host references data, there is an operating system page fault, a page of data is copied from device to host, and execution continues. Subsequent references to data on the same page execute at full speed. When a kernel is launched, all of the host-resident pages are flushed back to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec94fc-5508-4085-8f37-09b8ca346800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/usm_shared.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device : \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  //# USM allocation using malloc_shared\n",
    "  constexpr int N = 16;\n",
    "  int *data = sycl::malloc_shared<int>(N, q);\n",
    "\n",
    "  //# Initialize data array\n",
    "  for (int i = 0; i < N; i++) data[i] = 10;\n",
    "\n",
    "  //# Modify data array on device\n",
    "  q.parallel_for(N, [=](auto i) { data[i] += 1; }).wait();\n",
    "\n",
    "  //# print output\n",
    "  for (int i = 0; i < N; i++) std::cout << data[i] << \" \";std::cout << \"\\n\";\n",
    "  sycl::free(data, q);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bce27-4bb2-4a8c-b82f-daa09507212a",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fde75-a970-4eb3-b98d-5361997695f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_usm_shared.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774ee2c-bb5a-4183-8ace-fbb96ebc4871",
   "metadata": {},
   "source": [
    "#### Performance Considerations - USM Shared Allocation\n",
    "Compared to buffers, data references are simple pointers and perform well. However, servicing page faults to bring data to the host incurs overhead in addition to the cost of transferring data. The impact on the application depends on the reference pattern. Sparse random access has the highest overhead and linear scans through data have lower impact from page faults.\n",
    "\n",
    "Since all synchronization is explicit and under programmer control, concurrency is not an issue for a well designed program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7fc65-e589-41ef-b7e8-c32a5c1fd70d",
   "metadata": {},
   "source": [
    "### Performance Impact of USM Device Allocations\n",
    "The same program with device allocation can be found below. With device allocation, data can only be directly accessed on the device and must be explicitly copied to the host. All synchronization between device and host are explicit. The last `memcpy` ends with a wait so the host code will not execute until the asynchronous copy finishes. The `queue` definition uses an `in_order` queue so the memcpy waits for the `parallel_for` to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd65b3-c06c-49f4-8902-7983ac71ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/usm_device.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "  std::cout << \"Device : \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  //# initialize data on host\n",
    "  constexpr int N = 16;\n",
    "  int host_data[N];\n",
    "  for (int i = 0; i < N; i++) host_data[i] = 10;\n",
    "\n",
    "  //# Explicit USM allocation using malloc_device\n",
    "  int *device_data = sycl::malloc_device<int>(N, q);\n",
    "\n",
    "  //# copy mem from host to device\n",
    "  q.memcpy(device_data, host_data, sizeof(int) * N).wait();\n",
    "\n",
    "  //# update device memory\n",
    "  q.parallel_for(N, [=](auto i) { device_data[i] += 1; }).wait();\n",
    "\n",
    "  //# copy mem from device to host\n",
    "  q.memcpy(host_data, device_data, sizeof(int) * N).wait();\n",
    "\n",
    "  //# print output\n",
    "  for (int i = 0; i < N; i++) std::cout << host_data[i] << \" \";std::cout <<\"\\n\";\n",
    "  sycl::free(device_data, q);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fd052-313b-4531-900a-353a080ae044",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cce49-ab89-41f7-a32a-4bfe74aef808",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_usm_device.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c68a4c-dcb4-44cf-90be-3ab1c727db67",
   "metadata": {},
   "source": [
    "#### Performance Considerations - USM Device Allocation\n",
    "Both data movement and synchronization are explicit and under the full control of the programmer. Array references are array references on the host, so it has neither the page faults overhead of shared allocations, nor the overloading overhead associated with buffers. Shared allocations only transfer data that the host actually references, with a memory page granularity. In theory, device allocations allow on-demand movement of any granularity. In practice, fine-grained, asynchronous movement of data can be complex and most programmers simply move the entire data structure once. The requirement for explicit data movement and synchronization makes the code more complicated, but device allocations can provide the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3aa55-98d3-4e13-95fb-451d46a10f29",
   "metadata": {},
   "source": [
    "## Memory Optimizations for Buffers and USM\n",
    "Next we will look at optimizations specific to Buffers and USM:\n",
    "- [Buffers Optimization](031_Memory_Optimization_Buffers.ipynb)\n",
    "- [USM Optimizations](032_Memory_Optimization_USM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b655618-19a2-4b39-822c-9946b7b9c160",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
