{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cdc9c7-528c-496c-be93-ccad715e5a4a",
   "metadata": {},
   "source": [
    "# GPU Optimization\n",
    "In this section we will cover introduction and approaches to GPU Optimization, we will also cover the GPU Execution model overview:\n",
    "- [Introduction](#Introduction)\n",
    "- [Phases in the Optimization Workflow](#Phases-in-the-Optimization-Workflow)\n",
    "- [Profiling and Tuning Your Code](#Profiling-and-Tuning-Your-Code)\n",
    "- [Locality Matters](#Locality-Matters)\n",
    "- [Rightsize Your Work](#Rightsize-Your-Work)\n",
    "- [Parallelization](#Parallelization)\n",
    "- [GPU Execution Model Overview](#GPU-Execution-Model-Overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d33cc-3329-4547-83d9-cec07cc3e2d3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Designing high-performance software requires you to think differently than you might normally do when writing software. You need to be aware of the hardware on which your code is intended to run, and the characteristics that control the performance of that hardware. Your goal is to structure the code such that it produces correct answers, but does so in a way that maximizes the hardware’s ability to execute the code.\n",
    "\n",
    "A unique feature of accelerators is that they are additive to the main CPU on the platform. The primary benefit of using an accelerator is to improve the behavior of your software by partitioning it across the host and accelerator to specialize portions of the computation that run best on the accelerator. Accelerator architectures can offer a benefit through specialization of compute hardware for certain classes of computations. This enables them to deliver best results for software specialized to the accelerator architecture.\n",
    "\n",
    "The primary focus of this document is GPUs. Each section focuses on different topics to guide you in your path to creating optimized solutions. The Intel® oneAPI toolkits provide the languages and development tools you will use to optimize your code. This includes compilers, debuggers, profilers, analyzers, and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbbb69-2981-46b9-a553-ae335e78eb01",
   "metadata": {},
   "source": [
    "## Phases in the Optimization Workflow\n",
    "The first phase in using a GPU is to identify which parts of the application can benefit. This is usually compute-intensive code that has the right ratio of memory accesses to computation, and has the right data dependence patterns to map onto the GPU. GPUs include local memory and typically provide massive parallelism. This determines which characteristics of the code are most important when deciding what to offload.\n",
    "\n",
    "The Intel Advisor tool included in the Intel oneAPI Base Toolkit is designed to analyze your code and help you identify the best opportunities for parallel execution. The profilers in Intel Advisor measure the data movement in your functions, the memory access patterns, and the amount of computation in order to project how code will perform when mapped onto different accelerators. The regions with highest potential benefit should be your first targets for acceleration.\n",
    "\n",
    "GPUs often exploit parallelism at multiple levels. This includes overlap between host and GPU, parallelism across the compute cores, overlap between compute and memory accesses, concurrent pipelines, and vector computation. Using all of these levels of parallelism requires a good understanding of the GPU architecture and capabilities in the libraries and languages at your disposal.\n",
    "#### Keep all the compute resources busy. \n",
    "There must be enough independent tasks to saturate the device and fully utilize all execution resources. For example, if the device has 100 compute cores but you only have one task, 99% of the device will be idle. Often you create many more independent tasks than available compute resources so that the hardware can schedule more work as prior tasks complete.\n",
    "#### Minimize the synchronization between the host and the device. \n",
    "The host launches a kernel on the device and waits for its completion. Launching a kernel incurs overhead, so structure the computation to minimize the number of times a kernel is launched.\n",
    "#### Minimize the data transfer between host and device. \n",
    "Data typically starts on the host and is copied to the device as input to the computation. When a computation is finished, the results must be transferred back to the host. For best performance, minimize data transfer by keeping intermediate results on the device between computations. Reduce the impact of data transfer by overlapping computation and data movement so the compute cores never have to wait for data.\n",
    "#### Keep the data in faster memory and use an appropriate access pattern. \n",
    "GPU architectures have different types of memory and these have different access costs. Registers, caches, and scratchpads are cheaper to access than local memory, but have smaller capacity. When data is loaded into a register, cache line, or memory page, use an access pattern that will use all the data before moving to the next chunk. When memory is banked, use a stride that avoids all the compute cores trying to access the same memory bank simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e2b6a-fbe6-42fc-b4bc-aec2f4c65b07",
   "metadata": {},
   "source": [
    "## Profiling and Tuning Your Code\n",
    "After you have designed your code for high performance, the next step is to measure how it runs on the target accelerator. Add timers to the code, collect traces, and use tools like VTune Profiler to observe the program as it runs. The information collected can identify where hardware is bottlenecked and idle, illustrate how behavior compares with peak hardware roofline, and identify the most important hotspots to focus optimization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef4f0d-16d9-42b3-8793-92a3d4e95b3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Locality Matters\n",
    "An accelerator often has specialized memory with a disjoint address space. An application must allocate or move data into the right memory at the right time.\n",
    "Accelerator memory is arranged in a hierarchy. Registers are more efficient to access than caches, and caches are more efficient to access than main memory. Bringing data closer to the point of execution improves efficiency.\n",
    "\n",
    "There are many ways you can refactor your code to get your data closer to the execution. They will be outlined in the following sections. Here, we focus on three:\n",
    "\n",
    "#### Allocate your data on the accelerator, and when copied there, keep it resident for as long as possible. \n",
    "Your application may have many offloaded regions. If you have data that is common between these regions, it makes sense to amortize the cost of the first copy, and just reuse it in place for the remaining kernel invocations.\n",
    "\n",
    "#### Access contiguous blocks of memory as your kernel executes. \n",
    "The hardware will fetch contiguous blocks into the memory hierarchy, so you have already paid the cost for the entire block. After you use the first element of the block, the remaining elements are almost free to access so take advantage of it.\n",
    "\n",
    "#### Restructure your code into blocks with higher data reuse. \n",
    "In a two-dimensional matrix, you can arrange your work to process one block of elements before moving onto the next block of elements. For example, in a stencil operation you may access the prior row, the current row, and the next row. As you walk over the elements in a block you reuse the data and avoid the cost of requesting it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d1386-de50-4634-a757-c4b4aa7bb01f",
   "metadata": {},
   "source": [
    "## Rightsize Your Work\n",
    "Data-parallel accelerators are designed as throughput engines and are often specialized by replicating execution units many times. This is an easy way of getting higher performance on data-parallel algorithms since more of the elements can be processed at the same time.\n",
    "However, fully utilizing a parallel processor can be challenging. \n",
    "\n",
    "For example, imagine you have 512 execution units, where each execution unit had eight threads, and each thread has 16-element vectors. You need to have a minimum of `512 x 8 x 16 = 65536` parallel activities scheduled at all times just to match this capacity. In addition, if each parallel activity is small, you need another large factor to amortize the cost of submitting this work to the accelerator. Fully utilizing a single large accelerator may require decomposing a computation into millions of parallel activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7cc8e-8c93-4a22-b7c2-cabc193c0cad",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "Parallelism is essential to effective use of accelerators because they contain many independent processing elements that are capable of executing code in parallel. There are three ways to develop parallel code.\n",
    "#### Use a Parallel Programming Language or API\n",
    "There are many parallel programming languages and APIs that can be used to express parallelism. oneAPI supports parallel program development through the __SYCL__ language. oneAPI also has a number of code generation tools to convert these programs into binaries that can be executed on different accelerators. The usual workflow is that a user starts with a serial program, identifies the parts of the code that take a long time to execute (referred to as hotspots), and converts them into parallel kernels that can be offloaded to an accelerator for execution.\n",
    "#### Parallelizing Compilers\n",
    "Directive-based approaches like __OpenMP*__ are another way to develop parallel programs. In a directive-based approach, the programmer provides hints to the compiler about parallelism without modifying the code explicitly. This approach is easier than developing a parallel program from first principles.\n",
    "#### Parallel Libraries\n",
    "oneAPI includes a number of libraries like __oneTBB__, __oneMKL__, __oneDNN__, and __oneVPL__ that provide highly-optimized versions of common computational operations run across a variety of accelerator architectures. Depending on the needs of the application, a user can directly call the functions from these libraries and get efficient implementations of these for the underlying architecture. This is the easiest approach to developing parallel programs, provided the library contains the required functions. For example, machine learning applications can take advantage of the optimized primitives in oneDNN. These libraries have been thoroughly tested for both correctness and performance, which makes programs more reliable when using them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4604b2-6219-4b47-8ab7-764812a58f56",
   "metadata": {},
   "source": [
    "## GPU Execution Model Overview\n",
    "The General Purpose GPU (GPGPU) compute model consists of a host connected to one or more compute devices. Each compute device consists of many GPU Compute Engines (CE), also known as Execution Units (EU) or Vector Engines (VE). The compute devices may also include caches, shared local memory (SLM), high-bandwidth memory (HBM), and so on, as shown in the figure General Purpose Compute Model. Applications are then built as a combination of host software (per the host framework) and kernels submitted by the host to run on the VEs with a predefined decoupling point.\n",
    "\n",
    "General Purpose Compute Model:\n",
    "<img src=\"assets/gpu_compute_model.png\">\n",
    "\n",
    "The GPGPU compute architecture contains two distinct units of execution: a host program and a set of kernels that execute within the context set by the host. The host interacts with these kernels through a command queue. Each device may have its own command queue. When a command is submitted into the command queue, the command is checked for dependencies and then executed on a VE inside the compute unit clusters. Once the command has finished executing, the kernel communicates an end of life cycle through “end of thread” message.\n",
    "\n",
    "The GP execution model determines how to schedule and execute the kernels. When a kernel-enqueue command submits a kernel for execution, the command defines an index space or N-dimensional range. A kernel-instance consists of the kernel, the argument values associated with the kernel, and the parameters that define the index space. When a compute device executes a kernel-instance, the kernel function executes for each point in the defined index space or N-dimensional range.\n",
    "\n",
    "An executing kernel function is called a work-item, and a collection of these work-items is called a work-group. A compute device manages work-items using work-groups. Individual work-items are identified by either a global ID, or a combination of the work-group ID and a local ID inside the work-group.\n",
    "\n",
    "The work-group concept, which essentially runs the same kernel on several unit items in a group, captures the essence of data parallel computing. The VEs can organize work-items in SIMD vector format and run the same kernel on the SIMD vector, hence speeding up the compute for all such applications.\n",
    "\n",
    "A device can compute each work-group in any arbitrary order. Also, the work-items within a single work-group execute concurrently, with no guarantee on the order of progress. A high level work-group function, like Barriers, applies to each work-item in a work-group, to facilitate the required synchronization points.  Such a work-group function must be defined so that all work-items in the work-group encounter precisely the same work-group function.\n",
    "\n",
    "Synchronization can also occur at the command level, where the synchronization can happen between commands in host command-queues. In this mode, one command can depend on execution points in another command or multiple commands.\n",
    "\n",
    "Other types of synchronization based on memory-order constraints inside a program include Atomics and Fences. These synchronization types control how a memory operation of any particular work-item is made visible to another, which offers micro-level synchronization points in the data-parallel compute model.\n",
    "\n",
    "Note that an Intel GPU device is equipped with many Vector Engines (VEs), and each VE is a multi-threaded SIMD processor. Compiler generates SIMD code to map several work-items to be executed simultaneously within a given hardware thread. The SIMD-width for a kernel is a heuristic driven compiler choice. Common SIMD-width examples are SIMD-8, SIMD-16, and SIMD-32.\n",
    "\n",
    "For a given SIMD-width, if all kernel instances within a thread are executing the same instruction, the SIMD lanes can be maximally utilized. If one or more of the kernel instances choose a divergent branch, then the thread executes the two paths of the branch and merges the results by mask. The VE’s branch unit keeps track of such branch divergence and branch nesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb5490-d0b6-4ae5-98fa-b97af84e7677",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
