{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a374a3-84ac-45af-87b6-e048ebd90269",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Shared Local Memory\n",
    "Often work-items need to share data and communicate with each other. On one hand, all work-items in all work-groups can access global memory, so data sharing and communication can occur through global memory. However, due to its lower bandwidth and higher latency, sharing and communication through global memory is less efficient. On the other hand, work-items in a sub-group executing simultaneously in an execution unit (EU) thread can share data and communicate with each other very efficiently, but the number of work-items in a sub-group is usually small and the scope of data sharing and communication is very limited. Memory with higher bandwidth and lower latency accessible to a bigger scope of work-items is very desirable for data sharing communication among work-items. The shared local memory (SLM) in Intel® GPUs is designed for this purpose.\n",
    "\n",
    "<img src=\"assets/localmem.png\">\n",
    "\n",
    "Each SubSlice of Intel GPUs has its own SLM. Access to the SLM is limited to the EUs in the SubSlice or work-items in the same work-group scheduled to execute on the EUs of the same SubSlice. It is local to a SubSlice (or work-group) and shared by EUs in the same SubSlice (or work-items in the same work-group), so it is called SLM. Because it is on-chip in each SubSlice, the SLM has much higher bandwidth and much lower latency than global memory. Because it is accessible to all work-items in a work-group, the SLM can accommodate data sharing and communication among hundreds of work-items, depending on the work-group size.\n",
    "\n",
    "It is often helpful to think of SLM as a work-group managed cache. When a work-group starts, work-items in the work-group can explicitly load data from global memory into SLM. The data stays in SLM during the lifetime of the work-group for faster access. Before the work-group finishes, the data in the SLM can be explicitly written back to the global memory by the work-items. After the work-group completes execution, the data in SLM is also gone and invalid. Data consistency between the SLM and the global memory is the program’s responsibility. Properly using SLM can make a significant performance difference.\n",
    "\n",
    "In this section we cover performance impact and consideration when writing kernel with Shared Local Memory(SLM):\n",
    "\n",
    "- [SLM Size and Work-group Size](#Shared-Local-Memory-Size-and-Work-group-Size)\n",
    "- [Bank Conflicts](#Bank-Conflicts)\n",
    "- [Using SLM as Cache](#Using-SLM-as-Cache)\n",
    "- [Data Sharing and Work-group Barriers](#Data-Sharing-and-Work-group-Barriers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423668dd-a32b-4896-b256-89dcfcc2bf00",
   "metadata": {},
   "source": [
    "## Shared Local Memory Size and Work-group Size\n",
    "Because it is on-chip, the SLM has limited size. How much memory is available to a work-group is device-dependent and can be obtained by querying the device, e.g.:\n",
    "```cpp\n",
    "  std::cout << \"Local Memory Size: \"\n",
    "            << q.get_device().get_info<sycl::info::device::local_mem_size>()\n",
    "            << std::endl;\n",
    "```\n",
    "The output may look like:\n",
    "```\n",
    "Local Memory Size: 65536\n",
    "```\n",
    "The unit of the size is a byte. So this GPU device has 65,536 bytes or 64KB SLM for each work-group.\n",
    "\n",
    "It is important to know the maximum SLM size a work-group can have. In a lot of cases, the total size of SLM available to a work-group is a non-constant function of the number of work-items in the work-group. The maximum SLM size can limit the total number of work-items in a group, i.e. work-group size. For example, if the maximum SLM size is 64KB and each work-item needs 512 bytes of SLM, the maximum work-group size cannot exceed 128."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d4cf1-b663-45b7-979a-c33b8dea834d",
   "metadata": {},
   "source": [
    "## Bank Conflicts\n",
    "The SLM is divided into equally sized memory banks that can be accessed simultaneously for high bandwidth. The total number of banks is device-dependent. At the time of writing, 64 consecutive bytes are stored in 16 consecutive banks at 4-byte (32-bit) granularity. Requests for access to different banks can be serviced in parallel, but requests to different addresses in the same bank cause a bank conflict and are serialized. Bank conflicts adversely affect performance. \n",
    "\n",
    "Consider this example below, if the number of banks is 16, all work-items in the above example will read from and write to different addresses in the same bank. The memory bandwidth is 1/16 of full bandwidth.\n",
    "\n",
    "Changing `slm[j * 16]` to `slm[j]` in the code will not have SLM bank conflicts and achieves full memory bandwidth because every work-item reads from and writes to different addresses in different banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d992c9-f326-432a-a829-127b5809175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/slm_bank.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int N = 32;\n",
    "  auto data = sycl::malloc_shared<int>(N, q);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    sycl::local_accessor<int, 1> slm(sycl::range(32 * 64), h);\n",
    "    h.parallel_for(sycl::nd_range(sycl::range{N}, sycl::range{32}), [=](sycl::nd_item<1> it) {\n",
    "     int i = it.get_global_linear_id();\n",
    "     int j = it.get_local_linear_id();\n",
    "\n",
    "     slm[j * 16] = 0;\n",
    "     sycl::group_barrier(it.get_group());\n",
    "\n",
    "     for (int m = 0; m < 1024 * 1024; m++) {\n",
    "       slm[j * 16] += i * m;\n",
    "       sycl::group_barrier(it.get_group());\n",
    "     }\n",
    "\n",
    "     data[i] = slm[j * 16];\n",
    "   });\n",
    "  });\n",
    "  q.wait();\n",
    "  std::cout << \"Kernel time = \"\n",
    "            << (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>())\n",
    "            << \" ns\\n\";\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89235c1f-b98c-4d45-8d0c-c58ac6dac5bb",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eee276-8e3a-4d09-933c-c0e62586e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_slm_bank.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a91e75-2667-4eb3-8110-f755c3436dcb",
   "metadata": {},
   "source": [
    "## Using SLM as Cache\n",
    "You may sometimes find it more desirable to have the application manage caching of some hot data than to have the hardware do it automatically. With the application managing data caching directly, whenever the data is needed, you know exactly where the data is and the cost to access it. The SLM can be used for this purpose.\n",
    "\n",
    "Consider the following 1-D convolution example, The example convolves an integer array of 8192 x 8192 elements using a kernel array of 257 elements and writes the result to an output array. Each work-item convolves one element. To convolve one element, however, up to 256 neighboring elements are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310bebd-e020-49ce-b033-559f9ef0b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/convolution_global.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr size_t N = 8192 * 8192;\n",
    "  constexpr size_t M = 257;\n",
    "\n",
    "  std::vector<int> input(N);\n",
    "  std::vector<int> output(N);\n",
    "  std::vector<int> kernel(M);\n",
    "\n",
    "  srand(2009);\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    input[i] = rand();\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < M; ++i) {\n",
    "    kernel[i] = rand();\n",
    "  }\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  {\n",
    "    sycl::buffer<int> ibuf(input.data(), N);\n",
    "    sycl::buffer<int> obuf(output.data(), N);\n",
    "    sycl::buffer<int> kbuf(kernel.data(), M);\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor iacc(ibuf, h, sycl::read_only);\n",
    "      sycl::accessor oacc(obuf, h);\n",
    "      sycl::accessor kacc(kbuf, h, sycl::read_only);\n",
    "\n",
    "      h.parallel_for(sycl::nd_range<1>(N, 256), [=](sycl::nd_item<1> it) {\n",
    "           int i = it.get_global_linear_id();\n",
    "           int group = it.get_group()[0];\n",
    "           int gSize = it.get_local_range()[0];\n",
    "\n",
    "           int t = 0;\n",
    "\n",
    "           if ((group == 0) || (group == N / gSize - 1)) {\n",
    "             if (i < M / 2) {\n",
    "               for (int j = M / 2 - i, k = 0; j < M; j++, k++) {\n",
    "                 t += iacc[k] * kacc[j];\n",
    "               }\n",
    "             } else {\n",
    "               if (i + M / 2 >= N) {\n",
    "                 for (int j = 0, k = i - M / 2; j < M / 2 + N - i;\n",
    "                      j++, k++) {\n",
    "                   t += iacc[k] * kacc[j];\n",
    "                 }\n",
    "               } else {\n",
    "                 for (int j = 0, k = i - M / 2; j < M; j++, k++) {\n",
    "                   t += iacc[k] * kacc[j];\n",
    "                 }\n",
    "               }\n",
    "             }\n",
    "           } else {\n",
    "             for (int j = 0, k = i - M / 2; j < M; j++, k++) {\n",
    "               t += iacc[k] * kacc[j];\n",
    "             }\n",
    "           }\n",
    "\n",
    "           oacc[i] = t;\n",
    "         });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_ns = (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6 << \" msec\\n\";\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213603bf-3c5d-4724-8ae4-b830dd9890d3",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa9b8b-1350-49ac-9053-3e21a20f0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_convolution_global.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b31e51-fd16-4686-959b-c3991d531348",
   "metadata": {},
   "source": [
    "Noticing each input element is used by multiple work-items, you can preload all input elements needed by a whole work-group into SLM. Later, when an element is needed, it can be loaded from SLM instead of global memory.\n",
    "\n",
    "When the work-group starts, all input elements needed by each work-item are loaded into SLM. Each work-item, except the first one and the last one, loads one element into SLM. The first work-item loads neighbors on the left of the first element and the last work item loads neighbors on the right of the last element in the SLM. If no neighbors exist, elements in SLM are filled with 0s.\n",
    "Before convolution starts in each work-item, a local barrier is called to make sure all input elements are loaded into SLM.\n",
    "\n",
    "The convolution in each work-item is straightforward. All neighboring elements are loaded from the faster SLM instead of global memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdb7a8-ad9b-408f-883c-c6a66b4b891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/convolution_slm.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr size_t N = 8192 * 8192;\n",
    "  constexpr size_t M = 257;\n",
    "\n",
    "  std::vector<int> input(N);\n",
    "  std::vector<int> output(N);\n",
    "  std::vector<int> kernel(M);\n",
    "\n",
    "  srand(2009);\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    input[i] = rand();\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < M; ++i) {\n",
    "    kernel[i] = rand();\n",
    "  }\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  {\n",
    "    sycl::buffer<int> ibuf(input.data(), N);\n",
    "    sycl::buffer<int> obuf(output.data(), N);\n",
    "    sycl::buffer<int> kbuf(kernel.data(), M);\n",
    "\n",
    "    auto e = q.submit([&](auto &h) {\n",
    "      sycl::accessor iacc(ibuf, h, sycl::read_only);\n",
    "      sycl::accessor oacc(obuf, h);\n",
    "      sycl::accessor kacc(kbuf, h, sycl::read_only);\n",
    "      sycl::local_accessor<int, 1> ciacc(sycl::range(256 + (M / 2) * 2), h);\n",
    "\n",
    "      h.parallel_for(sycl::nd_range<1>(N, 256), [=](sycl::nd_item<1> it) {\n",
    "           int i = it.get_global_linear_id();\n",
    "           int group = it.get_group()[0];\n",
    "           int gSize = it.get_local_range()[0];\n",
    "           int local_id = it.get_local_id()[0];\n",
    "\n",
    "           ciacc[local_id + M / 2] = iacc[i];\n",
    "\n",
    "           if (local_id == 0) {\n",
    "             if (group == 0) {\n",
    "               for (int j = 0; j < M / 2; j++) {\n",
    "                 ciacc[j] = 0;\n",
    "               }\n",
    "             } else {\n",
    "               for (int j = 0, k = i - M / 2; j < M / 2; j++, k++) {\n",
    "                 ciacc[j] = iacc[k];\n",
    "               }\n",
    "             }\n",
    "           }\n",
    "           if (local_id == gSize - 1) {\n",
    "             if (group == it.get_group_range()[0] - 1) {\n",
    "               for (int j = gSize + M / 2;\n",
    "                    j < gSize + M / 2 + M / 2; j++) {\n",
    "                 ciacc[j] = 0;\n",
    "               }\n",
    "             } else {\n",
    "               for (int j = gSize + M / 2, k = i + 1;\n",
    "                    j < gSize + M / 2 + M / 2; j++, k++) {\n",
    "                 ciacc[j] = iacc[k];\n",
    "               }\n",
    "             }\n",
    "           }\n",
    "\n",
    "\t   sycl::group_barrier(it.get_group());\n",
    "\n",
    "           int t = 0;\n",
    "           for (int j = 0, k = local_id; j < M; j++, k++) {\n",
    "             t += ciacc[k] * kacc[j];\n",
    "           }\n",
    "\n",
    "           oacc[i] = t;\n",
    "         });\n",
    "    });\n",
    "    q.wait();\n",
    "\n",
    "    size_t kernel_ns = (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "    std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6 << \" msec\\n\";\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f168950-6305-442a-8d39-c545d0ea0c44",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab451bd-a72c-422c-8283-134c2cec7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_convolution_slm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d704767-9063-4c86-ab99-3c957990f82d",
   "metadata": {},
   "source": [
    "## Data Sharing and Work-group Barriers\n",
    "Let us consider the histogram with 256 bins example.\n",
    "\n",
    "This example has been optimized to use the integer data type instead of long and to share registers in the sub-group so that the private histogram bins can fit in registers for optimal performance. If you need a larger bin size (e.g., 1024), it is inevitable that the private histogram bins will spill to global memory.\n",
    "\n",
    "The histogram bins can be shared by work-items in a work-group as long as each bin is updated atomically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e6308-f92f-4aef-a657-1083d2eb6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/histogram_256_int.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr int N = 4096 * 4096;\n",
    "\n",
    "  std::vector<unsigned long> input(N);\n",
    "  srand(2009);\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    input[i] = (long)rand() % 256;\n",
    "    input[i] |= ((long)rand() % 256) << 8;\n",
    "    input[i] |= ((long)rand() % 256) << 16;\n",
    "    input[i] |= ((long)rand() % 256) << 24;\n",
    "    input[i] |= ((long)rand() % 256) << 32;\n",
    "    input[i] |= ((long)rand() % 256) << 40;\n",
    "    input[i] |= ((long)rand() % 256) << 48;\n",
    "    input[i] |= ((long)rand() % 256) << 56;\n",
    "  }\n",
    "\n",
    "  sycl::queue q{sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "\n",
    "  constexpr int blockSize = 256;\n",
    "  constexpr int NUM_BINS = 256;\n",
    "\n",
    "  std::vector<unsigned long> hist(NUM_BINS, 0);\n",
    "\n",
    "  sycl::buffer<unsigned long, 1> mbuf(input.data(), N);\n",
    "  sycl::buffer<unsigned long, 1> hbuf(hist.data(), NUM_BINS);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    sycl::accessor macc(mbuf, h, sycl::read_only);\n",
    "    auto hacc = hbuf.get_access<sycl::access::mode::atomic>(h);\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range(sycl::range{N / blockSize}, sycl::range{64}), [=\n",
    "    ](sycl::nd_item<1> it) [[intel::reqd_sub_group_size(16)]] {\n",
    "          int group = it.get_group()[0];\n",
    "          int gSize = it.get_local_range()[0];\n",
    "          sycl::sub_group sg = it.get_sub_group();\n",
    "          int sgSize = sg.get_local_range()[0];\n",
    "          int sgGroup = sg.get_group_id()[0];\n",
    "\n",
    "          unsigned int histogram[NUM_BINS];\n",
    "          for (int k = 0; k < NUM_BINS; k++) {\n",
    "            histogram[k] = 0;\n",
    "          }\n",
    "          for (int k = 0; k < blockSize; k++) {\n",
    "            unsigned long x =\n",
    "                sg.load(macc.get_pointer() + group * gSize * blockSize +\n",
    "                        sgGroup * sgSize * blockSize + sgSize * k);\n",
    "#pragma unroll\n",
    "            for (int i = 0; i < 8; i++) {\n",
    "              unsigned int c = x & 0x1FU;\n",
    "              histogram[c] += 1;\n",
    "              x = x >> 8;\n",
    "            }\n",
    "          }\n",
    "\n",
    "          for (int k = 0; k < NUM_BINS; k++) {\n",
    "            hacc[k].fetch_add(histogram[k]);\n",
    "          }\n",
    "        });\n",
    "  });\n",
    "\n",
    "  q.wait();\n",
    "\n",
    "  size_t kernel_ns = (e.template get_profiling_info<sycl::info::event_profiling::command_end>() - e.template get_profiling_info<sycl::info::event_profiling::command_start>());\n",
    "  std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6 << \" msec\" << std::endl;\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9be68-2bf3-4f99-a81f-61b5c96e8b46",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438d7de-dbb5-4b9d-b690-bc060af6a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_histogram_256_int.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600283d2-6440-4e16-95dc-a43f856348af",
   "metadata": {},
   "source": [
    "#### Histogram with SLM\n",
    "\n",
    "When the work-group is started, each work-item in the work-group initializes a portion of the histogram bins in SLM to 0. You could designate one work-item to initialize all the histogram bins, but it is usually more efficient to divide the job among all work-items in the work-group.\n",
    "\n",
    "The work-group barrier after initialization guarantees that all histogram bins are initialized to 0 before any work-item updates any bins.\n",
    "Because the histogram bins in SLM are shared among all work-items, updates to any bin by any work-item has to be atomic.\n",
    "\n",
    "The global histograms are updated once the local histograms in the work-group is completed. But before reading the local SLM bins to update the global bins, a work-group barrier is again called at line 43 to make sure all work-items have completed their work.\n",
    "\n",
    "When SLM data is shared, work-group barriers are often required for work-item synchronization. The barrier has a cost and the cost may increase with a larger work-group size. It is always a good idea to try different work-group sizes to find the best one for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcab52a-9247-45f3-88f5-b73ab3bae0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/histogram_256_slm.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  constexpr int N = 4096 * 4096;\n",
    "\n",
    "  std::vector<unsigned long> input(N);\n",
    "  srand(2009);\n",
    "  for (int i = 0; i < N; ++i) {\n",
    "    input[i] = (long)rand() % 256;\n",
    "    input[i] |= ((long)rand() % 256) << 8;\n",
    "    input[i] |= ((long)rand() % 256) << 16;\n",
    "    input[i] |= ((long)rand() % 256) << 24;\n",
    "    input[i] |= ((long)rand() % 256) << 32;\n",
    "    input[i] |= ((long)rand() % 256) << 40;\n",
    "    input[i] |= ((long)rand() % 256) << 48;\n",
    "    input[i] |= ((long)rand() % 256) << 56;\n",
    "  }\n",
    "\n",
    "  sycl::queue q{sycl::gpu_selector_v,\n",
    "                sycl::property::queue::enable_profiling{}};\n",
    "  std::cout << \"Device: \" << q.get_device().get_info<sycl::info::device::name>()\n",
    "            << \"\\n\";\n",
    "\n",
    "  // Snippet begin\n",
    "  constexpr int NUM_BINS = 256;\n",
    "  constexpr int blockSize = 256;\n",
    "\n",
    "  std::vector<unsigned long> hist(NUM_BINS, 0);\n",
    "  sycl::buffer<unsigned long, 1> mbuf(input.data(), N);\n",
    "  sycl::buffer<unsigned long, 1> hbuf(hist.data(), NUM_BINS);\n",
    "\n",
    "  auto e = q.submit([&](auto &h) {\n",
    "    sycl::accessor macc(mbuf, h, sycl::read_only);\n",
    "    sycl::accessor hacc(hbuf, h, sycl::read_write);\n",
    "    sycl::local_accessor<unsigned int> local_histogram(sycl::range(NUM_BINS),\n",
    "                                                       h);\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range(sycl::range{N / blockSize}, sycl::range{64}),\n",
    "        [=](sycl::nd_item<1> it) {\n",
    "          int group = it.get_group()[0];\n",
    "          int gSize = it.get_local_range()[0];\n",
    "          sycl::sub_group sg = it.get_sub_group();\n",
    "          int sgSize = sg.get_local_range()[0];\n",
    "          int sgGroup = sg.get_group_id()[0];\n",
    "\n",
    "          int factor = NUM_BINS / gSize;\n",
    "          int local_id = it.get_local_id()[0];\n",
    "          if ((factor <= 1) && (local_id < NUM_BINS)) {\n",
    "            sycl::atomic_ref<unsigned int, sycl::memory_order::relaxed,\n",
    "                             sycl::memory_scope::device,\n",
    "                             sycl::access::address_space::local_space>\n",
    "                local_bin(local_histogram[local_id]);\n",
    "            local_bin.store(0);\n",
    "          } else {\n",
    "            for (int k = 0; k < factor; k++) {\n",
    "              sycl::atomic_ref<unsigned int, sycl::memory_order::relaxed,\n",
    "                               sycl::memory_scope::device,\n",
    "                               sycl::access::address_space::local_space>\n",
    "                  local_bin(local_histogram[gSize * k + local_id]);\n",
    "              local_bin.store(0);\n",
    "            }\n",
    "          }\n",
    "          sycl::group_barrier(it.get_group());\n",
    "\n",
    "          for (int k = 0; k < blockSize; k++) {\n",
    "            unsigned long x =\n",
    "                sg.load(macc.get_pointer() + group * gSize * blockSize +\n",
    "                        sgGroup * sgSize * blockSize + sgSize * k);\n",
    "#pragma unroll\n",
    "            for (std::uint8_t shift : {0, 8, 16, 24, 32, 40, 48, 56}) {\n",
    "              constexpr unsigned long mask = 0xFFU;\n",
    "              sycl::atomic_ref<unsigned int, sycl::memory_order::relaxed,\n",
    "                               sycl::memory_scope::device,\n",
    "                               sycl::access::address_space::local_space>\n",
    "                  local_bin(local_histogram[(x >> shift) & mask]);\n",
    "              local_bin += 1;\n",
    "            }\n",
    "          }\n",
    "          sycl::group_barrier(it.get_group());\n",
    "\n",
    "          if ((factor <= 1) && (local_id < NUM_BINS)) {\n",
    "            sycl::atomic_ref<unsigned int, sycl::memory_order::relaxed,\n",
    "                             sycl::memory_scope::device,\n",
    "                             sycl::access::address_space::local_space>\n",
    "                local_bin(local_histogram[local_id]);\n",
    "            sycl::atomic_ref<unsigned long, sycl::memory_order::relaxed,\n",
    "                             sycl::memory_scope::device,\n",
    "                             sycl::access::address_space::global_space>\n",
    "                global_bin(hacc[local_id]);\n",
    "            global_bin += local_bin.load();\n",
    "          } else {\n",
    "            for (int k = 0; k < factor; k++) {\n",
    "              sycl::atomic_ref<unsigned int, sycl::memory_order::relaxed,\n",
    "                               sycl::memory_scope::device,\n",
    "                               sycl::access::address_space::local_space>\n",
    "                  local_bin(local_histogram[gSize * k + local_id]);\n",
    "              sycl::atomic_ref<unsigned long, sycl::memory_order::relaxed,\n",
    "                               sycl::memory_scope::device,\n",
    "                               sycl::access::address_space::global_space>\n",
    "                  global_bin(hacc[gSize * k + local_id]);\n",
    "              global_bin += local_bin.load();\n",
    "            }\n",
    "          }\n",
    "        });\n",
    "  });\n",
    "  // Snippet end\n",
    "  q.wait();\n",
    "\n",
    "  size_t kernel_ns = (e.template get_profiling_info<\n",
    "                          sycl::info::event_profiling::command_end>() -\n",
    "                      e.template get_profiling_info<\n",
    "                          sycl::info::event_profiling::command_start>());\n",
    "  std::cout << \"Kernel Execution Time Average: total = \" << kernel_ns * 1e-6\n",
    "            << \" msec\" << std::endl;\n",
    "\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5af817-f498-44f9-931b-4d26b0c209e7",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593eb2c-f747-443d-9270-c1f655ed6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_histogram_256_slm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438ce8b-62da-48bf-8838-a91e76212dad",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
