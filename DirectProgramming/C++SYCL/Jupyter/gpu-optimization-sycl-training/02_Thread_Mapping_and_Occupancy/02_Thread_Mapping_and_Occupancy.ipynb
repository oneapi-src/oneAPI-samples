{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2550f972-5286-4f9f-a37d-40d4fc6bc60a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Thread Mapping and GPU Occupancy\n",
    "In this section we will cover the how SYCL thread mapping works and how to achieve maximum occupancy with a kernel:\n",
    "- [SYCL Thread Mapping](#SYCL-Thread-Mapping)\n",
    "- [Mapping Work-groups to Xe-cores for Maximum Occupancy](#Mapping-Work-groups-to-Xe-cores-for-Maximum-Occupancy)\n",
    "- [Thread Synchronization](#Thread-Synchronization)\n",
    "- [GPU Occupancy Calculation Examples](#GPU-Occupancy-Calculation-Examples)\n",
    "- [Intel® GPU Occupancy Calculator](#Intel®-GPU-Occupancy-Calculator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696981e-8bf5-439e-aec1-5ce118d8c225",
   "metadata": {},
   "source": [
    "## SYCL Thread Mapping\n",
    "The SYCL* execution model exposes an abstract view of GPU execution. The SYCL thread hierarchy consists of a 1-, 2-, or 3-dimensional grid of work-items. These work-items are grouped into equal sized thread groups called work-groups. Threads in a work-group are further divided into equal sized vector groups called sub-groups (see the illustration that follows).\n",
    "#### Work-item\n",
    "A work-item represents one of a collection of parallel executions of a kernel.\n",
    "#### Sub-group\n",
    "A sub-group represents a short range of consecutive work-items that are processed together as a SIMD vector of length 8, 16, 32, or a multiple of the native vector length of a CPU with Intel® UHD Graphics.\n",
    "#### Work-group\n",
    "A work-group is a 1-, 2-, or 3-dimensional set of threads within the thread hierarchy. In SYCL, synchronization across work-items is only possible with barriers for the work-items within the same work-group.\n",
    "### nd_range\n",
    "An nd_range divides the thread hierarchy into 1-, 2-, or 3-dimensional grids of work-groups. It is represented by the global range, the local range of each work-group.\n",
    "\n",
    "Thread hierarchy:\n",
    "<img src=\"assets/nd_range.png\">\n",
    "\n",
    "The diagram above illustrates the relationship among ND-Range, work-group, sub-group, and work-item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478d584-bab2-4ea0-8b54-5c210486f08c",
   "metadata": {},
   "source": [
    "## Mapping Work-groups to Xe-cores for Maximum Occupancy\n",
    "The rest of this chapter explains how to pick a proper work-group size to maximize the occupancy of the GPU resources. The new terminologies are Xe-core (XC) for Subslice, and Xe Vector Engine (XVE) for Execution Unit(EU).\n",
    "\n",
    "From the key [GPU architecture parameters](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top/xe-arch.html), we summarize the architecture parameters for Gen9, Gen11, Gen12 and Xe-HPC below:\n",
    "\n",
    "Generations | Threads per XVE | XVEs per XC | Threads per XC | XCs | Total XVEs | Total Threads | Max Work Group Size\n",
    "---|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "Intel UHD Graphics P630 SKL (Gen9) | 7 | 8  | 56  | 3 | 24 | 168 | 256\n",
    "Intel Iris Xe ICL (Gen11)      | 7 | 8  | 56  | 8 | 64 | 448 | 256\n",
    "Intel Iris Xe-LP TGL (Gen12)   | 7 | 16 | 112 | 6 | 96 | 672 | 512\n",
    "Intel DataCenter Max 1100 PVC (Xe HPC)| 8 | 8  |  64 | 56 | 448 | 3584 | 1024\n",
    "Intel DataCenter Max 1550 PVC (Xe HPC)| 8 | 8  |  64 | 128 | 1024 | 8192 | 1024\n",
    "\n",
    "<img src=\"assets/xe_core.png\">\n",
    "\n",
    "<img src=\"assets/xe_stack_2.png\">\n",
    "\n",
    "The maximum work-group size is a constraint imposed by the hardware and GPU driver. You can query the maximum work-group using `device::get_info<sycl::info::device::max_work_group_size>()` on the supported size.\n",
    "\n",
    "Let’s start with a simple kernel:\n",
    "```cpp\n",
    "auto command_group = [&](auto &cgh) {\n",
    "      cgh.parallel_for(sycl::range<3>(112, 120, 128), // global range\n",
    "                       [=](item<3> it) {\n",
    "                         // (kernel code)\n",
    "      })\n",
    "}\n",
    "```\n",
    "This kernel contains 1,720,320 work-items structured as a 3D range of 112 x 120 x 128. It leaves the work-group and sub-group size selection to the compiler. To fully utilize the all parallel operations available in the GPU, the compiler must choose a proper work group size.\n",
    "\n",
    "The two most important GPU resources are:\n",
    "- __Thread Contexts:__\n",
    "The kernel should have a sufficient number of threads to utilize the GPU’s thread contexts.\n",
    "- __SIMD Units and SIMD Registers:__\n",
    "The kernel should be organized to vectorize the work-items and utilize the SIMD registers.\n",
    "\n",
    "In a SYCL kernel, the programmer can affect the work distribution by structuring the kernel with proper work-group size, sub-group size, and organizing the work-items for efficient vector execution. Writing efficient vector kernels is covered in a separate section. This chapter focuses on work-group and sub-group size selection.\n",
    "\n",
    "SYCL does not provide a mechanism to directly set the number of threads in a work-group. However, you can use work-group size and SIMD sub-group size to set the number of threads:\n",
    "```\n",
    "Work group size = Threads x SIMD sub-group size\n",
    "```\n",
    "You can increase the sub-group size as long as there are a sufficient number of registers for the kernel after widening. In SYCL, a programmer can explicitly specify sub-group size using `intel::reqd_sub_group_size({16|32})` to override the compiler’s selection.\n",
    "\n",
    "#### Query GPU info\n",
    "\n",
    "The code below creates a SYCL queue with default device selector and queries for `max_work_group_size`, `sub_group_sizes` and `local_mem_size`. This information can be useful to know and choose appropriate work-group, sub-group and local-memory sizes in kernel code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562458b6-320c-45b0-84e1-4d37173389e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpu_support.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "int main() {\n",
    "  sycl::queue q;\n",
    "   \n",
    "  //# Query for device information\n",
    "  auto device_name = q.get_device().get_info<sycl::info::device::name>();\n",
    "  auto wg_size = q.get_device().get_info<sycl::info::device::max_work_group_size>();\n",
    "  auto sg_sizes = q.get_device().get_info<sycl::info::device::sub_group_sizes>();\n",
    "  auto slm_size = q.get_device().get_info<sycl::info::device::local_mem_size>();\n",
    "    \n",
    "  std::cout << \"Device : \" << device_name << \"\\n\";\n",
    "\n",
    "  std::cout << \"Max Work-Group Size : \" << wg_size << \"\\n\";\n",
    "\n",
    "  std::cout << \"Supported Sub-Group Sizes : \";\n",
    "  for (int i=0; i<sg_sizes.size(); i++) std::cout << sg_sizes[i] << \" \"; std::cout << \"\\n\";\n",
    "\n",
    "  std::cout << \"Local Memory Size : \" << slm_size << \"\\n\";\n",
    "   \n",
    "  q.submit([&](sycl::handler &h){\n",
    "    h.parallel_for(sycl::nd_range<3>(sycl::range<3>(112, 120, 128), sycl::range<3>(1, 1, 128)), [=](sycl::nd_item<3> item)[[intel::reqd_sub_group_size(32)]] {\n",
    "     // Kernel Code\n",
    "    });\n",
    "  }).wait();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279af22-cd3a-4d16-9b23-1acbbb8ede5c",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768c60d-b7e3-4302-80ac-d655948d1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_gpu_support.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43319a9f-4443-4cd3-8aaf-54a93c50734f",
   "metadata": {},
   "source": [
    "## Thread Synchronization\n",
    "SYCL provides two synchronization mechanisms that can be called within a kernel function. Both are only defined for work-items within the same work-group. SYCL does not provide any global synchronization mechanism inside a kernel for all work-items across the entire nd_range.\n",
    "\n",
    "- __group_barrier__ inserts a memory fence and blocks the execution of all work-items within the work-group until all work-items have reached its location.\n",
    "\n",
    "- __mem_fence__ inserts a memory fence on global and local memory access across all work-items in a work-group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a76322-50af-4e60-b9ac-49ca7fbc3be5",
   "metadata": {},
   "source": [
    "### Impact of Work-item Synchronization Within Work-group\n",
    "Let’s look at a kernel requiring work-item synchronization:\n",
    "```cpp\n",
    "auto command_group = [&](auto &cgh) {\n",
    "      cgh.parallel_for(sycl::nd_range(sycl::range(112, 120, 128), // global range\n",
    "                                sycl::range(1, R, 128)    // local range\n",
    "                                ),\n",
    "                       [=](sycl::nd_item<3> item) {\n",
    "                         // (kernel code)\n",
    "                         // Internal synchronization\n",
    "                         sycl::group_barrier(item.get_group());\n",
    "                         // (kernel code)\n",
    "      });\n",
    "}\n",
    "```\n",
    "\n",
    "This kernel is similar to the previous example, except it requires work-group barrier synchronization. Work-item synchronization is only available to work-items within the same work-group. You must pick a work-group local range using nd_range and nd_item. All the work-items of a work-group must be allocated to the same Xe-core, which affects Xe-core occupancy and kernel performance.\n",
    "\n",
    "#### Xe-core Utilization for various configurations\n",
    "\n",
    "The table below summarizes the tradeoffs between work-group size, number of threads, Xe-core utilization, and occupancy for __Intel Data Center GPU MAX__\n",
    "\n",
    "      Total Work-Items = (112, 120, 128)\n",
    "\n",
    "      Work-Group Size  = (1, R, 128)\n",
    "\n",
    "      Sub-Group Size = 32\n",
    "\n",
    "Work-Group Size | Threads | Xe-core Utilization | Xe-core Occupancy\n",
    "--- | --- | --- | ---\n",
    "(1, 1, 128) = 128 | 128/32 = 4 | 4/64 = 6.25% | 100% with 16 work-groups\n",
    "(1, 2, 128) = 256 | 256/32 = 8| 8/64 = 12.5% | 100% with 8 work-groups\n",
    "(1, 3, 128) = 384 | 384/32 = 12 | 12/64 = 18.75% | 93.75% with 5 work-groups\n",
    "(1, 4, 128) = 512 | 512/32 = 16 | 16/64 = 25% | 100% with 4 work-groups\n",
    "(1, 5, 128) = 640 | 640/32 = 20 | 20/64 = 31.25% | 93.75% with 3 work-groups\n",
    "(1, 6, 128) = 768 | 768/32 = 24 | 24/64 = 37.5% | 75% with 2 work-groups\n",
    "(1, 7, 128) = 896 | 896/32 = 28 | 28/64 = 44.75% | 89.5% with 2 work-groups\n",
    "(1, 8, 128) = 1024 | 1024/32 = 32 | 32/64 = 50% | 100% with 2 work-groups\n",
    "(1, R>8, 128) = 1024+ |  |  | Fail to launch\n",
    "\n",
    "\n",
    "In this kernel, the local range of work-group is given as range(1, R, 128). Assuming the sub-group size is 32, let’s look at how the values of variable R affect Xe-core occupancy. In the case of R=1, the local group range is (1, 1, 128) and work-group size is 128. The Xe-core allocated for a work-group contains only 4 threads out of 64 available thread contexts (i.e., very low occupancy). However, the system can dispatch 16 work-groups to the same Xe-core to reach full occupancy at the expense of a higher number of dispatches.\n",
    "\n",
    "In the case of R>8, the work-group size will exceed the system-supported maximum work-group size of 1024, and the kernel will fail to launch. In the case of R=6, an Xe-core is only 75% occupied. Note that the driver may still be able to dispatch a partial work-group to an unused Xe-core. However, because of the barrier in the kernel, the partially dispatched work items would not be able to pass the barriers until the rest of the work group is dispatched. In most cases, the kernel’s performance would not benefit much from the partial dispatch. Hence, it is important to avoid this problem by properly choosing the work-group size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb882881-08e8-4e3c-951f-f6a03d12b534",
   "metadata": {},
   "source": [
    "### Impact of Local Memory Within Work-group\n",
    "Let’s look at an example where a kernel allocates local memory for a work-group:\n",
    "```cpp\n",
    "auto command_group =\n",
    "    [&](auto &cgh) {\n",
    "      // local memory variables shared among work items\n",
    "      sycl::local_accessor<int, 1> myLocal(sycl::range(R), cgh);\n",
    "      cgh.parallel_for(sycl::nd_range(sycl::range<3>(112, 120, 128), // global range\n",
    "                                sycl::range<3>(1, R, 128)    // local range\n",
    "                                ),\n",
    "                       [=](sycl::nd_item<3> item) {\n",
    "                         // (work group code)\n",
    "                         myLocal[item.get_local_id()[0]] = ...\n",
    "                       });\n",
    "    }\n",
    "```\n",
    "Because work-group local variables are shared among its work-items, they are allocated in a Xe-core’s SLM. Therefore, this work-group must be allocated to a single Xe-core, same as the intra-group synchronization. In addition, you must also weigh the sizes of local variables under different group size options such that the local variables fit within an Xe-core’s 128KB SLM capacity limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff3c72-e18c-4314-b06f-950962da21e0",
   "metadata": {},
   "source": [
    "## Intel® GPU Occupancy Calculator\n",
    "In summary, a SYCL work-group is typically dispatched to an Xe-core. All the work-items in a work-group shares the same SLM of an Xe-core for intra work-group thread barriers and memory fence synchronization. Multiple work-groups can be dispatched to the same Xe-core if there are sufficient XVE ALUs, SLM, and thread contexts to accommodate them.\n",
    "\n",
    "You can achieve higher performance by fully utilizing all available Xe-cores. Parameters affecting a kernel’s GPU occupancy are work-group size and SIMD sub-group size, which also determines the number of threads in the work-group.\n",
    "\n",
    "The [Intel® GPU Occupancy Calculator](https://oneapi-src.github.io/oneAPI-samples/Tools/GPU-Occupancy-Calculator/index.html) can be used to calculate the occupancy on an Intel GPU for a given kernel, and its work-group parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd5178-49e9-4261-bdc6-536c38dc2222",
   "metadata": {},
   "source": [
    "## GPU Occupancy Calculation Examples\n",
    "Before concluding this section, let’s look at the hardware occupancies from the variants of a simple vector add example.\n",
    "\n",
    "### Intel(R) UHD Graphics P630 (Gen9 GPU)\n",
    "\n",
    "GPU | Threads per XVE | XVEs per XC | Threads per XC | XCs | Total XVEs | Total Threads | Max Work Group Size\n",
    "---|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "Intel UHD Graphics P630 SKL (Gen9) | 7 | 8  | 56  | 3 | 24 | 168 | 256\n",
    "\n",
    "The _vec_add_ example below explicitly specifies the work-group size to `max_work_group_size` (256), SIMD width of 32, and a variable number of work-groups as a function parameter groups.\n",
    "\n",
    "In the absence of intra-work group synchronization, threads from any work-group can be dispatched to any Xe-core. Dividing the number of threads by the number of available thread contexts in the GPU (168) gives us an estimate of the GPU hardware occupancy. \n",
    "\n",
    "```\n",
    "GPU Occupancy = (Work-Groups * (WG-Size/SIMD)) / GPU-Threads\n",
    "\n",
    "GPU Occupancy = (Work-Groups * (256/32)) / 168\n",
    "\n",
    "```\n",
    "\n",
    "| Work-groups | Work-items | Work-group Size | SIMD | Threads work-group | Threads | XeCore Occupancy<br> = Threads / 56 |GPU Occupancy<br> = Threads/168 |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 1  | 256  | 256 | 32 | 8 | 8   | 14.2% | 4.7% |\n",
    "| 2  | 512  | 256 | 32 | 8 | 16  | 28.5% | 9.5% |\n",
    "| 3  | 768  | 256 | 32 | 8 | 24  | 42.8% | 14.2% |\n",
    "| 4  | 1024 | 256 | 32 | 8 | 32  | 57.1% | 19% |\n",
    "| 5  | 1280 | 256 | 32 | 8 | 40  | 71.4% | 23.8% |\n",
    "| 6  | 1536 | 256 | 32 | 8 | 48  | 85.7% | 28.5% |\n",
    "| 7  | 1792 | 256 | 32 | 8 | 56  | 100%  | 33.3% |\n",
    "| 8  | 2048 | 256 | 32 | 8 | 64  | 100%  | 38% |\n",
    "| 12 | 3072 | 256 | 32 | 8 | 96  | 100%  | 57% |\n",
    "| 16 | 4096 | 256 | 32 | 8 | 128 | 100%  | 76% |\n",
    "| 20 | 5120 | 256 | 32 | 8 | 160 | 100%  | 95% |\n",
    "| 24 | 6144 | 256 | 32 | 8 | 192 | 100%  | 100% |\n",
    "\n",
    "### Intel(R) Data Center GPU MAX 1100 (Xe HPC)\n",
    "\n",
    "GPU | Threads per XVE | XVEs per XC | Threads per XC | XCs | Total XVEs | Total Threads | Max Work Group Size\n",
    "---|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "Intel DataCenter Max 1100 PVC (Xe HPC)| 8 | 8  |  64 | 56 | 448 | 3584 | 1024\n",
    "\n",
    "The _vec_add_ example below explicitly specifies the work-group size to `max_work_group_size` (1024), SIMD width of 32, and a variable number of work-groups as a function parameter groups.\n",
    "\n",
    "In the absence of intra-work group synchronization, threads from any work-group can be dispatched to any Xe-core. Dividing the number of threads by the number of available thread contexts in the GPU (3584) gives us an estimate of the GPU hardware occupancy. \n",
    "\n",
    "```\n",
    "GPU Occupancy = (Work-Groups * (WG-Size/SIMD)) / GPU-Threads\n",
    "\n",
    "GPU Occupancy = (Work-Groups * (1024/32)) / 3584\n",
    "\n",
    "```\n",
    "\n",
    "| Work-groups | Work-items | Work-group Size | SIMD | Threads work-group | Threads | XeCore Occupancy<br> = Threads / 64 |GPU Occupancy<br> = Threads/3584 |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| 1  | 1024  | 1024 | 32 | 32 | 32  | 50% | 0.9% |\n",
    "| 2  | 2048  | 1024 | 32 | 32 | 64  | 100% | 1.7% |\n",
    "| 8  | 8192 | 1024 | 32 | 32 | 256  | 100%  | 7.1% |\n",
    "| 16 | 16384 | 1024 | 32 | 32 | 512 | 100%  | 14% |\n",
    "| 24 | 24576 | 1024 | 32 | 32 | 768 | 100%  | 21% |\n",
    "| 32 | 32768 | 1024 | 32 | 32 | 1024 | 100%  | 28% |\n",
    "| 40 | 40690 | 1024 | 32 | 32 | 1280 | 100%  | 35% |\n",
    "| 48 | 49152 | 1024 | 32 | 32 | 1536 | 100%  | 43% |\n",
    "| 56 | 57344 | 1024 | 32 | 32 | 1792 | 100%  | 50% |\n",
    "| 64 | 65536 | 1024 | 32 | 32 | 2048 | 100%  | 57% |\n",
    "| 72 | 73728 | 1024 | 32 | 32 | 2304 | 100%  | 64% |\n",
    "| 80 | 81920 | 1024 | 32 | 32 | 2560 | 100%  | 71% |\n",
    "| 88 | 90112 | 1024 | 32 | 32 | 2816 | 100%  | 78% |\n",
    "| 96 | 98304 | 1024 | 32 | 32 | 3072 | 100%  | 85% |\n",
    "| 104 | 106496 | 1024 | 32 | 32 | 3328 | 100%  | 92% |\n",
    "| 112 | 114688 | 1024 | 32 | 32 | 3584 | 100%  | 100% |\n",
    "| 120 | 122880 | 1024 | 32 | 32 | 3840 | 100%  | 100% |\n",
    "| 128 | 131072 | 1024 | 32 | 32 | 4096 | 100%  | 100% |\n",
    "| 256 | 262144 | 1024 | 32 | 32 | 8192 | 100%  | 100% |\n",
    "| 384 | 393216 | 1024 | 32 | 32 | 12288 | 100%  | 100% |\n",
    "| 512 | 524288 | 1024 | 32 | 32 | 16384 | 100%  | 100% |\n",
    "| 1024 | 1048576 | 1024 | 32 | 32 | 32768 | 100%  | 100% |\n",
    "\n",
    "\n",
    "The `vec_add.cpp` SYCL code example below can be used to run and capture VTune Profiling data to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc2c13-1473-4ad1-955e-fa32abbeab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/vec_add.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "\n",
    "#define N 13762560\n",
    "const int WG_SIZE = 256; // 256,512,1024\n",
    "const int SG_SIZE = 32;  // 8, 16, 32\n",
    "\n",
    "template <int groups, int wg_size, int sg_size>\n",
    "int VectorAdd(sycl::queue &q, std::vector<int> &a, std::vector<int> &b,\n",
    "               std::vector<int> &sum) {\n",
    "  sycl::range num_items{a.size()};\n",
    "\n",
    "  sycl::buffer a_buf(a);\n",
    "  sycl::buffer b_buf(b);\n",
    "  sycl::buffer sum_buf(sum.data(), num_items);\n",
    "  size_t num_groups = groups;\n",
    "\n",
    "  auto start = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
    "  q.submit([&](auto &h) {\n",
    "    sycl::accessor a_acc(a_buf, h, sycl::read_only);\n",
    "    sycl::accessor b_acc(b_buf, h, sycl::read_only);\n",
    "    sycl::accessor sum_acc(sum_buf, h, sycl::write_only, sycl::no_init);\n",
    "\n",
    "    h.parallel_for(\n",
    "        sycl::nd_range<1>(num_groups * wg_size, wg_size), [=\n",
    "    ](sycl::nd_item<1> index) [[intel::reqd_sub_group_size(sg_size)]] {\n",
    "          size_t grp_id = index.get_group()[0];\n",
    "          size_t loc_id = index.get_local_id();\n",
    "          size_t start = grp_id * N;\n",
    "          size_t end = start + N;\n",
    "          for (size_t i = start + loc_id; i < end; i += wg_size) {\n",
    "            sum_acc[i] = a_acc[i] + b_acc[i];\n",
    "          }\n",
    "        });\n",
    "  });\n",
    "  q.wait();\n",
    "  auto end = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
    "  std::cout << \"VectorAdd<\" << groups << \"> completed on device - \"\n",
    "            << (end - start) * 1e-9 << \" seconds\\n\";\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\n",
    "  sycl::queue q;\n",
    "\n",
    "  std::vector<int> a(N), b(N), sum(N);\n",
    "  for (size_t i = 0; i < a.size(); i++){\n",
    "    a[i] = i;\n",
    "    b[i] = i;\n",
    "    sum[i] = 0;\n",
    "  }\n",
    "\n",
    "  std::cout << \"Running on device: \"\n",
    "            << q.get_device().get_info<sycl::info::device::name>() << \"\\n\";\n",
    "  std::cout << \"Vector size: \" << a.size() << \"\\n\";\n",
    "    \n",
    "  VectorAdd<1,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<2,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<3,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<4,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<5,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<6,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<7,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<8,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<12,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<16,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<20,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<24,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<28,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<32,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<40,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<48,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<56,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<64,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<80,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<96,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<112,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<128,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<192,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<256,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<384,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<512,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "  VectorAdd<1024,WG_SIZE,SG_SIZE>(q, a, b, sum);\n",
    "    \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e816c60-f8eb-4602-ac96-ec9acab4294e",
   "metadata": {},
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f4efd-54e9-4c96-806d-0aa1340bcf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./q.sh run_vec_add.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac73977-004d-4875-8b9b-2d883cec3608",
   "metadata": {},
   "source": [
    "#### VTune Analysis\n",
    "You can collect VTune `gpu_hotspots` data and check VTune Profiling Data using the `vtune_collect.sh` script, this script will collect VTune data using command-line options, you can then open the results in the VTune Profiler tool.\n",
    "\n",
    "You can run this command in terminal, login to a GPU nodeand run the script `source vtune_collect.sh`, collecting vtune data will take few minutes and will generate a html report in the same folder, it will also create a folder `vtune_data` folder, this folder can be zipped up and copied to local machine with VTune GUI tool installed to analyze further like shown below:\n",
    "\n",
    "From the VTune output below you can see that Occupancy we predicted earlier doing the manual calculation and the VTune data are the same:\n",
    "\n",
    "<img src=\"assets/vtune_tasks_vec_add_gen9.png\">\n",
    "\n",
    "The GPU Compute Threads Dispatch can be analyzed on VTune Profiler:\n",
    "\n",
    "<img src=\"assets/vtune_threads_vec_add_gen9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a6365-c721-44ba-9d37-1517e7e0aa8a",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Intel GPU Optimization Guide](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top.html) - Up to date resources for Intel GPU Optimization\n",
    "- [SYCL Specification](https://registry.khronos.org/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf) - Latest Specification document for reference\n",
    "- [SYCL Essentials Training](https://github.com/oneapi-src/oneAPI-samples/tree/master/DirectProgramming/C%2B%2BSYCL/Jupyter/oneapi-essentials-training) - Learn basics of C++ SYCL Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
