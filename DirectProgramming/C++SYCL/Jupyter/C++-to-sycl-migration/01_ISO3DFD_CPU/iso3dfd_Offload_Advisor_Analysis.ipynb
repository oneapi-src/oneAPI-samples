{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISO3DFD and Offload Advisor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<ul>\n",
    "    <li>To run offload Advisor and generate a HTML report</li>\n",
    "    <li>To read and understand the metrics in the report</li>\n",
    "    <li>To get a performance estimation of your application on the target hardware</li>\n",
    "    <li>To decide which loops are good candidate for offload</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISO3DFD Application basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, initially we will assume that the developer already has a code running on a CPU. At this stage, it doesn't matter if the code is written in C/C++ or Fortran. Before porting a code on a GPU, the developer should try to understand which parts of the code should be offloaded on the GPU. This step is not always trivial because the developer needs to understand the code but also the hardware that will be used for offloading the computations.\n",
    "The goal of this activity is to show how Intel® Advisor can help deciding what part of the code should or should not be offloaded on the GPU. At the end of this activity, you will be able:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iso3DFD is a wave propagation kernel used in Oil and Gas applications. The resolution of the wave equation is based on finite differences which results in implementing a stencil in a 3D volume.\n",
    "\n",
    "![3D Stencil](img/stencil_mount.png)\n",
    "\n",
    "The general algorithm can be described as follow, using next and prev to store the pressure and vel to store velocity: <br />\n",
    "\n",
    "iterate over time steps<br />\n",
    "|&emsp;    iterate over Z <br />\n",
    "|&emsp;    |&emsp;    iterate over Y <br />\n",
    "|&emsp;    |&emsp;    |&emsp;    iterate over X <br />\n",
    "|&emsp;    |&emsp;    |&emsp;    |&emsp;    tmp = compute stencil for prev[x,y,z] <br />\n",
    "|&emsp;    |&emsp;    |&emsp;    |&emsp;    next[x,y,z] = update(prev[x,y,z], next[x,y,z], vel[x,y,z]) <br />\n",
    "|&emsp;    swap(prev, next) <br />\n",
    "\n",
    "If we try to extract a 2D cut of the volume at different time steps, we can see a perturbation evolving and reflecting on the edges.\n",
    "<table style=\"text-align:left\">\n",
    "    <tr>\n",
    "        <th><img src='img/prop2.png' alt='Propagation at T10'/></th>\n",
    "        <th><img src='img/prop3.png' alt='Propagation at T20'/></th>\n",
    "        <th><img src='img/prop4.png' alt='Propagation at T30'/></th>\n",
    "        <th><img src='img/prop5.png' alt='Propagation at T40'/></th>\n",
    "   </tr>\n",
    "   <tr>\n",
    "        <th style=\"text-align:center\">Propagation at t10</th>\n",
    "        <th style=\"text-align:center\">Propagation at t20</th>\n",
    "        <th style=\"text-align:center\">Propagation at t30</th>\n",
    "        <th style=\"text-align:center\">Propagation at t40</th>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and running iso3DFD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to compile and run for the first time this application. Below is the step by step guide that shows how to optimize iso3dfd. We'll start with code that runs on the CPU, then a basic implementation of GPU offload, then make several iterations to optimize the code. The below uses the Intel® Advisor analysis tool to provide performance analysis of the built applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Offloading modeling\n",
    "The first step is to run offload modeling on the CPU only version of the application (1_CPU_only) to identify code regions that are good opportunities for GPU offload. Running accurate modeling can take considerable time as Intel® Advisor performs analysis on your project. There are two commands provided below. The first is fast, but less accurate and should only be used as a proof of concept. The second will give considerably more helpful and accurate profile information. Depending on your system, modeling may take well over an hour.\n",
    "\n",
    "The SYCL code below shows CPU code: Inspect code, there are no modifications necessary:\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/1_CPU_only.cpp\n",
    "//==============================================================\n",
    "// Copyright   2022 Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "\n",
    "#include <chrono>\n",
    "#include <string>\n",
    "#include <fstream>\n",
    "\n",
    "#include \"Utils.hpp\"\n",
    "\n",
    "void inline iso3dfdIteration(float* ptr_next_base, float* ptr_prev_base,\n",
    "                             float* ptr_vel_base, float* coeff, const size_t n1,\n",
    "                             const size_t n2, const size_t n3) {\n",
    "  auto dimn1n2 = n1 * n2;\n",
    "\n",
    "  // Remove HALO from the end\n",
    "  auto n3_end = n3 - kHalfLength;\n",
    "  auto n2_end = n2 - kHalfLength;\n",
    "  auto n1_end = n1 - kHalfLength;\n",
    "\n",
    "  for (auto iz = kHalfLength; iz < n3_end; iz++) {\n",
    "    for (auto iy = kHalfLength; iy < n2_end; iy++) {\n",
    "      // Calculate start pointers for the row over X dimension\n",
    "      float* ptr_next = ptr_next_base + iz * dimn1n2 + iy * n1;\n",
    "      float* ptr_prev = ptr_prev_base + iz * dimn1n2 + iy * n1;\n",
    "      float* ptr_vel = ptr_vel_base + iz * dimn1n2 + iy * n1;\n",
    "\n",
    "      // Iterate over X\n",
    "      for (auto ix = kHalfLength; ix < n1_end; ix++) {\n",
    "        // Calculate values for each cell\n",
    "        float value = ptr_prev[ix] * coeff[0];\n",
    "        for (int i = 1; i <= kHalfLength; i++) {\n",
    "          value +=\n",
    "              coeff[i] *\n",
    "               (ptr_prev[ix + i] + ptr_prev[ix - i] +\n",
    "                ptr_prev[ix + i * n1] + ptr_prev[ix - i * n1] +\n",
    "                ptr_prev[ix + i * dimn1n2] + ptr_prev[ix - i * dimn1n2]);\n",
    "        }\n",
    "        ptr_next[ix] = 2.0f * ptr_prev[ix] - ptr_next[ix] + value * ptr_vel[ix];\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void iso3dfd(float* next, float* prev, float* vel, float* coeff,\n",
    "             const size_t n1, const size_t n2, const size_t n3,\n",
    "             const size_t nreps) {\n",
    "  for (auto it = 0; it < nreps; it++) {\n",
    "    iso3dfdIteration(next, prev, vel, coeff, n1, n2, n3);\n",
    "    // Swap the pointers for always having current values in prev array\n",
    "    std::swap(next, prev);\n",
    "  }\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "  // Arrays used to update the wavefield\n",
    "  float* prev;\n",
    "  float* next;\n",
    "  // Array to store wave velocity\n",
    "  float* vel;\n",
    "\n",
    "  // Variables to store size of grids and number of simulation iterations\n",
    "  size_t n1, n2, n3;\n",
    "  size_t num_iterations;\n",
    "\n",
    "  if (argc < 5) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    // Parse command line arguments and increase them by HALO\n",
    "    n1 = std::stoi(argv[1]) + (2 * kHalfLength);\n",
    "    n2 = std::stoi(argv[2]) + (2 * kHalfLength);\n",
    "    n3 = std::stoi(argv[3]) + (2 * kHalfLength);\n",
    "    num_iterations = std::stoi(argv[4]);\n",
    "  } catch (...) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  // Validate input sizes for the grid\n",
    "  if (ValidateInput(n1, n2, n3, num_iterations)) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  // Compute the total size of grid\n",
    "  size_t nsize = n1 * n2 * n3;\n",
    "\n",
    "  prev = new float[nsize];\n",
    "  next = new float[nsize];\n",
    "  vel = new float[nsize];\n",
    "\n",
    "  // Compute coefficients to be used in wavefield update\n",
    "  float coeff[kHalfLength + 1] = {-3.0548446,   +1.7777778,     -3.1111111e-1,\n",
    "                                  +7.572087e-2, -1.76767677e-2, +3.480962e-3,\n",
    "                                  -5.180005e-4, +5.074287e-5,   -2.42812e-6};\n",
    "\n",
    "  // Apply the DX, DY and DZ to coefficients\n",
    "  coeff[0] = (3.0f * coeff[0]) / (dxyz * dxyz);\n",
    "  for (auto i = 1; i <= kHalfLength; i++) {\n",
    "    coeff[i] = coeff[i] / (dxyz * dxyz);\n",
    "  }\n",
    "\n",
    "  // Initialize arrays and introduce initial conditions (source)\n",
    "  initialize(prev, next, vel, n1, n2, n3);\n",
    "\n",
    "  std::cout << \"Running on CPU serial version\\n\";\n",
    "  auto start = std::chrono::steady_clock::now();\n",
    "\n",
    "  // Invoke the driver function to perform 3D wave propagation 1 thread serial\n",
    "  // version\n",
    "  iso3dfd(next, prev, vel, coeff, n1, n2, n3, num_iterations);\n",
    "\n",
    "  auto end = std::chrono::steady_clock::now();\n",
    "  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n",
    "                  .count();\n",
    "\n",
    "  printStats(time, n1, n2, n3, num_iterations);\n",
    "\n",
    "  delete[] prev;\n",
    "  delete[] next;\n",
    "  delete[] vel;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the application is created, we can run it from the command line by using few parameters as following:\n",
    "src/1_CPU_only 256 256 256 100\n",
    "<ul>\n",
    "    <li>bin/1_CPU_only is the binary</li>\n",
    "    <li>128 128 128 are the size for the 3 dimensions, increasing it will result in more computation time</li>    \n",
    "    <li>100 is the number of time steps</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_cpu_only.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_cpu_only.sh; else ./run_cpu_only.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have been able to compile and execute the code, let's start profiling what should be offloaded !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Offload Advisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current code is running on a CPU and is actually not even threaded. For Intel® Offload Advisor, it doesn't matter if your code is already threaded. Advisor will run several analyses on your application to extract several metric such as the number of operations, the number of memory transfers, data dependencies and many more.\n",
    "We are going to detail each of these steps. Remember that our goal here is to decide if some of our loops are good candidates for offload. In this section, we will generate the report assuming that we want to offload our computations on a GPU on Intel Devcloud.\n",
    "Keep in mind that if you want Advisor to extract as much information as possible, you need to compile your application with debug information (-g with intel compilers).\n",
    "\n",
    "The first step is to run offload modeling on the CPU only version of the application (1_CPU_only) to identify code regions that are good opportunities for GPU offload. Running accurate modeling can take considerable time as Intel® Advisor performs analysis on your project. There are two commands provided below. The first is fast, but less accurate and should only be used as a proof of concept. The second will give considerably more helpful and accurate profile information. Depending on your system, modeling may take well over an hour.\n",
    "\n",
    "Run one of the following from the from the \"build\" directory\n",
    "```\n",
    "advisor --collect=offload --config=pvc_xt_448xve --project-dir=./../advisor/1_cpu -- ./build/src/1_CPU_only 256 256 256 20\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple method: Use Collection Presets\n",
    "For the Offload Modeling perspective, Intel Advisor has a special collection mode --collect=offload that allows you to run several analyses using only oneIntel Advisor CLI command. When you run the collection, it sequentially runs data collection and performance modeling steps.\n",
    " In the commands below, make sure to replace the myApplication with your application executable path and name before executing a command. If your application requires additional command line options, add them after the executable name.\n",
    "```\n",
    "advisor --collect=offload --project-dir=./advi_results -- ./myApplication \n",
    "```\n",
    "The iso3DFD CPU code can be run using\n",
    "```\n",
    "advisor --collect=offload --config=pvc_xt_448xve --project-dir=./../advisor/1_cpu -- ./build/src/1_CPU_only 256 256 256 20\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_offload_advisor.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_offload_advisor.sh; else ./run_offload_advisor.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Method to run the Offload Advisor\n",
    "\n",
    "### Running the Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Survey is usually the first analysis you want to run with Intel® Advisor. The survey is mainly used to time your application as well as the different loops and functions. There is a minimal performance penalty at this stage. This analysis is also used to extract information embedded by the compiler in your binary. These information are mainly related to vectorization (why or why not vectorization, vectorization efficiency, etc).\n",
    "\n",
    "```\n",
    "advisor --collect=survey --auto-finalize --static-instruction-mix -- ./build/src/1_CPU_only 128 128 128 20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_advisor_survey.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_advisor_survey.sh; else ./run_advisor_survey.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the trip count and cache simulation \n",
    "The second step to decide what should be offloaded, will be to run the trip count analysis as well as the cache simulation. This second step uses instrumentation to count how many iterations you are running in each loops. Adding the option -flop will also provide the precise number of operations executed in each of your code sections.\n",
    "\n",
    "In this step, we also ask advisor to run a cache simulation, specifying the memory configuration of the hardware we are targeting for offload\n",
    "\n",
    "Be aware that this step will take much more time than simply running your application. You can expect something like a 10x speed-down due to the many parameters Advisor tries to extract during the run.\n",
    "```\n",
    "advisor --collect=tripcounts --flop --auto-finalize --target-device=gen9_gt2 -- ./build/src/1_CPU_only 128 128 128 20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_advisor_tripcounts.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_advisor_tripcounts.sh; else ./run_advisor_tripcounts.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Dependency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forcing threading in location where it is not supposed to happen might be quite dangerous and result in computation changes. In order to avoid parallelizing loops that cannot be parallelized, it is possible to run an additional analysis called the dependency analysis. This step was initially used to help users implementing vectorization but Offload Advisor can also use it to recommend what can be offloaded or not.\n",
    "\n",
    "```\n",
    "advisor -collect=dependencies --loop-call-count-limit=16 --select markup=gpu_generic --filter-reductions --project-dir=./advi_results -- ./myApplication\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_cpu_only.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_cpu_only.sh; else ./run_cpu_only.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the HTML report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally reached the last step and only need to generate our HTML report for offloading on GPU. This report will show us:\n",
    "<ul>\n",
    "    <li>What is the expected speedup on the target device</li>\n",
    "    <li>What will most likely be our bottleneck on the target device</li>\n",
    "    <li>What are the good candidates for offload</li>\n",
    "    <li>What are the loops that should not be offloaded</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Advisor report overview\n",
    "To display the report, just execute the following frame. In practice, the report will be available in the folder you defined as --out-dir in the previous script.\n",
    "\n",
    "[View the report in HTML](reports/advisor_report_overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='reports/advisor-report.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<html><body><span style=\"color:green\"><h1>Survey</h1></span></body></html>\n",
    "\n",
    "[Tell us how we did in this module with a short survey. We will use your feedback to improve the quality and impact of these learning materials. Thanks!](https://intel.az1.qualtrics.com/jfe/form/SV_6m4G7BXPNSS7FBz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='reports/advisor_report_overview.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Advisor report\n",
    "To display the report, just execute the following frame. In practice, the report will be available in the folder you defined as --out-dir in the previous script. \n",
    "\n",
    "[View the report in HTML](reports/report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='reports/report.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate in the report and try to understand what should be the speedup, what should be offloaded and what should not be offloaded. Navigate also to the \"Offloaded Regions\" tab to see exactly which part of the code should run on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to remember these complex command lines ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think that the command lines we used are too complex to be remembered and you are right ! This is the reason why Advisor provides an option called --dry-run that will give you all the independent commands you need to use to run this analysis from scratch.\n",
    "\n",
    "Generate pre-configured command lines with --collect=offload and the --dry-run option.\n",
    "The option generates:\n",
    "* Commands for the Intel Advisor CLI collection workflow\n",
    "* Commands that correspond to a specified accuracy level\n",
    "\n",
    "```\n",
    "advisor --collect=offload --accuracy=low --dry-run --project-dir=./advi_results -- ./myApplication\n",
    "```\n",
    "\n",
    "```\n",
    "advisor --collect=offload --accuracy=low --dry-run -- ./build/src/1_CPU_only 128 128 128 20\n",
    "```\n",
    "--config can use the following devices:\n",
    "<ul>    \n",
    "    <li>pvc_xt_448xve</li>\n",
    "    <li>xehpg_512xve</li>\n",
    "    <li>xehpg_256xve</li>\n",
    "    <li>gen12_tgl</li>\n",
    "    <li>gen12_dg1</li>\n",
    "    <li>gen11_icl</li>\n",
    "    <li>gen11_gt2</li>    \n",
    "    <li>gen9_gt2</li>\n",
    "    <li>gen9_gt3</li>\n",
    "    <li>gen9_gt4</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_dry_run_advisor; if [ -x \"$(command -v qsub)\" ]; then ./q run_dry_run_advisor.sh; else ./run_dry_run_advisor.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Next Iteration of implemeting the parallelism using SYCL\n",
    "In this module\n",
    "\n",
    "* Started with serial C++ code that runs on the CPU. \n",
    "* Used the Intel® Advisor analysis tool to provide performance analysis/projections of the application.\n",
    "* Ran offload modeling on the CPU version of the application to identify code regions that are good opportunities for GPU offload.\n",
    "* Reviewed the Offload report and we are ready to build an implementation of GPU offload using SYCL\n",
    "* We will also make several iterations of the SYCL code to optimize the code for GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
