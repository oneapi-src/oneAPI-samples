{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISO3DFD on a GPU and Index computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Understand how to address the application being compute bound by reducing index calculations</li>    \n",
    "    <li>Run roofline analysis and the VTune reports again to gauge the results and look for additional opportunities</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iso3DFD reducing the index calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous activity, we used Intel® Advisor roofline analysis to decide on if the application is compute bound and specifically that the kernels have high arithmetic intensity and we are bounded by the INT operations which is all about index computations.\n",
    "What we need to solve, is to provide to the kernel the good index (offset in the original code). SYCL provides this information through an iterator that is sent by the runtime to the function. This iterator allows to identify the position of the current iteration in the 3D space. It can be accessed on 3 dimensions by calling: it.get_global_id(0), it.get_global_id(1), it.get_global_id(2).\n",
    "\n",
    "In this notebook, we'll address the problem being compute bound in kernels by reducing index calculations by changing how we calculate indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optimizing the Indexing of the Iso3DFD application\n",
    "The 3_GPU_linear version of the sample has implemented the index calculations optimization, where we can change the 3D indexing to 1D. We need to flatten the buffers change how we calculate location in the memory for each kernel, and change how we are accessing the neighbors.\n",
    "* For index calculations optimization, we need to change the 3D indexing to 1D and also need to flatten the buffers\n",
    "\n",
    "```\n",
    "// Create 1D SYCL range for buffers which include HALO\n",
    "range<1> buffer_range(n1 * n2 * n3);\n",
    "// Create buffers using SYCL class buffer\n",
    "buffer next_buf(next, buffer_range);\n",
    "buffer prev_buf(prev, buffer_range);\n",
    "buffer vel_buf(vel, buffer_range);\n",
    "buffer coeff_buf(coeff, range(kHalfLength + 1));\n",
    "```\n",
    "\n",
    "* We change how we calculate location in the memory for each kernel\n",
    "\n",
    "```\n",
    "// Start of device code\n",
    "// Add offsets to indices to exclude HALO\n",
    "int n2n3 = n2 * n3;\n",
    "int i = nidx[0] + kHalfLength;\n",
    "int j = nidx[1] + kHalfLength;\n",
    "int k = nidx[2] + kHalfLength;\n",
    "\n",
    "// Calculate linear index for each cell\n",
    "int idx = i * n2n3 + j * n3 + k;\n",
    "\n",
    "```\n",
    "* We change how we are accessing the neighbors\n",
    "\n",
    "```\n",
    "// Calculate values for each cell\n",
    "    float value = prev_acc[idx] * coeff_acc[0];\n",
    "#pragma unroll(8)\n",
    "    for (int x = 1; x <= kHalfLength; x++) {\n",
    "      value +=\n",
    "          coeff_acc[x] * (prev_acc[idx + x]        + prev_acc[idx - x] +\n",
    "                          prev_acc[idx + x * n3]   + prev_acc[idx - x * n3] +\n",
    "                          prev_acc[idx + x * n2n3] + prev_acc[idx - x * n2n3]);\n",
    "    }\n",
    "    next_acc[idx] = 2.0f * prev_acc[idx] - next_acc[idx] +\n",
    "                    value * vel_acc[idx];\n",
    "// End of device code\n",
    "});\n",
    "});\n",
    "\n",
    "```\n",
    "We will run roofline analysis and the VTune reports again to gauge the results and look for additional opportunities for optimization based on 3_GPU_linear.\n",
    "\n",
    "The SYCL code below shows Iso3dFD GPU code using SYCL with Index optimizations: Inspect code, there are no modifications necessary:\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/3_GPU_linear_USM.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <chrono>\n",
    "#include <string>\n",
    "#include <fstream>\n",
    "\n",
    "#include \"Utils.hpp\"\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "bool iso3dfd(sycl::queue &q, float *ptr_next, float *ptr_prev,\n",
    "                   float *ptr_vel, float *ptr_coeff, size_t n1, size_t n2,\n",
    "                   size_t n3, unsigned int nIterations) {\n",
    "  auto nx = n1;\n",
    "  auto nxy = n1*n2;\n",
    "  auto grid_size = n1*n2*n3;\n",
    "  auto b1 = kHalfLength;\n",
    "  auto b2 = kHalfLength;\n",
    "  auto b3 = kHalfLength;\n",
    "  \n",
    "  // Create 3D SYCL range for kernels which not include HALO\n",
    "  range<3> kernel_range(n1 - 2 * kHalfLength, n2 - 2 * kHalfLength,\n",
    "                        n3 - 2 * kHalfLength);\n",
    "\n",
    "  auto next = sycl::aligned_alloc_device<float>(64, grid_size + 16, q);\n",
    "  next += (16 - b1);\n",
    "  q.memcpy(next, ptr_next, sizeof(float)*grid_size);\n",
    "  auto prev = sycl::aligned_alloc_device<float>(64, grid_size + 16, q);\n",
    "  prev += (16 - b1);\n",
    "  q.memcpy(prev, ptr_prev, sizeof(float)*grid_size);\n",
    "  auto vel = sycl::aligned_alloc_device<float>(64, grid_size + 16, q);\n",
    "  vel += (16 - b1);\n",
    "  q.memcpy(vel, ptr_vel, sizeof(float)*grid_size);\n",
    "  auto coeff = sycl::aligned_alloc_device<float>(64, kHalfLength + 1, q);\n",
    "  //coeff += (16 - b1);\n",
    "  q.memcpy(coeff, ptr_coeff, sizeof(float)*(kHalfLength+1));\n",
    "  q.wait();\n",
    "\n",
    "  for (auto it = 0; it < nIterations; it += 1) {\n",
    "    // Submit command group for execution\n",
    "    q.submit([&](handler& h) {\n",
    "      // Send a SYCL kernel(lambda) to the device for parallel execution\n",
    "      // Each kernel runs single cell\n",
    "      h.parallel_for(kernel_range, [=](id<3> idx) {\n",
    "        // Start of device code\n",
    "        // Add offsets to indices to exclude HALO\n",
    "        int n2n3 = n2 * n3;\n",
    "        int i = idx[0] + kHalfLength;\n",
    "        int j = idx[1] + kHalfLength;\n",
    "        int k = idx[2] + kHalfLength;\n",
    "\n",
    "        // Calculate linear index for each cell\n",
    "        int gid = i * n2n3 + j * n3 + k;\n",
    "        auto value = coeff[0] * prev[gid];\n",
    "          \n",
    "        // Calculate values for each cell\n",
    "#pragma unroll(8)\n",
    "        for (int x = 1; x <= kHalfLength; x++) {\n",
    "          value += coeff[x] * (prev[gid + x] + prev[gid - x] +\n",
    "                               prev[gid + x * n3]   + prev[gid - x * n3] +\n",
    "                               prev[gid + x * n2n3] + prev[gid - x * n2n3]);\n",
    "        }\n",
    "        next[gid] = 2.0f * prev[gid] - next[gid] + value * vel[gid];\n",
    "          \n",
    "        // End of device code\n",
    "      });\n",
    "    }).wait();\n",
    "\n",
    "    // Swap the buffers for always having current values in prev buffer\n",
    "    std::swap(next, prev);\n",
    "  }\n",
    "  q.memcpy(ptr_prev, prev, sizeof(float)*grid_size);\n",
    "\n",
    "  sycl::free(next - (16 - b1),q);\n",
    "  sycl::free(prev - (16 - b1),q);\n",
    "  sycl::free(vel - (16 - b1),q);\n",
    "  sycl::free(coeff,q);\n",
    "  return true;\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "  // Arrays used to update the wavefield\n",
    "  float* prev;\n",
    "  float* next;\n",
    "  // Array to store wave velocity\n",
    "  float* vel;\n",
    "\n",
    "  // Variables to store size of grids and number of simulation iterations\n",
    "  size_t n1, n2, n3;\n",
    "  size_t num_iterations;\n",
    "\n",
    "  // Flag to verify results with CPU version\n",
    "  bool verify = false;\n",
    "\n",
    "  if (argc < 5) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    // Parse command line arguments and increase them by HALO\n",
    "    n1 = std::stoi(argv[1]) + (2 * kHalfLength);\n",
    "    n2 = std::stoi(argv[2]) + (2 * kHalfLength);\n",
    "    n3 = std::stoi(argv[3]) + (2 * kHalfLength);\n",
    "    num_iterations = std::stoi(argv[4]);\n",
    "    if (argc > 5) verify = true;\n",
    "  } catch (...) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  // Validate input sizes for the grid\n",
    "  if (ValidateInput(n1, n2, n3, num_iterations)) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  // Create queue and print target info with default selector and in order\n",
    "  // property\n",
    "  queue q(default_selector_v, {property::queue::in_order()});\n",
    "  std::cout << \" Running linear indexed GPU version\\n\";\n",
    "  printTargetInfo(q);\n",
    "\n",
    "  // Compute the total size of grid\n",
    "  size_t nsize = n1 * n2 * n3;\n",
    "\n",
    "  prev = new float[nsize];\n",
    "  next = new float[nsize];\n",
    "  vel = new float[nsize];\n",
    "\n",
    "  // Compute coefficients to be used in wavefield update\n",
    "  float coeff[kHalfLength + 1] = {-3.0548446,   +1.7777778,     -3.1111111e-1,\n",
    "                                  +7.572087e-2, -1.76767677e-2, +3.480962e-3,\n",
    "                                  -5.180005e-4, +5.074287e-5,   -2.42812e-6};\n",
    "\n",
    "  // Apply the DX, DY and DZ to coefficients\n",
    "  coeff[0] = (3.0f * coeff[0]) / (dxyz * dxyz);\n",
    "  for (auto i = 1; i <= kHalfLength; i++) {\n",
    "    coeff[i] = coeff[i] / (dxyz * dxyz);\n",
    "  }\n",
    "\n",
    "  // Initialize arrays and introduce initial conditions (source)\n",
    "  initialize(prev, next, vel, n1, n2, n3);\n",
    "\n",
    "  auto start = std::chrono::steady_clock::now();\n",
    "\n",
    "  // Invoke the driver function to perform 3D wave propagation offloaded to\n",
    "  // the device\n",
    "  iso3dfd(q, next, prev, vel, coeff, n1, n2, n3, num_iterations);\n",
    "\n",
    "  auto end = std::chrono::steady_clock::now();\n",
    "  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n",
    "                  .count();\n",
    "  printStats(time, n1, n2, n3, num_iterations);\n",
    "\n",
    "  // Verify result with the CPU serial version\n",
    "  if (verify) {\n",
    "    VerifyResult(prev, next, vel, coeff, n1, n2, n3, num_iterations);\n",
    "  }\n",
    "\n",
    "  delete[] prev;\n",
    "  delete[] next;\n",
    "  delete[] vel;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the application is created, we can run it from the command line by using few parameters as following:\n",
    "src/3_GPU_linear 1024 1024 1024 100\n",
    "<ul>\n",
    "    <li>bin/3_GPU_linear is the binary</li>\n",
    "    <li>1024 1024 1024 are the size for the 3 dimensions, increasing it will result in more computation time</li>    \n",
    "    <li>100 is the number of time steps</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpu_linear_usm.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_gpu_linear_usm.sh; else ./run_gpu_linear_usm.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISO3DFD Linear using Buffers and Accessors\n",
    "\n",
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/3_GPU_linear.cpp\n",
    "//==============================================================\n",
    "// Copyright © Intel Corporation\n",
    "//\n",
    "// SPDX-License-Identifier: MIT\n",
    "// =============================================================\n",
    "#include <sycl/sycl.hpp>\n",
    "#include <chrono>\n",
    "#include <string>\n",
    "#include <fstream>\n",
    "\n",
    "#include \"Utils.hpp\"\n",
    "\n",
    "using namespace sycl;\n",
    "\n",
    "void iso3dfd(sycl::queue &q, float *ptr_next, float *ptr_prev,\n",
    "                   float *ptr_vel, float *ptr_coeff, size_t n1, size_t n2,\n",
    "                   size_t n3, size_t n1_block, size_t n2_block, size_t n3_block,\n",
    "                   size_t end_z, unsigned int nIterations) {\n",
    "  // Create 3D SYCL range for kernels which not include HALO\n",
    "  range<3> kernel_range(n1 - 2 * kHalfLength, n2 - 2 * kHalfLength,\n",
    "                        n3 - 2 * kHalfLength);\n",
    "  // Create 1D SYCL range for buffers which include HALO\n",
    "  range<1> buffer_range(n1 * n2 * n3);\n",
    "  // Create buffers using SYCL class buffer\n",
    "  buffer next_buf(next, buffer_range);\n",
    "  buffer prev_buf(prev, buffer_range);\n",
    "  buffer vel_buf(vel, buffer_range);\n",
    "  buffer coeff_buf(coeff, range(kHalfLength + 1));\n",
    "\n",
    "  for (auto it = 0; it < nreps; it++) {\n",
    "    // Submit command group for execution\n",
    "    q.submit([&](handler& h) {\n",
    "      // Create accessors\n",
    "      accessor next_acc(next_buf, h);\n",
    "      accessor prev_acc(prev_buf, h);\n",
    "      accessor vel_acc(vel_buf, h, read_only);\n",
    "      accessor coeff_acc(coeff_buf, h, read_only);\n",
    "\n",
    "      // Send a SYCL kernel(lambda) to the device for parallel execution\n",
    "      // Each kernel runs single cell\n",
    "      h.parallel_for(kernel_range, [=](id<3> nidx) {\n",
    "        // Start of device code\n",
    "        // Add offsets to indices to exclude HALO\n",
    "        int n2n3 = n2 * n3;\n",
    "        int i = nidx[0] + kHalfLength;\n",
    "        int j = nidx[1] + kHalfLength;\n",
    "        int k = nidx[2] + kHalfLength;\n",
    "\n",
    "        // Calculate linear index for each cell\n",
    "        int idx = i * n2n3 + j * n3 + k;\n",
    "\n",
    "        // Calculate values for each cell\n",
    "        float value = prev_acc[idx] * coeff_acc[0];\n",
    "#pragma unroll(8)\n",
    "        for (int x = 1; x <= kHalfLength; x++) {\n",
    "          value +=\n",
    "              coeff_acc[x] * (prev_acc[idx + x]        + prev_acc[idx - x] +\n",
    "                              prev_acc[idx + x * n3]   + prev_acc[idx - x * n3] +\n",
    "                              prev_acc[idx + x * n2n3] + prev_acc[idx - x * n2n3]);\n",
    "        }\n",
    "        next_acc[idx] = 2.0f * prev_acc[idx] - next_acc[idx] +\n",
    "                            value * vel_acc[idx];\n",
    "        // End of device code\n",
    "      });\n",
    "    });\n",
    "\n",
    "    // Swap the buffers for always having current values in prev buffer\n",
    "    std::swap(next_buf, prev_buf);\n",
    "  }\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "  // Arrays used to update the wavefield\n",
    "  float* prev;\n",
    "  float* next;\n",
    "  // Array to store wave velocity\n",
    "  float* vel;\n",
    "\n",
    "  // Variables to store size of grids and number of simulation iterations\n",
    "  size_t n1, n2, n3;\n",
    "  size_t num_iterations;\n",
    "\n",
    "  // Flag to verify results with CPU version\n",
    "  bool verify = false;\n",
    "\n",
    "  if (argc < 5) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    // Parse command line arguments and increase them by HALO\n",
    "    n1 = std::stoi(argv[1]) + (2 * kHalfLength);\n",
    "    n2 = std::stoi(argv[2]) + (2 * kHalfLength);\n",
    "    n3 = std::stoi(argv[3]) + (2 * kHalfLength);\n",
    "    num_iterations = std::stoi(argv[4]);\n",
    "    if (argc > 5) verify = true;\n",
    "  } catch (...) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  // Validate input sizes for the grid\n",
    "  if (ValidateInput(n1, n2, n3, num_iterations)) {\n",
    "    Usage(argv[0]);\n",
    "    return 1;\n",
    "  }\n",
    "\n",
    "  // Create queue and print target info with default selector and in order\n",
    "  // property\n",
    "  queue q(default_selector_v, {property::queue::in_order()});\n",
    "  std::cout << \" Running linear indexed GPU version\\n\";\n",
    "  printTargetInfo(q);\n",
    "\n",
    "  // Compute the total size of grid\n",
    "  size_t nsize = n1 * n2 * n3;\n",
    "\n",
    "  prev = new float[nsize];\n",
    "  next = new float[nsize];\n",
    "  vel = new float[nsize];\n",
    "\n",
    "  // Compute coefficients to be used in wavefield update\n",
    "  float coeff[kHalfLength + 1] = {-3.0548446,   +1.7777778,     -3.1111111e-1,\n",
    "                                  +7.572087e-2, -1.76767677e-2, +3.480962e-3,\n",
    "                                  -5.180005e-4, +5.074287e-5,   -2.42812e-6};\n",
    "\n",
    "  // Apply the DX, DY and DZ to coefficients\n",
    "  coeff[0] = (3.0f * coeff[0]) / (dxyz * dxyz);\n",
    "  for (auto i = 1; i <= kHalfLength; i++) {\n",
    "    coeff[i] = coeff[i] / (dxyz * dxyz);\n",
    "  }\n",
    "\n",
    "  // Initialize arrays and introduce initial conditions (source)\n",
    "  initialize(prev, next, vel, n1, n2, n3);\n",
    "\n",
    "  auto start = std::chrono::steady_clock::now();\n",
    "\n",
    "  // Invoke the driver function to perform 3D wave propagation offloaded to\n",
    "  // the device\n",
    "  iso3dfd(q, next, prev, vel, coeff, n1, n2, n3, num_iterations);\n",
    "\n",
    "  auto end = std::chrono::steady_clock::now();\n",
    "  auto time = std::chrono::duration_cast<std::chrono::milliseconds>(end - start)\n",
    "                  .count();\n",
    "  printStats(time, n1, n2, n3, num_iterations);\n",
    "\n",
    "  // Verify result with the CPU serial version\n",
    "  if (verify) {\n",
    "    VerifyResult(prev, next, vel, coeff, n1, n2, n3, num_iterations);\n",
    "  }\n",
    "\n",
    "  delete[] prev;\n",
    "  delete[] next;\n",
    "  delete[] vel;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the application is created, we can run it from the command line by using few parameters as following:\n",
    "src/3_GPU_linear 1024 1024 1024 100\n",
    "<ul>\n",
    "    <li>bin/3_GPU_linear is the binary</li>\n",
    "    <li>1024 1024 1024 are the size for the 3 dimensions, increasing it will result in more computation time</li>    \n",
    "    <li>100 is the number of time steps</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpu_linear.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_gpu_linear.sh; else ./run_gpu_linear.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISO3DFD GPU Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We started from a code version running with standard C++ on the CPU.\n",
    "* Using Intel® Offload Advisor, we determined which loop was a good candidate for offload and then using SYCL we worked on a solution to make our code run on the GPU but also on the CPU.\n",
    "* We identifed the application is bound by Integer opearations.\n",
    "* And finally we fixed the indexing in the current module to make the code more optimized.\n",
    "* The next step, is to to run the Roofline Model and VTune to\n",
    "    * Check the current optimizations to see if we fixed the application being compute and INT bound\n",
    "    * And look for oppurtunites to optimize further on the GPU to understand if we still have obvious bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running the GPU Roofline Analysis\n",
    "With the offload implemented in 3_GPU_linear using SYCL, we'll want to run roofline analysis to see the improvements we made to the application and look for more areas where there is room for performance optimization.\n",
    "```\n",
    "advisor --collect=roofline --profile-gpu --project-dir=./advi_results -- ./myApplication \n",
    "```\n",
    "The iso3DFD GPU Linear code can be run using\n",
    "```\n",
    "advisor --collect=roofline --profile-gpu --project-dir=./../advisor/3_gpu -- ./build/src/3_GPU_linear 1024 1024 1024 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpu_roofline_advisor_usm.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_gpu_roofline_advisor_usm.sh; else ./run_gpu_roofline_advisor_usm.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the roofline analysis of the 3_GPU_linear.cpp version, we can see that the performance is close to predicted. \n",
    "As noted in the below roofline model we can observe that,\n",
    "\n",
    "* The Improvements we see are :\n",
    "    * GINTOPS is 3X lower now compared to the previous version of the  GPU code without linear indexing optimizations. Similary we got more GFLOPS\n",
    "    * Lesser Data transfer time\n",
    "    * Higher bandwidth usage\n",
    "* Bottlenecks we see are:\n",
    "    * The application is now bounded by memory, specifically by the L3 bandwidth.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/gpu_linear.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Roofline Analysis report overview\n",
    "To display the report, just execute the following frame. In practice, the report will be available in the folder you defined as --out-dir in the previous script. \n",
    "\n",
    "[View the report in HTML](reports/advisor_report_linear.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(src='reports/advisor_report_linear.html', width=1024, height=768))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generating VTune reports\n",
    "Below exercises we use VTune™  analyzer as a way to see what is going on with each implementation. The information was the high-level hotspot generated from the collection and rendered in an HTML iframe. Depending on the options chosen, many of the VTune analyzer's performance collections can be rendered via HTML pages. The below vtune scripts collect GPU offload and GPU hotspots information.\n",
    "\n",
    "#### Learn more about VTune\n",
    "​\n",
    "There is extensive training on VTune, click [here](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/vtune-profiler.html#gs.2xmez3) to get deep dive training.\n",
    "\n",
    "```\n",
    "vtune -run-pass-thru=--no-altstack -collect=gpu-offload -result-dir=vtune_dir -- ./build/src/3_GPU_linear 1024 1024 1024 100\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -run-pass-thru=--no-altstack -collect=gpu-hotspots -result-dir=vtune_dir_hotspots -- ./build/src/3_GPU_linear 1024 1024 1024 100\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -report summary -result-dir vtune_dir -format html -report-output ./reports/output_offload.html\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -report summary -result-dir vtune_dir_hotspots -format html -report-output ./reports/output_hotspots.html\n",
    "```\n",
    "\n",
    "[View the report in HTML](reports/output_offload_linear.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(src='reports/output_offload_linear.html', width=1024, height=768))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[View the report in HTML](reports/output_hotspots_linear.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(src='reports/output_hotspots_linear.html', width=1024, height=768))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpu_linear_vtune.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run_gpu_linear_vtune.sh; else ./run_gpu_linear_vtune.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "### Next Iteration of implemeting GPU Optimizations\n",
    "We ran the roofline model and observed:\n",
    "\n",
    "* With the code changes that are in the 3_GPU_linear.cpp file, we can see in the roofline model that the INT operations decreased significantly \n",
    "* The kernel now has much lower arithmetic intensity and increased bandwidth\n",
    "* But now we can see the application is now bounded by memory i.e L3 bandwidth\n",
    "* In this next iteration, we'll address the problem being memory bound in kernels by increasing the L1 cache reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
