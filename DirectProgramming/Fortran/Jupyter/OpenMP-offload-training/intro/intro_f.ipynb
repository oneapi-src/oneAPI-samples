{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to oneAPI and OpenMP* Offload with Fortran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sections:\n",
    "- [oneAPI Software Model Overview](#oneAPI-Software-Model-Overview)\n",
    "- [HPC Single Node Workflow with oneAPI](#HPC-Single-Node-Workflow-with-oneAPI)\n",
    "- Code: [Simple Exercise](#Simple-Exercise)\n",
    "- [Compile and Running Fortran Programs](#Compile-and-Running-Fortran-Programs)\n",
    "- [Target Directive](#Target-Directive)\n",
    "- Code: [Simple Vector Increment with Target Directive](#Lab-Exercise:-Running-an-OpenMP-program-with-the-Target-Directive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Explain how __oneAPI__ can solve the challenges of programming in a heterogeneous world \n",
    "* Use oneAPI solutions to enable your workflows\n",
    "* Use __OpenMP Offload__ directives to execute code on the GPU\n",
    "* Familiarization on the use Jupyter notebooks for training throughout the course\n",
    "\n",
    "### Prerequisites\n",
    "This course assumes general OpenMP knowledge for CPUs. If you are new to OpenMP, below are some great resources to get you started.\n",
    "* [Basic Course on OpenMP](https://www.youtube.com/watch?v=nE-xN4Bf8XI&list=PLLX-Q6B8xqZ8n8bwjGdzBJ25X2utwnoEG)\n",
    "* [OpenMP Specification (for version 5.0)](https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5.0.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oneAPI Software Model Overview\n",
    "The oneAPI software model provides a comprehensive and unified portfolio of developer tools that can\n",
    "be used across hardware targets, including a range of performance libraries spanning several workload\n",
    "domains. The libraries include functions custom-coded for each target architecture so the same\n",
    "function call delivers optimized performance across supported architectures. oneAPI initiative is based on __industry standards and open specifications__ and is interoperable with existing HPC programming models.\n",
    "\n",
    "<img src=\"Assets/oneapi2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPC Single-Node Workflow with oneAPI \n",
    "Accelerated code can be written in either a kernel (DPC++) or __directive-based style__(OpenMP). Developers can use the __Intel® DPC++ Compatibility tool__ to perform a one-time migration from __CUDA*__ to __Data Parallel C++__. Existing __Fortran__ applications can use a __directive style based on OpenMP__. Existing __C++__ applications can choose either the __Kernel style__ or the __directive based style option__ and existing __OpenCL__ applications can remain in the OpenCL language or migrate to Data Parallel C++.\n",
    "\n",
    "__Intel® Advisor__ is recommended to  __Optimize__ the design for __vectorization and memory__ (CPU and GPU) and __Identify__ loops that are candidates for __offload__ and project the __performance on target accelerators.__\n",
    "\n",
    "The figure below shows the recommended approach of different starting points for HPC developers:\n",
    "\n",
    "<img src=\"Assets/workflow.png\">\n",
    "\n",
    "## OpenMP vs DPC++\n",
    "Both OpenMP and DPC++ are open standards that can be used to accelerate algorithms on GPUs. As the workflow diagram shows, oneAPI supports both methodologies and you should be able to achieve similar optimized performance with either option. The decision between the two choices likely depends on workflow requirements and ease of porting. When migrating from existing __CUDA__ or __OpenCL__ projects, DPC++ would likely make more sense. When migrating from existing C/Fortran applications with __OpenMP__, then OpenMP offload would be the easier alternative.\n",
    "\n",
    "## OpenMP Offload\n",
    "**OpenMP Offload** constructs are a set of directives for C++ and Fortran introduced in OpenMP 4.0 and further enhanced in later versions that allows developers to offload data and execution to target accelerators such as GPUs. OpenMP offload is supported in the Intel® oneAPI HPC Toolkit with the Intel® C++ Compiler and the Intel® Fortran Compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Simple Exercise\n",
    "This exercise introduces OpenMP offload to the developer by way of a small simple code. In addition, it introduces the developer to the Jupyter notebook environment for editing and saving code; and for running and submitting programs to the Intel® oneAPI DevCloud.\n",
    "\n",
    "We start with a simple program that includes basic OpenMP constructs including *parallel* and *do*. We will then add the *target* directive to offload part of the program to the GPU device.\n",
    "\n",
    "This simple program loops through all of the elements of data array and multiplies it by 2. \n",
    "\n",
    "###  Editing the simple.f90 code\n",
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "\n",
    "The first line of the cell contains the command **%%writefile 'simple.f90'** This tells the input cell to save the contents of the cell into the file name 'simple.f90'  As you edit the cell and run it, it will save your changes into that file.\n",
    "The code below shows the simple OpenMP code. Inspect the code, there are no modifications necessary:\n",
    "1. Inspect the code cell below and click run ▶ to save the code to file\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/simple.f90\n",
    "!==============================================================\n",
    "! Copyright © 2020 Intel Corporation\n",
    "!\n",
    "! SPDX-License-Identifier: MIT\n",
    "! =============================================================\n",
    "program main\n",
    "    use omp_lib\n",
    "    integer, parameter :: N=16\n",
    "    integer :: i, x(N)\n",
    "    logical :: is_cpu = .true.\n",
    "        \n",
    "    do i=1,N\n",
    "        x(i) = i\n",
    "    end do\n",
    "        \n",
    "     if ( .not.(omp_is_initial_device()) )  is_cpu=.false.\n",
    "    \n",
    "    !$omp parallel do\n",
    "    do i=1,N\n",
    "        x(i) = x(i) * 2\n",
    "    end do\n",
    "    !$omp end parallel do\n",
    "        \n",
    "\n",
    "    if (is_cpu) then\n",
    "        print *, \"Running on CPU\"\n",
    "    else\n",
    "        print *, \"Running on GPU\"\n",
    "    end if\n",
    "        \n",
    "    do i=1,N\n",
    "        print *, x(i)\n",
    "    end do\n",
    "end program main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Running Fortran Programs\n",
    " \n",
    "#### Compiling and Running on DevCloud:\n",
    " \n",
    "For this training purposes, we have written a script (__q__) to simplify launching tasks on the DevCloud. The __q__ script does the job of submiting a script to a GPU node on DevCloud for execution, waits for the job to complete and prints out the output/errors. We will be using this command to run programs on the DevCloud: `./q <script>.sh`\n",
    "\n",
    "#### Compiling and Running on local system:\n",
    "\n",
    "If you have installed oneAPI HPC Toolkit on your local system, you can use the commands below to compile and run a OpenMP offload program:\n",
    "```shell\n",
    "source /opt/intel/inteloneapi/setvars.sh\n",
    "\n",
    "ifx -fiopenmp -fopenmp-targets=spir64 simple.f90\n",
    "\n",
    "./simple\n",
    "  \n",
    "Note: our scripts is a combination of the above three steps.\n",
    "```\n",
    "\n",
    "Using the __ifx__ compiler with the _\"-fiopenmp -fopenmp-targets=spir64\"_ options enables OpenMP offload to the GPU.\n",
    "\n",
    "### Compile the code\n",
    "To compile the code above, we'll be using the _compile_f.sh_ script. This script sets up the compile environment and executes the Intel® Fortran Compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: Examine contents of compile_f.sh\n",
    "%pycat compile_f.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to submit the compile_c.sh script using the q script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 compile_f.sh; ./compile_f.sh;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the samples, please restart the Kernel and compile the samples again_\n",
    "### Running the code\n",
    "To execute the compiled executable, we'll be using the _run.sh_ script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: Examine contents of run.sh\n",
    "%pycat run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to submit the run.sh script using the q script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Directive\n",
    "The `omp target` construct transfers control and data from the host to the device. The transfer of control is sequential and synchronous. In a multi-device environment, the _device_ clause can be optionally used to denote a specific device. Each device is assigned an implementation-specific integer number. Map clauses can be used to control the direction of data flow. Map clauses will be discussed in detail in the next module.\n",
    "\n",
    "Example:\n",
    "```fortran\n",
    "! Sequential Host Code\n",
    "...\n",
    "\n",
    "!$omp target\n",
    "! Target Region Executed on the Device\n",
    "   do i=1,N\n",
    "       ...\n",
    "   end do\n",
    "!$omp end target\n",
    "\n",
    "! More Sequential Host Code\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise: Running an OpenMP program with the Target Directive\n",
    "In the example below, add the `!$omp target map(tofrom:is_cpu)` directive where stated to offload execution to the GPU.  We use the map clause here to copy the value of is_cpu back to the host to see if our code actually executed on the GPU. Ensure to also include the `!$omp end target` statement. The *map* clause will be discussed in detail in the next module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/simple.f90\n",
    "!==============================================================\n",
    "! Copyright © 2020 Intel Corporation\n",
    "!\n",
    "! SPDX-License-Identifier: MIT\n",
    "! =============================================================\n",
    "program main\n",
    "    use omp_lib\n",
    "    integer, parameter :: N=16\n",
    "    integer :: i, x(N)\n",
    "    logical :: is_cpu = .true.\n",
    "        \n",
    "    do i=1,N\n",
    "        x(i) = i\n",
    "    end do\n",
    "       \n",
    "    !TODO Place the target directive here including the map(tofrom:is_cpu) clause\n",
    "    \n",
    "    !$omp parallel do\n",
    "    do i=1,N\n",
    "        if ((i==1) .and. (.not.(omp_is_initial_device()))) is_cpu=.false.\n",
    "        x(i) = x(i) * 2\n",
    "    end do\n",
    "    \n",
    "    !TODO Place the end target directive here\n",
    "\n",
    "    if (is_cpu) then\n",
    "        print *, \"Running on CPU\"\n",
    "    else\n",
    "        print *, \"Running on GPU\"\n",
    "    end if\n",
    "        \n",
    "    do i=1,N\n",
    "        print *, x(i)\n",
    "    end do\n",
    "end program main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to compile the code\n",
    "! chmod 755 compile_f.sh; ./compile_f.sh;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to run the code\n",
    "! chmod 755 q; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again_\n",
    "\n",
    "Once execution completes, you should see the message that the the program ran on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the solution by running this cell\n",
    "%pycat simple_solution.f90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this module you have learned the following:\n",
    "* How oneAPI solves the challenges of programming in a heterogeneous world \n",
    "* Take advantage of oneAPI solutions to enable your workflows\n",
    "* Use the Intel DevCloud to test drive oneAPI tools and libraries\n",
    "* Introduced to the target directive to enable OpenMP offload\n",
    "* Become familiarized with the use of Juypter notebooks by editing of source code in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Check out these related resources\n",
    "\n",
    "#### Intel® oneAPI\n",
    "* [oneAPI main page](https://software.intel.com/oneapi \"oneAPI main page\")\n",
    "* [Intel® DevCloud](https://software.intel.com/en-us/devcloud/oneapi \"Intel DevCloud\")\n",
    "* [Get Started with oneAPI for Linux*](https://software.intel.com/en-us/get-started-with-intel-oneapi-linux)\n",
    "* [Get Started with oneAPI for Windows*](https://software.intel.com/en-us/get-started-with-intel-oneapi-windows)\n",
    "* [oneAPI Release Notes](https://software.intel.com/en-us/articles/intel-oneapi-release-notes)\n",
    "* [oneAPI Sample Codes](https://software.intel.com/en-us/articles/code-samples-for-intel-oneapibeta-toolkits)\n",
    "\n",
    "#### OpenMP\n",
    "* [OpenMP Specification (for version 5.0)](https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5.0.pdf)\n",
    "***\n",
    "\n",
    "@Intel Corporation | [\\*Trademark](https://www.intel.com/content/www/us/en/legal/trademarks.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
