{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMP* Device Parallelism (Fortran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sections\n",
    "- [Learning Objectives](#Learning-Objectives)\n",
    "- [Device Parallelism](#Device-Parallelism)\n",
    "- [GPU Architecture](#GPU-Architecture)\n",
    "- [\"Normal\" OpenMP constructs](#\"Normal\"-OpenMP-constructs)\n",
    "- [League of Teams](#League-of-Teams)\n",
    "- [Worksharing with Teams](#Worksharing-with-Teams)\n",
    "- _Code:_ [Lab Exercise: OpenMP Device Parallelism](#Lab-Exercise:-OpenMP-Device-Parallelism)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Explain basic GPU Architecture \n",
    "* Be able to use OpenMP offload worksharing constructs to fully utilize the GPU\n",
    "\n",
    "### Prerequisites\n",
    "Basic understanding of OpenMP constructs are assumed for this module. You also should have already went through the  [Introduction to OpenMP Offload module](../intro/intro_f.ipynb) and [Managing Device Data module](../datatransfer/datatransfer_f.ipynb), where the basics of using the Jupyter notebooks with the Intel® DevCloud and an introduction to the OpenMP `target` and `target data` constructs were discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Device Parallelism\n",
    "As we've discussed in the previous modules, the OpenMP `target` construct transfers the control flow to the target device. However, the transfer of control is sequential and synchronous.\n",
    "\n",
    "In OpenMP, offload and parallelism are separate, so programmers need to explicitly create parallel regions on the target device. In theory, constructs that create parallelism on offload devices can be combined with any OpenMP construct, but in practice, only a subset of OpenMP constructs are useful for the target device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Architecture\n",
    "Before diving into OpenMP parallelism constructs for target divices, let's first examine Intel® GPU architecture.\n",
    "\n",
    "<img src=\"Assets/GPU_Arch.png\">\n",
    "\n",
    "Intel® GPUs contain 1 or more slices. Each slice is composed of several Subslices. Each Subslice contain multiple EUs (likely 8 or more), has it's own thread dispatcher unit, instruction cache, share local memory, and other resources. EUs are compute processors that drive the SIMD ALUs.\n",
    "\n",
    "The following table displays how the OpenMP concepts of League, Team, Thread, and SIMD are mapped to GPU hardware.\n",
    "\n",
    "|OpenMP | GPU Hardware |\n",
    "|:----:|:----|\n",
    "|SIMD | SIMD Lane (Channel)|\n",
    "|Thread | SIMD Thread mapped to an EU |\n",
    "|Team | Group of threads mapped to a Subslice |\n",
    "|League | Multiple Teams mapped to a GPU |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Normal\" OpenMP constructs\n",
    "OpenMP GPU offload support all \"normal\" OpenMP constructs such as `parallel`, `do`, `barrier`, `sections`, `tasks`, etc. However, not every construct will be useful for the GPU. When using these constructs, the full threading model is only supported with in a subslice, this is because there's no synchronization among subslices, and there's no coherence and memory fence among subslices' L1 caches.\n",
    "\n",
    "Let's examine the following example.\n",
    "```fortran\n",
    "subroutine saxpy(a, x, y, sz)\n",
    "    ! Declarations Omitted\n",
    "    !$omp target map(to:x(1:sz)) map(tofrom(y(1:sz))\n",
    "    !$omp parallel do simd\n",
    "    do i=1,sz\n",
    "        y(i) = a * x(i) + y(i);\n",
    "    end do\n",
    "    !$omp end target\n",
    "end subroutine\n",
    "```\n",
    "Here, we use the `target` pragma to offload the execution to the GPU. We then use `parallel` to create a team of threads, `do` to distribute loop iterations to those threads, and `simd` to request iteration vectorization with SIMD instructions. However, due to the restrictions aforementioned, only one GPU subslice is utilized here, so the GPU would be significantly underutilized. In some cases, the compiler may deduce `team distribute` from `parallel for` and still use the entire GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## League of Teams\n",
    "To take advantage of multiple subslices, use the `teams` pragma to create multiple **master** threads for execution. When combined with the `parallel` pragma, these master threads become a league of thread teams. Becuase there's no synchronization across teams of threads, the teams could then be assigned to different GPU subslices.\n",
    "\n",
    "<img src=\"Assets/teams.JPG\">\n",
    "\n",
    "When using the `teams` construct, the number of teams created is implementation defined. Although, you may optionally specify an upper limit with the **num_teams** clause. The **thread_limit** clause of the `teams` pragma can be optionally used to limit the number of threads in each team.\n",
    "\n",
    "Example: `!$omp teams num_teams(8) thread_limit(16)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Worksharing with Teams\n",
    "After a league of teams is created by `teams`, use the `distribute` construct to distribute chunks of iterations of a loop across the different teams in the league. This is analogous to what the `do` construct does for `parallel` regions. The `distribute` pragma is associated with a loop nest inside a teams region.\n",
    "\n",
    "For nested loops, the **collapse** clause can be used to specify how many loops are associated with the `distribute` pragma. You may specify a **collapse** clause with a parameter value greater than 1 to collapse associated loops into one large loop.\n",
    "\n",
    "You can also use **dist_schedule** clause on the `distribute` construct to manually specify the chunk size that are distributed to master threads of each team. For example, `!$omp distribute dist_schedule(static, 512)` would create chunks of 512 iterations.\n",
    "\n",
    "### Example with Combined Constructs\n",
    "For convenience, OpenMP supports combined constructs for OpenMP offload. The code below shows how a single line can encompass all of the directives that we've discussed.\n",
    "```fortran\n",
    "subroutine saxpy(a, x, y, sz)\n",
    "    ! Declarations Omitted\n",
    "    !$omp target teams distribute parallel do simd map(to:x(1:sz)) map(tofrom(y(1:sz))\n",
    "    do i=1,sz\n",
    "        y(i) = a * x(i) + y(i);\n",
    "    end do\n",
    "    !$omp end target teams distribute parallel do simd\n",
    "end subroutine\n",
    "```\n",
    "When these constructs are used without additional clauses, the number of teams created, the number of threads created per team, and how loop iterations are distributed are all implementation defined.\n",
    "The following diagram breaks down the effects of each pragma in the previous example. Here we assume that there are a total of 128 loop iterations and that 4 teams, and 4 threads per team are created by the implementation.\n",
    "\n",
    "1. The `omp target` pragma offloads the execution to device\n",
    "2. The `omp teams` pragma creates multiple master threads, 4 thread teams in this diagram.\n",
    "3. The `omp distribute` pragma distributes loop iterations to those 4 thread teams, 32 threads for each team shown.\n",
    "4. The `omp parallel` pragma creates a team of threads for each master thread (team), 4 threads created for each team shown.\n",
    "5. The `omp do` pragma distributes the 32 iterations to each of the 4 threads.\n",
    "6. The `omp simd` pragma specifies that multiple iterations of the loop can be executed using SIMD instructions.\n",
    "\n",
    "<img src=\"Assets/distribute.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise: OpenMP Device Parallelism\n",
    "In this exercise, we will practice using the offload worksharing constructs on the saxpy function that we've already worked with in the previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional, see the contents of main.cpp\n",
    "%pycat main.f90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, add an OpenMP directive before the outer loop to perform the following tasks.\n",
    "1. Offload execution to the GPU, use the clause `map(tofrom:y) map(to:x) map(from:is_cpu, num_teams)`\n",
    "2. Create NUM_BLOCKS of **master** threads, use the clause `num_teams(NUM_BLOCKS)`\n",
    "3. Distribute the outer loop iterations to the varoius master threads.\n",
    "\n",
    "Ensure to also include the appropriate end directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/saxpy_func_parallel.f90\n",
    "! Add combined directive here to\n",
    "!    1. Offload execution to the GPU, use the cause map(tofrom:y)\n",
    "!       map(to: x) map(from:is_cpu) map(from:num_teams)\n",
    "!    2. Create multiple master threads use clause num_teams(NUM_BLOCKS)\n",
    "!    3. Distribute loop iterations to the various master threads.\n",
    "\n",
    "do ib=1,ARRAY_SIZE, NUM_BLOCKS\n",
    "        if (ib==1) then\n",
    "                !Test if target is the CPU host or the GPU device\n",
    "                is_cpu=omp_is_initial_device()\n",
    "                !Query number of teams created\n",
    "                num_teams=omp_get_num_teams()\n",
    "        end if\n",
    "\n",
    "        do i=ib, ib+NUM_BLOCKS-1\n",
    "                y(i) = a*x(i) + y(i)\n",
    "        end do\n",
    "end do\n",
    "\n",
    "!TODO add the appropriate end directive here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compile code using *compile_f.sh*. If you would like to see the contents of compile_f.sh execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat compile_f.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this cell to compile the code\n",
    "! chmod 755 compile_f.sh; ./compile_f.sh;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code has been successfully compiled, run the code by executing the _run.sh_ script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optionally examine the run script by executing this cell.\n",
    "%pycat run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to execute the program. Make sure you see the \"Passed!\" message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the samples, please restart the Kernel and compile the samples again_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat saxpy_func_parallel_solution.f90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this module, you have learned the following:\n",
    "* High-level overview of GPU architecture and how OpenMP constructs map to it.\n",
    "* Create multiple master threads that can be assigned to GPU subslices using the `teams` construct.\n",
    "* Distribute loop iterations to those master threads using the `distribute` construct.\n",
    "* Use the `teams` and `distribute` constructs combined with other OpenMP constructs for better performance.\n",
    "\n",
    "***\n",
    "\n",
    "@Intel Corporation | [\\*Trademark](https://www.intel.com/content/www/us/en/legal/trademarks.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
