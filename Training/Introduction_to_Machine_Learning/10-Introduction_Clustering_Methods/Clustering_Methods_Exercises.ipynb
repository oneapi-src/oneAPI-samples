{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Methods Exercises\n",
    "\n",
    "![Clustering.png](Assets/Clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Discuss unsupervised learning algorithms and how they can be applied \n",
    "- Apply clustering \n",
    "- Apply dimensionality reduction \n",
    "- Apply Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware\n",
    "\n",
    "# scikit-learn* \n",
    "\n",
    "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
    "\n",
    "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
    "\n",
    "These exercises will demonstrate how to apply **the Intel® Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel® oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
    "\n",
    "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code PRIOR to importing sklearn: \n",
    "\n",
    "- **from sklearnex import patch_sklearn**\n",
    "- **patch_sklearn()**\n",
    "\n",
    "## This exercise relies on installation of  Intel® Extension for Scikit-learn*\n",
    "\n",
    "If you have not already done so, follow the instructions from Week 1 for instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is `Wine_Quality_Data`.\n",
    "\n",
    "We will be using the chemical properties (i.e. everything but quality and color) to cluster the wine. Though this is unsupervised learning, it can be fun to see how our clustering results map onto color and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:26:42.720121Z",
     "start_time": "2021-09-24T17:26:41.236502Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['../data']\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data and examine the features. \n",
    "* Note which are continuous, categorical, and boolean. \n",
    "* How many entries are there for the two colors and range of qualities? \n",
    "* Make a histogram plot of the quality for each of the wine colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:53.607400Z",
     "start_time": "2021-09-17T10:04:53.532400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the data\n",
    "filepath = os.sep.join(data_path + ['Wine_Quality_Data.csv'])\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "data.head(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:53.623394Z",
     "start_time": "2021-09-17T10:04:53.610396Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data types for each entry. The implementation of K-means in Scikit-learn is designed only to work with continuous data (even though it is sometimes used with categorical or boolean types). Fortunately, all the columns we will be using (everything except quality and color) are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:53.638398Z",
     "start_time": "2021-09-17T10:04:53.627396Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of entries for each wine color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:53.654400Z",
     "start_time": "2021-09-17T10:04:53.641398Z"
    }
   },
   "outputs": [],
   "source": [
    "data.color.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of quality values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:53.687402Z",
     "start_time": "2021-09-17T10:04:53.659400Z"
    }
   },
   "outputs": [],
   "source": [
    "data.quality.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:54.271759Z",
     "start_time": "2021-09-17T10:04:53.692402Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:54.712865Z",
     "start_time": "2021-09-17T10:04:54.276759Z"
    }
   },
   "outputs": [],
   "source": [
    "# seaborn styles\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "# custom colors\n",
    "red = sns.color_palette()[2]\n",
    "white = 'gray'\n",
    "\n",
    "# set bins for histogram\n",
    "bin_range = np.array([3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "# plot histogram of quality counts for red and white wines\n",
    "ax = plt.axes()\n",
    "for color, plot_color in zip(['red', 'white'], [red, white]):\n",
    "    q_data = data.loc[data.color==color, 'quality']\n",
    "    q_data.hist(bins=bin_range, \n",
    "                alpha=0.5, ax=ax, \n",
    "                color=plot_color, label=color)\n",
    "    \n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel='Quality', ylabel='Occurrence')\n",
    "\n",
    "# force tick labels to be in middle of region\n",
    "ax.set_xlim(3,10)\n",
    "ax.set_xticks(bin_range+0.5)\n",
    "ax.set_xticklabels(bin_range);\n",
    "ax.grid('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Example the correlation and skew of the relevant variables--everything except color and quality.\n",
    "* Perform any appropriate feature transformations and/or scaling.\n",
    "* Examine the pairwise distribution of the variables with pairplots to verify scaling and normalization efforts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:54.775487Z",
     "start_time": "2021-09-17T10:04:54.716400Z"
    }
   },
   "outputs": [],
   "source": [
    "float_columns = [x for x in data.columns if x not in ['color', 'quality']]\n",
    "\n",
    "# The correlation matrix\n",
    "corr_mat = data[float_columns].corr()\n",
    "\n",
    "# Strip out the diagonal values for the next step\n",
    "for x in range(len(float_columns)):\n",
    "    corr_mat.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:54.807503Z",
     "start_time": "2021-09-17T10:04:54.779491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pairwise maximal correlations\n",
    "corr_mat.abs().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an examination of the skew values in anticipation of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:54.966491Z",
     "start_time": "2021-09-17T10:04:54.812494Z"
    }
   },
   "outputs": [],
   "source": [
    "skew_columns = (data[float_columns]\n",
    "                .skew()\n",
    "                .sort_values(ascending=False))\n",
    "\n",
    "skew_columns = skew_columns.loc[skew_columns > 0.75]\n",
    "skew_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:54.999504Z",
     "start_time": "2021-09-17T10:04:54.972496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform log transform on skewed columns\n",
    "for col in skew_columns.index.tolist():\n",
    "    data[col] = np.log1p(data[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:55.660716Z",
     "start_time": "2021-09-17T10:04:55.007491Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "data[float_columns] = sc.fit_transform(data[float_columns])\n",
    "\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:04:55.771768Z",
     "start_time": "2021-09-17T10:04:55.664722Z"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the pairplot of the transformed and scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:38.935090Z",
     "start_time": "2021-09-17T10:04:55.774757Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = data[float_columns + ['color']]\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.pairplot(temp, \n",
    "             hue='color', \n",
    "             hue_order=['white', 'red'],\n",
    "             palette={'red':'red', 'white':'gray'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Fit a K-means clustering model with two clusters.\n",
    "* Examine the clusters by wine color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:39.046256Z",
     "start_time": "2021-09-17T10:06:38.935090Z"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=42)\n",
    "km = km.fit(data[float_columns])\n",
    "\n",
    "data['kmeans'] = km.predict(data[float_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:39.078261Z",
     "start_time": "2021-09-17T10:06:39.050257Z"
    }
   },
   "outputs": [],
   "source": [
    "(data[['color','kmeans']]\n",
    " .groupby(['color','kmeans'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Now fit K-Means models with cluster values ranging from 2 to 20.\n",
    "* For each model, store the number of clusters and the inertia value. \n",
    "* Plot cluster number vs inertia. Does there appear to be an ideal cluster number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:40.056736Z",
     "start_time": "2021-09-17T10:06:39.083258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit a range of models\n",
    "km_list = list()\n",
    "\n",
    "for clust in range(2,5):\n",
    "    km = KMeans(n_clusters=clust, random_state=42)\n",
    "    km = km.fit(data[float_columns])\n",
    "    \n",
    "    km_list.append(pd.Series({'clusters': clust, \n",
    "                              'inertia': km.inertia_,\n",
    "                              'model': km}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:40.420123Z",
     "start_time": "2021-09-17T10:06:40.056736Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_data = (pd.concat(km_list, axis=1)\n",
    "             .T\n",
    "             [['clusters','inertia']]\n",
    "             .set_index('clusters'))\n",
    "\n",
    "ax = plot_data.plot(marker='o',ls='-')\n",
    "ax.set_xticks(range(0,21,2))\n",
    "ax.set_xlim(0,21)\n",
    "ax.set(xlabel='Cluster', ylabel='Inertia');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Fit an agglomerative clustering model with two clusters.\n",
    "* Compare the results to those obtained by K-means with regards to wine color.\n",
    "* Visualize the dendrogram produced by agglomerative clustering. *Hint:* SciPy has a module called [`cluster.hierarchy`](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy) that contains the `linkage` and `dendrogram` functions required to create the linkage map and plot the resulting dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:43.380493Z",
     "start_time": "2021-09-17T10:06:40.423134Z"
    }
   },
   "outputs": [],
   "source": [
    "ag = AgglomerativeClustering(n_clusters=2, linkage='ward', compute_full_tree=True)\n",
    "ag = ag.fit(data[float_columns])\n",
    "data['agglom'] = ag.fit_predict(data[float_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cluster assignment is arbitrary, the respective primary cluster numbers for red and white may not be identical to the ones below and also may not be the same for both K-means and agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:43.410715Z",
     "start_time": "2021-09-17T10:06:43.382552Z"
    }
   },
   "outputs": [],
   "source": [
    "(data[['color','agglom','kmeans']]\n",
    " .groupby(['color','agglom','kmeans'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the cluster numbers are not necessarily identical, the clusters are very consistent within a single wine variety (red or white).\n",
    "\n",
    "And here is a plot of the dendrogram created from agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:44.550074Z",
     "start_time": "2021-09-17T10:06:43.411716Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from matplotlib import colors\n",
    "\n",
    "Z = hierarchy.linkage(ag.children_, method='ward')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "# Some color customization\n",
    "dark_palette = sns.color_palette()\n",
    "red = colors.to_hex(dark_palette[2])\n",
    "blue = colors.to_hex(dark_palette[0])\n",
    "\n",
    "hierarchy.set_link_color_palette([red, 'gray'])\n",
    "\n",
    "den = hierarchy.dendrogram(Z, orientation='top', \n",
    "                           p=30, truncate_mode='lastp',\n",
    "                           show_leaf_counts=True, ax=ax,\n",
    "                           above_threshold_color=blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "In this question, we are going to explore clustering as a form of feature engineering.\n",
    "\n",
    "* Create a **binary** target variable `y`, denoting if the quality is greater than 7 or not.\n",
    "* Create a variable called `X_with_kmeans` from `data`, by dropping the columns \"quality\", \"color\" and \"agglom\" from the dataset. Create `X_without_kmeans` from that by dropping \"kmeans\".\n",
    "* For both datasets, using `StratifiedShuffleSplit` with 10 splits, fit 10 Random Forest Classifiers and average out the roc-auc scores.\n",
    "* Compare the average roc-auc scores for the models using the kmeans clusters as a feature and the one that doesn't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:06:55.809956Z",
     "start_time": "2021-09-17T10:06:44.552898Z"
    }
   },
   "outputs": [],
   "source": [
    "y = (data['quality'] > 7).astype(int)\n",
    "X_with_kmeans = data.drop(['agglom', 'color', 'quality'], axis=1)\n",
    "X_without_kmeans = X_with_kmeans.drop('kmeans', axis=1)\n",
    "sss = StratifiedShuffleSplit(n_splits=10, random_state=6532)\n",
    "\n",
    "\n",
    "def get_avg_roc_10splits(estimator, X, y):\n",
    "    roc_auc_list = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        estimator.fit(X_train, y_train)\n",
    "        y_predicted = estimator.predict(X_test)\n",
    "        y_scored = estimator.predict_proba(X_test)[:, 1]\n",
    "        roc_auc_list.append(roc_auc_score(y_test, y_scored))\n",
    "    return np.mean(roc_auc_list)\n",
    "# return classification_report(y_test, y_predicted)\n",
    "\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "roc_with_kmeans = get_avg_roc_10splits(estimator, X_with_kmeans, y)\n",
    "roc_without_kmeans = get_avg_roc_10splits(estimator, X_without_kmeans, y)\n",
    "print(\"Without kmeans cluster as input to Random Forest, roc-auc is \\\"{0}\\\"\".format(roc_without_kmeans))\n",
    "print(\"Using kmeans cluster as input to Random Forest, roc-auc is \\\"{0}\\\"\".format(roc_with_kmeans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore if the number of clusters have an effect in this improvement.\n",
    "\n",
    "* Create the basis training set from `data` by restricting to float_columns.\n",
    "* For $n = 1, \\ldots, 20$, fit a kmeans algotihm with n clusters. One hot encode it and add it to the **basis** training set. Don't add it to the previous iteration.\n",
    "* Fit 10 **Logistic Regression** models and compute the average roc-auc-score.\n",
    "* Plot the average roc-auc scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T10:07:13.483587Z",
     "start_time": "2021-09-17T10:06:55.812905Z"
    }
   },
   "outputs": [],
   "source": [
    "X_basis = data[float_columns]\n",
    "sss = StratifiedShuffleSplit(n_splits=10, random_state=6532)\n",
    "\n",
    "\n",
    "def create_kmeans_columns(n):\n",
    "    km = KMeans(n_clusters=n)\n",
    "    km.fit(X_basis)\n",
    "    km_col = pd.Series(km.predict(X_basis))\n",
    "    km_cols = pd.get_dummies(km_col, prefix='kmeans_cluster')\n",
    "    return pd.concat([X_basis, km_cols], axis=1)\n",
    "\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "ns = range(2, 21)\n",
    "roc_auc_list = [get_avg_roc_10splits(estimator, create_kmeans_columns(n), y)\n",
    "                for n in ns]\n",
    "\n",
    "\n",
    "# seaborn styles\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(ns, roc_auc_list)\n",
    "ax.set(\n",
    "    xticklabels= ns,\n",
    "    xlabel='number of clusters as features',\n",
    "    ylabel='average roc-auc over 10 iterations',\n",
    "    title='KMeans + LogisticRegression'\n",
    ")\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Let's now explore on DBSCAN clustering method and if the number of samples have an effect in this improvement.\n",
    "\n",
    "* Create the basis training set from `data` by restricting to float_columns.\n",
    "* For $n = 1, \\ldots, 20$, fit a kmeans algotihm with n clusters. One hot encode it and add it to the **basis** training set. Don't add it to the previous iteration.\n",
    "* Fit 10 **Logistic Regression** models and compute the average roc-auc-score.\n",
    "* Plot the average roc-auc scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T09:48:22.407Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Let's now explore on Birch clustering method and if the number of clusters have an effect in this improvement.\n",
    "\n",
    "* Create the basis training set from `data` by restricting to float_columns.\n",
    "* For $n = 1, \\ldots, 20$, fit a kmeans algotihm with n clusters. One hot encode it and add it to the **basis** training set. Don't add it to the previous iteration.\n",
    "* Fit 10 **Logistic Regression** models and compute the average roc-auc-score.\n",
    "* Plot the average roc-auc scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T09:48:22.410Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2022.3)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
