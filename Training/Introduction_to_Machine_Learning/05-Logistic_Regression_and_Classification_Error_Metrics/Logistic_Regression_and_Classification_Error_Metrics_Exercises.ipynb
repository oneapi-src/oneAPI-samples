{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Logistic Regression and Classification Error Metrics\n",
    "![LogRegr.png](Assets/LogRegr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Describe Logistic regression and how it differs from linear regression\n",
    "- Identify metrics for classification errors and scenarios in which they can be used\n",
    "- Apply Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware\n",
    "\n",
    "\n",
    "# scikit-learn* \n",
    "\n",
    "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
    "\n",
    "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
    "\n",
    "These exercises will demonstrate how to apply **the Intel® Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel® oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
    "\n",
    "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code PRIOR to importing sklearn: \n",
    "\n",
    "- **from sklearnex import patch_sklearn**\n",
    "- **patch_sklearn()**\n",
    "\n",
    "## This exercise relies on installation of  Intel® Extension for Scikit-learn*\n",
    "\n",
    "If you have not already done so, follow the instructions from Week 1 for instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n",
    "\n",
    "Alternatively the same data set can be found at https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones/downloads/human-activity-recognition-with-smartphones.zip\n",
    "The train file can be renamed as Human_Activity_Recognition_Using_Smartphones_Data.csv \n",
    "\n",
    "For each record in the dataset it is provided: \n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. \n",
    "- Triaxial Angular velocity from the gyroscope. \n",
    "- A 561-feature vector with time and frequency domain variables. \n",
    "- Its activity label. \n",
    "\n",
    "More information about the features is available on the website above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:46.801355Z",
     "start_time": "2021-09-17T02:19:46.797352Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['../data']\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Import the data and do the following:\n",
    "\n",
    "* Examine the data types--there are many columns, so it might be wise to use value counts\n",
    "* Determine if the floating point values need to be scaled\n",
    "* Determine the breakdown of each activity\n",
    "* Encode the activity label as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:48.669458Z",
     "start_time": "2021-09-17T02:19:46.817352Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#The filepath is dependent on the data_path set in the previous cell \n",
    "filepath = os.sep.join(data_path + ['Human_Activity_Recognition_Using_Smartphones_Data.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data columns are all floats except for the activity label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:48.701458Z",
     "start_time": "2021-09-17T02:19:48.672460Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:48.717461Z",
     "start_time": "2021-09-17T02:19:48.705464Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data are all scaled from -1 (minimum) to 1.0 (maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:48.813458Z",
     "start_time": "2021-09-17T02:19:48.720459Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.iloc[:, :-1].min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:48.908458Z",
     "start_time": "2021-09-17T02:19:48.816459Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data.iloc[:, :-1].max().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the breakdown of activities--they are relatively balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:48.924475Z",
     "start_time": "2021-09-17T02:19:48.911463Z"
    }
   },
   "outputs": [],
   "source": [
    "data.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n",
    "Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:19:49.796294Z",
     "start_time": "2021-09-17T02:19:48.928460Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Activity'] = le.fit_transform(data.Activity)\n",
    "data['Activity'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Calculate the correlations between the dependent variables.\n",
    "* Create a histogram of the correlation values\n",
    "* Identify those that are most correlated (either positively or negatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:12.085552Z",
     "start_time": "2021-09-17T02:19:49.801326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation values\n",
    "feature_cols = data.columns[:-1]\n",
    "corr_values = data[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.NaN\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values.stack().to_frame().reset_index().rename(columns={'level_0':'feature1','level_1':'feature2',0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of the absolute value correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:12.511272Z",
     "start_time": "2021-09-17T02:20:12.087622Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:13.001294Z",
     "start_time": "2021-09-17T02:20:12.514240Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50)\n",
    "\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:13.096326Z",
     "start_time": "2021-09-17T02:20:13.004294Z"
    }
   },
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n",
    "* Regardless of methods used to split the data, compare the ratio of classes in both the train and test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:13.255496Z",
     "start_time": "2021-09-17T02:20:13.099327Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1,test_size=0.3, random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'Activity']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:13.271495Z",
     "start_time": "2021-09-17T02:20:13.257495Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:20:13.287496Z",
     "start_time": "2021-09-17T02:20:13.274495Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n",
    "* Using cross validation to determine the hyperparameters, fit models using L1, and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:04:21.034422Z",
     "start_time": "2021-09-17T03:04:18.664864Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard logistic regression\n",
    "lr = LogisticRegression(C=.001, max_iter=295).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:43.281330Z",
     "start_time": "2021-09-17T03:04:37.543307Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L1 regularized logistic regression\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T02:29:53.904879Z",
     "start_time": "2021-09-17T02:29:53.890832Z"
    }
   },
   "outputs": [],
   "source": [
    "#Try with different solvers like ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’ and give your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:47.053827Z",
     "start_time": "2021-09-17T03:12:43.283329Z"
    }
   },
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=1, cv=4, penalty='l2').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:47.181913Z",
     "start_time": "2021-09-17T03:12:47.120912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine all the coefficients into a dataframe\n",
    "coefficients = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    coeffs = mod.coef_\n",
    "    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n",
    "                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n",
    "    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n",
    "\n",
    "coefficients = pd.concat(coefficients, axis=1)\n",
    "\n",
    "coefficients.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:48.946569Z",
     "start_time": "2021-09-17T03:12:47.184915Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axList = plt.subplots(nrows=3, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "\n",
    "for ax in enumerate(axList):\n",
    "    loc = ax[0]\n",
    "    ax = ax[1]\n",
    "    \n",
    "    data = coefficients.xs(loc, level=1, axis=1)\n",
    "    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n",
    "    \n",
    "    if ax is axList[0]:\n",
    "        ax.legend(loc=4)\n",
    "        \n",
    "    ax.set(title='Coefficient Set '+str(loc))\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Also store the probability for the predicted class for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:49.026658Z",
     "start_time": "2021-09-17T03:12:48.948608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the class and the probability for each\n",
    "\n",
    "y_pred = list()\n",
    "y_prob = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n",
    "    \n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "y_prob = pd.concat(y_prob, axis=1)\n",
    "\n",
    "y_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:49.042661Z",
     "start_time": "2021-09-17T03:12:49.028663Z"
    }
   },
   "outputs": [],
   "source": [
    "y_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* fscore\n",
    "* confusion matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:49.106660Z",
     "start_time": "2021-09-17T03:12:49.046661Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Preciision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n",
    "    \n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "    \n",
    "    # ROC-AUC scores can be calculated by binarizing the data\n",
    "    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n",
    "              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n",
    "              average='weighted')\n",
    "    \n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "    \n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                              'fscore':fscore, 'accuracy':accuracy,\n",
    "                              'auc':auc}, \n",
    "                             name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:49.122663Z",
     "start_time": "2021-09-17T03:12:49.108662Z"
    }
   },
   "outputs": [],
   "source": [
    "#Run the metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Display or plot the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.030396Z",
     "start_time": "2021-09-17T03:12:49.124662Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axList = plt.subplots(nrows=2, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "axList[-1].axis('off')\n",
    "\n",
    "for ax,lab in zip(axList[:-1], coeff_labels):\n",
    "    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n",
    "    ax.set(title=lab);\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    " Identify highly correlated columns and drop those columns before building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.188590Z",
     "start_time": "2021-09-17T03:12:51.032396Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#threshold with .7\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.7 * (1 - .7)))\n",
    "\n",
    "data2 = pd.concat([X_train,X_test])\n",
    "data_new = pd.DataFrame(sel.fit_transform(data2))\n",
    "\n",
    "\n",
    "data_y = pd.concat([y_train,y_test])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_new,X_test_new = train_test_split(data_new)\n",
    "Y_new,Y_test_new = train_test_split(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Repeat Model building with new training data after removing higly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.204590Z",
     "start_time": "2021-09-17T03:12:51.190623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try standard, L1 and L2 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.220590Z",
     "start_time": "2021-09-17T03:12:51.206594Z"
    }
   },
   "outputs": [],
   "source": [
    "#Try with different solvers like ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’ and give your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.236633Z",
     "start_time": "2021-09-17T03:12:51.222595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine all the coefficients into a dataframe for comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.252591Z",
     "start_time": "2021-09-17T03:12:51.238596Z"
    }
   },
   "outputs": [],
   "source": [
    "# try the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Also store the probability for the predicted class for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.268592Z",
     "start_time": "2021-09-17T03:12:51.254594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the class and the probability for each\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* fscore\n",
    "* confusion matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.284594Z",
     "start_time": "2021-09-17T03:12:51.270594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the error metrics as listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.300595Z",
     "start_time": "2021-09-17T03:12:51.286595Z"
    }
   },
   "outputs": [],
   "source": [
    "#Run the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "Display or plot the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.315594Z",
     "start_time": "2021-09-17T03:12:51.302595Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.331594Z",
     "start_time": "2021-09-17T03:12:51.320595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a comparison of the outputs between Question 7 and 12 and give your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T03:12:51.346595Z",
     "start_time": "2021-09-17T03:12:51.333594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a comparison of the outputs between Question 8 and 13 and give your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2022.3)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
