{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The concepts build on top of each other introducing and reinforcing the concepts of machine learning. This course covers commonly used AI algorithms implemented in Scikit-learn. It provides the conceptual framework for how and why these algorithms can be used to solve machine learning problems. It also applies acceleration via the Intel Extensions for Scikit-learn. The accelerations is accomplished with two lines of code and achieves acceleration of one to three orders of magnitude speed-ups over the stock scikit-learn implementations on Intel CPU's\n",
    "\n",
    "| Modules | Description |\n",
    "|:--- |:------ |\n",
    "|[01_Introduction_to_Machine_Learning_and_Tools](01_Introduction_to_Machine_Learning_and_Tools/Introduction_to_Machine_Learning_and_Toolkit.ipynb)| Demonstrate key concepts like under- and over-fitting, supervised learning algorithms, regularization, and cross-validation. Explain how to describe the type of problem to be solved in terms of classification, regression, unsupervised or supervised learing. Choose an algorithm, tune parameters, and validate a model. Accelerate key Scikit-learn* algorithms to leverage underlying compute capabilities of hardware.|\n",
    "|[02-Introduction_to_Supervised_Learning_and_K_Nearest_Neighbors](02-Introduction_to_Supervised_Learning_and_K_Nearest_Neighbors/Supervised_Learning_and_K_Nearest_Neighbors_Exercises.ipynb)|Explain supervised learning and how it can be applied to regression and classification problems. Apply K-Nearest Neighbor (KNN) algorithm for classification, then accelerate it using patching with Intel® Extension for Scikit-learn*|\n",
    "|[03-Train_Test_Splits_Validation_Linear_Regression](03-Train_Test_Splits_Validation_Linear_Regression/Train_Test_Splits_Validation_Linear_Regression.ipynb)|Explain the difference between over-fitting and under-fitting a model abd the trade of bewteen Bias-variance. Find the optimal training and test data set splits, perform cross-validation, and describe model complexity versus error. Apply a linear regression model for supervised learning, then accelerate it with Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware.|\n",
    "|[04-Regularization_and_Gradient_Descent](04-Regularization_and_Gradient_Descent/Regularization_and_Gradient_Descent_Exercises.ipynb)|Explain cost functions, regularization, feature selection, and hyper-parameters. Summarize complex statistical optimization algorithms like gradient descent and its application to linear regression.|\n",
    "|[05-Logistic_Regression_and_Classification_Error_Metrics](05-Logistic_Regression_and_Classification_Error_Metrics/Logistic_Regression_and_Classification_Error_Metrics_Exercises.ipynb)|Describe Logistic regression and how it differs from linear regression. Identify metrics for classification errors and scenarios in which they can be used.|\n",
    "|[06-SVM_and_Kernels](06-SVM_and_Kernels/SVM_Kernels_Exercises.ipynb)|Apply support vector machines (SVMs) to a classification problems, Describe SVM similarity to logistic regression and compute the cost function for SVMs. Apply regularization in SVMs and some tips to obtain non-linear classifications with SVMs.|\n",
    "|[07-Decision_Trees](07-Decision_Trees/Decision_Trees_Exercises.ipynb)|Describe Decision trees and how to use them for classification problems and articulate some strengths and weaknesses of the algorithm. Explain how regression trees help with classifying continuous values. Describe motivation for choosing Random Forest Classifier over Decision Trees. Then accelerate Random Forest Classifier with Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware.|\n",
    "|[08-Bagging](08-Bagging/Bagging_Exercises.ipynbb)|Associate concepts of bootstrapping and aggregating (commonly known as “bagging”) to reduce variance. Apply Random Forest algorithm tp reduce the correlation seen in bagging models. Then accelerate Random Forest Classifier with Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware|\n",
    "|[09-Boosting_and_Stacking](09-Boosting_and_Stacking/Boosting_and_Stacking_Exercises.ipynb)|Explain how the boosting algorithms help reduce variance and bias. Then apply patching to the exercise to leverage underlying compute capabilities of hardware.|\n",
    "|[10-Introduction_to_Unsupervised_Learning_and_Clustering_Methods](10-Introduction_to_Unsupervised_Learning_and_Clustering_Methods/Clustering_Methods_Exercises.ipynb)|Discuss unsupervised learning algorithms and scenarios where they can be applied. Apply clustering using Kmeans Then apply patching to Kmeans to leverage underlying compute capabilities of hardware.|\n",
    "|[11-Dimensionality_Reduction_and_Advanced_Topics](11-Dimensionality_Reduction_and_Advanced_Topics/Dimensionality_Reduction_Exercises.ipynb)|Explain and Apply Principal Component Analysis (PCA), Multidimensional Scaling (MDS) and then Apply Intel® Extension for Scikit-learn* to PCA leverage underlying compute capabilities of hardware.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
