{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Exercises\n",
    "![PCAScatter.png](Assets/PCAScatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Explain and Apply Principal Component Analysis (PCA)\n",
    "- Explain Multidimensional Scaling (MDS)\n",
    "- Apply Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware\n",
    "\n",
    "# scikit-learn* \n",
    "\n",
    "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
    "\n",
    "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
    "\n",
    "These exercises will demonstrate how to apply **the Intel® Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel® oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
    "\n",
    "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code PRIOR to importing sklearn: \n",
    "\n",
    "- **from sklearnex import patch_sklearn**\n",
    "- **patch_sklearn()**\n",
    "\n",
    "## This exercise relies on installation of  Intel® Extension for Scikit-learn*\n",
    "\n",
    "If you have not already done so, follow the instructions from Week 1 for instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using customer data from a [Portuguese wholesale distributor](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers) for clustering. This data file is called `Wholesale_Customers_Data`.\n",
    "\n",
    "It contains the following features:\n",
    "\n",
    "* Fresh: annual spending (m.u.) on fresh products\n",
    "* Milk: annual spending (m.u.) on milk products\n",
    "* Grocery: annual spending (m.u.) on grocery products\n",
    "* Frozen: annual spending (m.u.) on frozen products\n",
    "* Detergents_Paper: annual spending (m.u.) on detergents and paper products\n",
    "* Delicatessen: annual spending (m.u.) on delicatessen products\n",
    "* Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n",
    "* Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n",
    "\n",
    "In this data, the values for all spending are given in an arbitrary unit (m.u. = monetary unit).\n",
    "\n",
    "### Prerequisite packages\n",
    "pandas\n",
    "numpy\n",
    "sklearn\n",
    "seaborn\n",
    "matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:06.918277Z",
     "start_time": "2021-09-24T17:32:05.258366Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['../data']\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data and check the data types.\n",
    "* Drop the channel and region columns as they won't be used.\n",
    "* Convert the remaining columns to floats if necessary.\n",
    "* Copy this version of the data (using the `copy` method) to a variable to preserve it. We will be using it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.009067Z",
     "start_time": "2021-09-24T17:32:06.921274Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda info -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.025067Z",
     "start_time": "2021-09-24T17:32:08.011067Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = os.sep.join(data_path + ['Wholesale_Customers_Data.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.041068Z",
     "start_time": "2021-09-24T17:32:08.028067Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.071066Z",
     "start_time": "2021-09-24T17:32:08.046066Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.087067Z",
     "start_time": "2021-09-24T17:32:08.073066Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Channel', 'Region'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.103067Z",
     "start_time": "2021-09-24T17:32:08.090068Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.118065Z",
     "start_time": "2021-09-24T17:32:08.108068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to floats\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preserve the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.134064Z",
     "start_time": "2021-09-24T17:32:08.121068Z"
    }
   },
   "outputs": [],
   "source": [
    "data_orig = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "As with the previous lesson, we need to ensure the data is scaled and (relatively) normally distributed.\n",
    "\n",
    "* Examine the correlation and skew.\n",
    "* Perform any transformations and scale data using your favorite scaling method.\n",
    "* View the pairwise correlation plots of the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.165067Z",
     "start_time": "2021-09-24T17:32:08.136065Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_mat = data.corr()\n",
    "\n",
    "# Strip the diagonal for future examination\n",
    "for x in range(corr_mat.shape[0]):\n",
    "    corr_mat.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the two categories with their respective most strongly correlated variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.181065Z",
     "start_time": "2021-09-24T17:32:08.169073Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_mat.abs().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the skew values and log transform. Looks like all of them need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.227067Z",
     "start_time": "2021-09-24T17:32:08.184068Z"
    }
   },
   "outputs": [],
   "source": [
    "log_columns = data.skew().sort_values(ascending=False)\n",
    "log_columns = log_columns.loc[log_columns > 0.75]\n",
    "\n",
    "log_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.243074Z",
     "start_time": "2021-09-24T17:32:08.230066Z"
    }
   },
   "outputs": [],
   "source": [
    "# The log transformations\n",
    "for col in log_columns.index:\n",
    "    data[col] = np.log1p(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data again. Let's use `MinMaxScaler` this time just to mix things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:08.652097Z",
     "start_time": "2021-09-24T17:32:08.247066Z"
    }
   },
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col] = mms.fit_transform(data[[col]]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:09.061136Z",
     "start_time": "2021-09-24T17:32:08.655099Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:34.244943Z",
     "start_time": "2021-09-24T17:32:09.064135Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "sns.set_style('white')\n",
    "\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Using Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html), recreate the data pre-processing scheme above (transformation and scaling) using a pipeline. If you used a non-Scikit learn function to transform the data (e.g. NumPy's log function), checkout  the custom transformer class called [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers).\n",
    "* Use the pipeline to transform the original data that was stored at the end of question 1.\n",
    "* Compare the results to the original data to verify that everything worked.\n",
    "\n",
    "*Hint:* Scikit-learn has a more flexible `Pipeline` function and a shortcut version called `make_pipeline`. Either can be used. Also, if different transformations need to be performed on the data, a [`FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html#featureunion-composite-feature-spaces) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:34.276981Z",
     "start_time": "2021-09-24T17:32:34.247939Z"
    }
   },
   "outputs": [],
   "source": [
    "# The custom NumPy log transformer\n",
    "log_transformer = FunctionTransformer(np.log1p)\n",
    "\n",
    "# The pipeline\n",
    "estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "# Convert the original data\n",
    "data_pipe = pipeline.fit_transform(data_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are identical. Note that machine learning models and grid searches can also be added to the pipeline (and in fact, usually are.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:34.292985Z",
     "start_time": "2021-09-24T17:32:34.279984Z"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(data_pipe, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Perform PCA with `n_components` ranging from 1 to 5. \n",
    "* Store the amount of explained variance for each number of dimensions.\n",
    "* Also store the feature importance for each number of dimensions. *Hint:* PCA doesn't explicitly provide this after a model is fit, but the `components_` properties can be used to determine something that approximates importance. How you decided to do so is entirely up to you.\n",
    "* Plot the explained variance and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:34.372979Z",
     "start_time": "2021-09-24T17:32:34.295985Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Fit a range of PCA models\n",
    "\n",
    "for n in range(1, 6):\n",
    "    \n",
    "    # Create and fit the model\n",
    "    PCAmod = PCA(n_components=n)\n",
    "    PCAmod.fit(data)\n",
    "    \n",
    "    # Store the model and variance\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calculate and store feature importances\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': data.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table of feature importances for each data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:34.419975Z",
     "start_time": "2021-09-24T17:32:34.376985Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values'))\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plot of explained variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:34.858657Z",
     "start_time": "2021-09-24T17:32:34.422979Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a plot of feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:35.623402Z",
     "start_time": "2021-09-24T17:32:34.862658Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Fit a `KernelPCA` model with `kernel='rbf'`. You can choose how many components and what values to use for the other parameters.\n",
    "* If you want to tinker some more, use `GridSearchCV` to tune the parameters of the `KernelPCA` model. \n",
    "\n",
    "The second step is tricky since grid searches are generally used for supervised machine learning methods and rely on scoring metrics, such as accuracy, to determine the best model. However, a custom scoring function can be written for `GridSearchCV`, where larger is better for the outcome of the scoring function. \n",
    "\n",
    "What would such a metric involve for PCA? What about percent of explained variance? Or perhaps the negative mean squared error on the data once it has been transformed and then inversely transformed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:41.952271Z",
     "start_time": "2021-09-24T17:32:35.630403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom scorer--use negative rmse of inverse transform\n",
    "def scorer(pcamodel, X, y=None):\n",
    "\n",
    "    try:\n",
    "        X_val = X.values\n",
    "    except:\n",
    "        X_val = X\n",
    "        \n",
    "    # Calculate and inverse transform the data\n",
    "    data_inv = pcamodel.fit(X_val).transform(X_val)\n",
    "    data_inv = pcamodel.inverse_transform(data_inv)\n",
    "    \n",
    "    # The error calculation\n",
    "    mse = mean_squared_error(data_inv.ravel(), X_val.ravel())\n",
    "    \n",
    "    # Larger values are better for scorers, so take negative value\n",
    "    return -1.0 * mse\n",
    "\n",
    "# The grid search parameters\n",
    "param_grid = {'gamma':[0.001, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "              'n_components': [2, 3, 4]}\n",
    "\n",
    "# The grid search\n",
    "kernelPCA = GridSearchCV(KernelPCA(kernel='rbf', fit_inverse_transform=True),\n",
    "                         param_grid=param_grid,\n",
    "                         scoring=scorer,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "\n",
    "kernelPCA = kernelPCA.fit(data)\n",
    "\n",
    "kernelPCA.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Let's explore how our model accuracy may change if we include a `PCA` in our model building pipeline. Let's plan to use sklearn's `Pipeline` class and create a pipeline that has the following steps:\n",
    "<ol>\n",
    "  <li>A scaler</li>\n",
    "  <li>`PCA(n_components=n)`</li>\n",
    "  <li>`LogisticRegression`</li>\n",
    "</ol>\n",
    "\n",
    "* Load the Human Activity data from the datasets.\n",
    "* Write a function that takes in a value of `n` and makes the above pipeline, then predicts the \"Activity\" column over a 5-fold StratifiedShuffleSplit, and returns the average test accuracy\n",
    "* For various values of n, call the above function and store the average accuracies.\n",
    "* Plot the average accuracy by number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:32:44.082308Z",
     "start_time": "2021-09-24T17:32:41.957272Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = os.sep.join(data_path + ['Human_Activity_Recognition_Using_Smartphones_Data.csv'])\n",
    "data2 = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.232534Z",
     "start_time": "2021-09-24T17:32:44.082308Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data2.drop('Activity', axis=1)\n",
    "y = data2.Activity\n",
    "sss = StratifiedShuffleSplit(n_splits=5, random_state=42)\n",
    "\n",
    "def get_avg_score(n):\n",
    "    pipe = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=n)),\n",
    "        ('estimator', LogisticRegression(max_iter=295, C=.001, penalty='l2'))\n",
    "    ]\n",
    "    pipe = Pipeline(pipe)\n",
    "    scores = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        pipe.fit(X_train, y_train)\n",
    "        scores.append(accuracy_score(y_test, pipe.predict(X_test)))\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "ns = [10, 20, 50, 100, 150, 200, 300, 400]\n",
    "#ns = [5, 10, 15, 20, 30]\n",
    "score_list = [get_avg_score(n) for n in ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.595731Z",
     "start_time": "2021-09-24T17:33:43.234536Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(ns, score_list)\n",
    "ax.set(xlabel='Number of Dimensions',\n",
    "       ylabel='Average Accuracy',\n",
    "       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Use a different scaler (MinMaxScaler) and try the PCA and observe how the Average accuracy changes with dimension count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.611020Z",
     "start_time": "2021-09-24T17:33:43.597620Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the previous question build the code for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.627021Z",
     "start_time": "2021-09-24T17:33:43.614021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the Average Accuracy to Number of Dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "\n",
    "Use Randomized PCA and draw your observations on how the Average Accuracy changed with number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.643062Z",
     "start_time": "2021-09-24T17:33:43.629023Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the previous question build the code for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.659029Z",
     "start_time": "2021-09-24T17:33:43.645021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the average accuracy against the number of Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "Try using Robust Scaler and try the PCA and observe how the Average accuracy changes with dimension count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.674167Z",
     "start_time": "2021-09-24T17:33:43.661020Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the previous question build the code for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:33:43.690162Z",
     "start_time": "2021-09-24T17:33:43.677022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the average accuracy against the number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2022.3)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
