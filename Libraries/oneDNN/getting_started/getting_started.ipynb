{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.1 - Introduction to Intel® oneAPI Deep Neural Network Library (oneDNN)  - Getting Started\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn different oneDNN configurations inside the Intel® oneAPI toolkit\n",
    "* Learn how to compile a oneDNN sample with different configurations via batch jobs on the Intel® DevCloud for oneAPI or in local environments.\n",
    "* Learn how to program oneDNN with a simple sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Getting Started Sample Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduce oneDNN configurations inside Intel oneAPI toolkits\n",
    "oneDNN has four different configurations inside the Intel oneAPI toolkits. Each configuration is in a different folder under the oneDNN installation path, and each configurations supports a different compiler or threading library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the installation path of your oneAPI toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n'     $ONEAPI_INSTALL/oneDNN/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are four different folders under the oneDNN installation path, and each of those configurations supports different features. This tutorial will show you how to compile and run against different oneDNN configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preparing the getting_started.cpp code\n",
    "\n",
    "This exercise use the getting_started.cpp example from oneDNN installation path.\n",
    "\n",
    "First, below section will copy the getting_started.cpp file into lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/getting_started.cpp lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can browser source codes by running below section, and below section also remove comments for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cpp -fpreprocessed  -dD -E lab/getting_started.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, copy the required header files and CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.hpp lab/\n",
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.h lab/\n",
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Run with oneAPI DPC++ Compiler \n",
    "one of the oneDNN configurations supports oneAPI DPC++ compiler, and it can run on different architectures by using DPC++.\n",
    "The following section shows you how to build with DPC++ and run on different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler **dpcpp** command and flags that will generate the exectuable.\n",
    "In order to use DPC++ compiler and related SYCL runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ configuration : \n",
    "\n",
    "   -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir dpcpp\n",
    "cd dpcpp\n",
    "cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or a local machine.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "By default, the built program uses CPU as the execution engine, but the user can switch to GPU by giving an input argument \"gpu\".\n",
    "The user can refer run.sh below to run on GPU.\n",
    "To run on CPU, simply remove the input argument \"gpu\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./dpcpp/out/getting-started-cpp cpu\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! rm -rf dpcpp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Build and Run with GNU Compiler and OpenMP \n",
    "One of the oneDNN configurations supports GNU compilers, but it can run only on CPU.\n",
    "The following section shows you how to build with G++ and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler command and flags that will generate the exectuable.\n",
    "The user must switch to the G++ oneDNN configurations by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "In order to use the G++ compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ configuration : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the G++ oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_gomp/out/getting-started-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! rm -rf cpu_gomp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Build and Run with Intel Compiler and OpenMP\n",
    "One of the oneDNN configurations supports Intel compilers, but it can run only on CPU.\n",
    "The following section shows you how to build with ICC and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : This section is optional and it is for developers who want to use Intel Compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler command and flags that will generate the executable.\n",
    "The user must switch to the ICC oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_iomp\" when running \"source setvars.sh\".\n",
    "In order to use ICC compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ configuration : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=icc -DCMAKE_CXX_COMPILER=icpc -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_iomp --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_iomp\n",
    "cd cpu_iomp\n",
    "cmake .. -DCMAKE_C_COMPILER=icc -DCMAKE_CXX_COMPILER=icpc -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the ICC oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_iomp\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_iomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_iomp/out/getting-started-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Build and Run with GNU Compiler and oneTBB \n",
    "One of the oneDNN configurations supports Intel® oneAPI Threading bBuilding block Blocks (oneTBB) as its threading library, but it can run only on CPU.\n",
    "The following section shows you how to build with oneTBB and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : This section is optional and it is for developers who want to use Intel oneTBB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler **dpcpp** command and flags that will generate the executable.\n",
    "The user must switch to the G++ oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "In order to use G++ compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ configuration : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=TBB -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_tbb --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_tbb\n",
    "cd cpu_tbb\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=TBB -DDNNL_GPU_RUNTIME=NONE\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or a local machine.\n",
    "\n",
    "#### Script - run.sh\n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the oneDNN with oneTBB threading configuration by inputting a custom configuration \"--dnnl-configuration=cpu_tbb\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_tbb --force> /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_tbb/out/getting-started-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab the developer learned the following:\n",
    "* What are the different oneDNN configurations inside the Intel oneAPI toolkits\n",
    "* How to compile a oneDNN sample with different configurations via batch jobs on the Intel oneAPI DevCloud or in local environments\n",
    "* How to program oneDNN with a simple sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
