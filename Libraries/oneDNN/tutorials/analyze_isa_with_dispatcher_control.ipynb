{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze ISA usage with IntelÂ® oneAPI Deep Neural Network Library (oneDNN) Samples by using CPU Dispatcher Control\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn how to use CPU Dispatch Control to generate JIT codes among different Instruction Set Architecture (ISA) on CPU\n",
    "* Analyze different JIT Kernel and CPU instructions usage among different ISA\n",
    "    - AVX512 vs AVX2\n",
    "    - AVX512 VNNI vs AVX512\n",
    "    - AVX512 BF16 vs AVX512 (Optional, no hardware support in DevCloud now.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module also shows the elapsed time percentage over different oneDNN JIT kernels, so users can also see the usage of specific JIT Kernels for VNNI or BF16 instructions.\n",
    "\n",
    "<img src=\"images/vnni.JPG\" style=\"float:left\" width=400>\n",
    "<img src=\"images/bf16.JPG\" style=\"float:right\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CPU Dispatch Control and ISA Analysis Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## prerequisites\n",
    "****\n",
    "### Step 1: Prepare the build/run environment\n",
    "oneDNN has four different configurations inside the Intel oneAPI toolkits. Each configuration is in a different folder under the oneDNN installation path, and each configuration supports a different compiler or threading library  \n",
    "\n",
    "Set the installation path of your oneAPI toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default path: /opt/intel/oneapi\n",
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n'     $ONEAPI_INSTALL/dnnl/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are four different folders under the oneDNN installation path, and each of those configurations supports different features. This tutorial will use the cpu_gomp configuration to do ISA analysis on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get current platform information for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PlatformUtils\n",
    "plat_utils = PlatformUtils()\n",
    "plat_utils.dump_platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2: Preparing the samples code\n",
    "\n",
    "This exercise uses the cnn_inference_f32.cpp and cnn_inference_int8.cpp examples from the oneDNN installation path.\n",
    "\n",
    "The section below will copy the cnn_inference_f32.cpp and cnn_inference_int8.cpp files into lab folder.  \n",
    "This section also copies the required header files and CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_gomp/examples/cnn_inference_f32.cpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_gomp/examples/cnn_inference_int8.cpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_gomp/examples/cpu_cnn_training_bf16.cpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_gomp/examples/example_utils.hpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_gomp/examples/example_utils.h lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_gomp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build and Run with GNU Compiler and OpenMP \n",
    "One of the oneDNN configurations supports the GNU Compiler.\n",
    "The following section shows you how to build with the GNU Compiler and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler **g++** command and flags that will generate the exectuable.\n",
    "In order to use GNU compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for cpu_gomp configuration: \n",
    "\n",
    "   -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or a local machine.\n",
    "\n",
    "#### Script - run.sh\n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user can refer to run.sh below to run cnn-inference-f32-cpp on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=0\n",
    "./cpu_gomp/out/cnn-inference-f32-cpp\n",
    "./cpu_gomp/out/cnn-inference-int8-cpp\n",
    "./cpu_gomp/out/cpu-cnn-training-bf16-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit **build.sh** and **run.sh** to the job queue.\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts either on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf dpcpp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Run Time CPU Dispatcher Controls\n",
    "***\n",
    "In this section, we run workloads on the latest Xeon server from DevCloud, and use CPU dispatcher controls to generate JIT kernels among different ISA for comparison.\n",
    "Users will understand the usage of different ISA by analyzing oneDNN Verbose logs and JIT Dump files.\n",
    "Refer to the [link](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html) for detailed CPU Dispatcher Controls information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the feature is enabled at build-time, you can use the DNNL_MAX_CPU_ISA environment variable to limit processor features. oneDNN is able to detect to certain Instruction Set Architecture (ISA) and older instruction sets. It can also be used to enable ISAs with initial support in the library that are otherwise disabled by default.\n",
    "\n",
    "|Environment variable Value|Description| introduced with microarchitecture |\n",
    "|:----|:-----|:-----|\n",
    "|SSE41|Intel Streaming SIMD Extensions 4.1 (Intel SSE4.1)| Penryn |\n",
    "|AVX|Intel Advanced Vector Extensions (Intel AVX)|Sandy Bridge |\n",
    "|AVX2|Intel Advanced Vector Extensions 2 (Intel AVX2)| Haswell |\n",
    "|AVX512_CORE|Intel AVX-512 with AVX512BW, AVX512VL, and AVX512DQ extensions| Skylake-X |\n",
    "|AVX512_CORE_VNNI|Intel AVX-512 with Intel Deep Learning Boost (Intel DL Boost)| Cascade Lake |\n",
    "|AVX512_CORE_BF16|Intel AVX-512 with Intel DL Boost and bfloat16 support| Cooper Lake |\n",
    "|ALL|No restrictions on the above ISAs, but excludes the below ISAs with initial support in the library (default)| |\n",
    "|AVX512_CORE_AMX|Intel AVX-512 with Intel DL Boost and bfloat16 support and Intel Advanced Matrix Extensions (Intel AMX) with 8-bit integer and bfloat16 support (initial support) | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ISA Comparison\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below compares and analyzes different ISA upon JIT Kernel usage and CPU instruction usage.\n",
    "\n",
    "The table below shows the different comparison by using different oneDNN samples,   \n",
    "and also brings up the keypoint of the comparison. \n",
    "\n",
    "|ISA Comparation | oneDNN sample | Description | \n",
    "|:----|:-----|:-----|\n",
    "|AVX512 vs AVX2 |cnn-inference-f32-cpp| show the usage of zmm instruction and avx512 JIT kernel | \n",
    "|AVX512 VNNI vs AVX512 |cnn-inference-int8-cpp| show the usage of VNNI instruction and VNNI JIT kernel|\n",
    "|AVX512 BF16 vs AVX512| cnn-training-bf16-cpp| show the usage of BF16 instruction and BF16 JIT kernel| \n",
    "\n",
    "Those comparisons can be conducted on the same CPU microarchitecture with the help of oneDNN CPU dispatcher control.  \n",
    "Users can also conduct similiar comparisons for TensorFlow or PyTorch workloads by replacing the oneDNN sample with other workloads.  \n",
    "By conducting similar comparisons of real workloads, users can understand:  \n",
    "* Whether the workloads leverage the latest instructions like VNNI on the platform\n",
    "* How much performance benefit is gained by using the latest instruction on the same platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pick one of ISA comparisons\n",
    "After users pick an ISA comparison, related environment variables will be exported.  \n",
    "  \n",
    "The section below will list out all ISA comparison options with index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA_COMPARISON_LIST=[\"avx512_avx2\",\"avx512-vnni_avx512\",\"avx512-bf16_avx512\"]\n",
    "index =0 \n",
    "for ISA_C in ISA_COMPARISON_LIST:\n",
    "    print(\" %d : %s \" %(index, ISA_C))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please select a comparison option and assign its index to the ISAIndex variable.\n",
    ">NOTE: no bf16 support in DevCloud now. Please **IGNORE avx512-bf16_avx512** comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISAIndex=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below will export related environment variables according to the selected ISA comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA_COMPARISON = ISA_COMPARISON_LIST[ISAIndex]\n",
    "print(\" Compare between \", ISA_COMPARISON)\n",
    "import os\n",
    "if ISA_COMPARISON == \"avx512_avx2\":\n",
    "    # variables for AVX2\n",
    "    os.environ[\"DNNL_MAX_CPU_ISA_VAL1\"] = \"AVX2\"\n",
    "    os.environ[\"DNNL_APP_VAL1\"] = \"cnn-inference-f32-cpp\"\n",
    "    os.environ[\"DNNL_LOG_VAL1\"] = \"log_cpu_f32_avx2.csv\"\n",
    "    os.environ[\"DNNL_JIT_FD_VAL1\"] = \"jitdump_f32_avx2\"\n",
    "    # variables for AVX512\n",
    "    os.environ[\"DNNL_MAX_CPU_ISA_VAL2\"] = \"AVX512_CORE\"\n",
    "    os.environ[\"DNNL_APP_VAL2\"] = \"cnn-inference-f32-cpp\"\n",
    "    os.environ[\"DNNL_LOG_VAL2\"] = \"log_cpu_f32_avx512.csv\"\n",
    "    os.environ[\"DNNL_JIT_FD_VAL2\"] = \"jitdump_f32_avx512\"\n",
    "    # AVX512 specific register\n",
    "    os.environ[\"DNNL_ISA_KEYWORD\"] = \"zmm\"\n",
    "    \n",
    "elif ISA_COMPARISON == \"avx512-vnni_avx512\":\n",
    "    # variables for AVX512\n",
    "    os.environ[\"DNNL_MAX_CPU_ISA_VAL1\"] = \"AVX512_CORE\"\n",
    "    os.environ[\"DNNL_APP_VAL1\"] = \"cnn-inference-int8-cpp\"\n",
    "    os.environ[\"DNNL_LOG_VAL1\"] = \"log_cpu_int8_avx512.csv\"\n",
    "    os.environ[\"DNNL_JIT_FD_VAL1\"] = \"jitdump_int8_avx512\"\n",
    "    # variables for AVX512 VNNI\n",
    "    os.environ[\"DNNL_MAX_CPU_ISA_VAL2\"] = \"AVX512_CORE_VNNI\"\n",
    "    os.environ[\"DNNL_APP_VAL2\"] = \"cnn-inference-int8-cpp\"\n",
    "    os.environ[\"DNNL_LOG_VAL2\"] = \"log_cpu_int8_avx512_vnni.csv\"\n",
    "    os.environ[\"DNNL_JIT_FD_VAL2\"] = \"jitdump_int8_avx512_vnni\"\n",
    "    # VNNI specific instruction\n",
    "    os.environ[\"DNNL_ISA_KEYWORD\"] = \"vpdpbusd\"   \n",
    "    \n",
    "elif ISA_COMPARISON == \"avx512-bf16_avx512\":\n",
    "    # variables for AVX512\n",
    "    os.environ[\"DNNL_MAX_CPU_ISA_VAL1\"] = \"AVX512_CORE\"\n",
    "    os.environ[\"DNNL_APP_VAL1\"] = \"cpu-cnn-training-bf16-cpp\"\n",
    "    os.environ[\"DNNL_LOG_VAL1\"] = \"log_cpu_bf16_avx512.csv\"\n",
    "    os.environ[\"DNNL_JIT_FD_VAL1\"] = \"jitdump_bf16_avx512\"\n",
    "    # variables for AVX512 BF16\n",
    "    os.environ[\"DNNL_MAX_CPU_ISA_VAL2\"] = \"AVX512_CORE_BF16\"\n",
    "    os.environ[\"DNNL_APP_VAL2\"] = \"cpu-cnn-training-bf16-cpp\"\n",
    "    os.environ[\"DNNL_LOG_VAL2\"] = \"log_cpu_bf16_avx512_bf16.csv\"\n",
    "    os.environ[\"DNNL_JIT_FD_VAL2\"] = \"jitdump_bf16_avx512_bf16\"\n",
    "    # BF16 specific instructions\n",
    "    os.environ[\"DNNL_ISA_KEYWORD\"] = \"vdpbf16ps|vcvtne2ps2bf16\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Script - run.sh for first selected ISA.    ex: AVX2, or AVX512_CORE\n",
    "****\n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user can refer to run.sh below to run the oneDNN sample on CPU with the selcted ISA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "print out the selected ISA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $DNNL_MAX_CPU_ISA_VAL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare run.sh and use DNNL_MAX_CPU_ISA to run sample on selected ISA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force  > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=2 \n",
    "# enable JIT Dump\n",
    "export DNNL_JIT_DUMP=1\n",
    "\n",
    "DNNL_MAX_CPU_ISA=$DNNL_MAX_CPU_ISA_VAL1 ./cpu_gomp/out/$DNNL_APP_VAL1 cpu >> $DNNL_LOG_VAL1 2>&1\n",
    "\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting  **run.sh** to the job queue\n",
    "> NOTE: By assigning clx to property, users can execute the sample on a Cascade Lake platform from Intel DevCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export property=clx; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  gather all JIT bin files into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $DNNL_JIT_FD_VAL1; mkdir $DNNL_JIT_FD_VAL1; mv *.bin $DNNL_JIT_FD_VAL1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Script - run.sh for second selected ISA. ex: AVX512_CORE_VNNI or AVX512_CORE_BF16\n",
    "**** \n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user can refer to run.sh below to run the oneDNN sample on CPU with the selcted ISA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "print out the selected ISA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $DNNL_MAX_CPU_ISA_VAL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare run.sh and use DNNL_MAX_CPU_ISA to run sample on selected ISA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force  > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=2 \n",
    "# enable JIT Dump\n",
    "export DNNL_JIT_DUMP=1\n",
    "\n",
    "DNNL_MAX_CPU_ISA=$DNNL_MAX_CPU_ISA_VAL2 ./cpu_gomp/out/$DNNL_APP_VAL2 cpu >> $DNNL_LOG_VAL2 2>&1\n",
    "\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting  **run.sh** to the job queue\n",
    "> NOTE: By assigning clx to property, users can execute the sample on a Cascade Lake platform from Intel DevCloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export property=clx; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  gather all JIT bin files into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf $DNNL_JIT_FD_VAL2; mkdir $DNNL_JIT_FD_VAL2; mv *.bin $DNNL_JIT_FD_VAL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### Step 4: oneDNN Verbose Log JIT Kernel Time BreakDown\n",
    "oneDNN uses just-in-time compilation (JIT) to generate optimal code for some functions based on input parameters and instruction set supported by the system.   \n",
    "Therefore, users can see different JIT kernel type among different first selected ISA and second selected ISA.   \n",
    "For example, users can see avx_core_vnni JIT kernel if the workload uses VNNI instruction on Cascake Lake platform.  \n",
    "Moreover, users can identify the top hotspots of JIT kernel executions with this time breakdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse verbose log and get the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import oneDNNUtils, oneDNNLog\n",
    "onednn = oneDNNUtils()\n",
    "\n",
    "logfile1 = os.environ[\"DNNL_LOG_VAL1\"]\n",
    "log1 = oneDNNLog()\n",
    "log1.load_log(logfile1)\n",
    "exec_data1 = log1.exec_data\n",
    "\n",
    "logfile2 = os.environ[\"DNNL_LOG_VAL2\"]\n",
    "log2 = oneDNNLog()\n",
    "log2.load_log(logfile2)\n",
    "exec_data2 = log2.exec_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   JIT Kernel Type Time breakdown for first selected ISA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data1,\"jit\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   JIT Kernel Type Time breakdown for second selected ISA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: users should be able to see **avx512_core_vnni** JIT Kernel if the sample run with **VNNI** instruction  \n",
    "> NOTE: users should be able to see **avx512_core_bf16** JIT Kernel if the sample run with **BF16** instruction  \n",
    "> NOTE: users should be able to see **avx512** JIT Kernel if the sample run with **AVX512** instructions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data2,\"jit\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Primitives Type Speedup from second selected ISA\n",
    "oneDNN samples here are not for performance benchmarking, so the digram below gives you only a rough idea of performance speedup from the second selected ISA such as AVX512, VNNI, or BF16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " onednn.stats_comp('type', 'time',log2, log1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### Step 5: Inspect JIT Kernel \n",
    "In this section, we analyze dump JIT files on the built samples from Step 2 and Step 3.   \n",
    "Users should be able to see exact CPU instruction usage like VNNI or BF16 from those JIT Dump files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspect either first or second selected ISA by setting VALIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To inspect the first selected ISA JIT Dump files, set VALIndex as 1.  \n",
    "* To inspect second selected ISA JIT Dump files, set VALIndex as 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIndex=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List out all JIT Dump Files with index number for the first or second selected ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VAL=\"DNNL_JIT_FD_VAL\"+str(VALIndex)\n",
    "JIT_DUMP_FD=os.environ[VAL]\n",
    "print(\"Inspect Folder: \", JIT_DUMP_FD)\n",
    "\n",
    "filenames= os.listdir (JIT_DUMP_FD) \n",
    "result = []\n",
    "keyword = \".bin\"\n",
    "for filename in filenames: \n",
    "    #if os.path.isdir(os.path.join(os.path.abspath(\".\"), filename)): \n",
    "    if filename.find(keyword) != -1:\n",
    "        result.append(filename)\n",
    "result.sort()\n",
    "\n",
    "index =0 \n",
    "for folder in result:\n",
    "    print(\" %d : %s \" %(index, folder))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a JIT Dump file by putting its index value below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FdIndex=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export JIT Dump file to environment variable JITFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FdIndex < len(result):\n",
    "    logfile = result[FdIndex]\n",
    "    os.environ[\"JITFILE\"] = JIT_DUMP_FD+os.sep+logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### disassembler JIT Dump file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: zmm register is introduced by AVX512 ISA.  \n",
    "Users should see usage of **zmm** register in AVX512 JIT dump files.  \n",
    "\n",
    "> NOTE: vpdpbusd is introduced by AVX512_VNNI ISA.  \n",
    "Users should see usage of **vpdpbusd** in AVX512_VNNI JIT dump files. \n",
    "\n",
    "> NOTE: **vdpbf16ps**, **vcvtne2ps2bf16**, and **vcvtneps2bf16** are introduced by AVX512_BF16 ISA.  \n",
    "Users should see usage of vdpbf16ps, vcvtne2ps2bf16 or vcvtneps2bf16 in AVX512_BF16 JIT dump files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: For disassembler vdpbf16ps, vcvtne2ps2bf16, and vcvtneps2bf16 instructions, users must use objdump with **v2.34** or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!objdump -D -b binary -mi386:x86-64 $JITFILE | grep -E $DNNL_ISA_KEYWORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab the developer learned the following:\n",
    "* use CPU Dispatch Control to generate JIT codes among different Instruction Set Architecture on CPU\n",
    "* understand different JIT Kernels and CPU instructions usage among different ISA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
