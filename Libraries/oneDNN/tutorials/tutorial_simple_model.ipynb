{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.1 - port a Intel® oneAPI Deep Neural Network Library (oneDNN)  sample from CPU to GPU  - oneDNN CNN FP32 Inference\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn how to port a oneDNN sample from a CPU-only version to a CPU&GPU version by using DPC++\n",
    "* Learn how to program a simple convolutional neural network by using oneDNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise : Porting oneDNN application from CPU to GPU \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : introduce oneDNN configurations inside Intel® oneAPI toolkits\n",
    "oneDNN has four different configurations inside the Intel oneAPI toolkits. Each configuration is in a different folder under the oneDNN installation path, and each configuration supports different compilers or threading libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the installation path of your Intel oneAPI toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isdir(os.environ['ONEAPI_INSTALL']) == False:\n",
    "    print(\"ERROR! wrong oneAPI installation path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n'    $ONEAPI_INSTALL/oneDNN/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 4 different folders under the oneDNN installation path, and each of those configurations supports different features. This tutorial will make use of two configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir lab;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2 : scanning the cnn_inference_f32.cpp code which only supports CPU\n",
    "\n",
    "This C++ API example demonstrates how to build an AlexNet neural network topology for forward-pass inference, and it can run only on CPU.\n",
    "You can find a detailed code explanation at this [link](https://oneapi-src.github.io/oneDNN/cnn_inference_f32_cpp.html)\n",
    "\n",
    "There is a cnn_inference_f32.cpp, which has a CPU-only implementation.\n",
    "Let us copy into the lab folder, and use it as the base of the lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp codes_for_ipynb/cnn_inference_f32.cpp lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user could check the source file using the following command, but we recommened to use the detailed code explanation at this [link](https://oneapi-src.github.io/oneDNN/cnn_inference_f32_cpp.html) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lab/cnn_inference_f32.cpp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, copy the required CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_gomp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3:   Build and Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run with GNU Compiler and OpenMP \n",
    "For this CPU-only AlexNet neural network topology for forward-pass inference sample, the GNU compiler is used.\n",
    "The following section guides you how to build with G++ and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler  command and flags that will generate the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp  --force > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make cnn-inference-f32-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the Intel DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the G++ oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "\n",
    "By default, oneDNN Verbose log is disabled.\n",
    "You can unmark  #export DNNL_VERBOSE=1 to enable oneDNN verbose log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp  --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# unmark below line to enable oneDNN verbose log\n",
    "#export DNNL_VERBOSE=1\n",
    "./cpu_gomp/out/cnn-inference-f32-cpp\n",
    "echo \"########## Done with the run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!rm -rf cpu_gomp; chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable oneDNN Verbose log and check the engine kind for each operation\n",
    "cpu should be the engine kind for most of the operations, and you should be able to check the engine kind after \"dnnl_verbose,exec,\" for each operation.\n",
    "Check this [link](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) for a detailed explanation of oneDNN verbose log.\n",
    "\n",
    "Below is an example for oneDNN verbose log for convolution on CPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb1_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0,0.458008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4 : Modifying the cnn_inference_f32.cpp code to support both CPU and GPU\n",
    "\n",
    "In this session, we will convert the above cnn_inference_f32.cpp to support both CPU and GPU and compile the sample with DPC++ instead of G++.\n",
    "\n",
    "There are three steps to do the code conversion from CPU to GPU for this sample.\n",
    "\n",
    "* Step 1 : change engine::kind from CPU to GPU\n",
    "* Step 2 : implement a function to access GPU memory via SYCL buffer and its accessor\n",
    "* Step 3 : write user's data into GPU memory via the implemented function from Step 2\n",
    "\n",
    "There is a cnn_inference_f32.patch file inside the src folder. It contains all the changes for porting CPU to GPU against the CPU-only version of cnn_inference_f32.cpp.\n",
    "First we must patch the cnn_inference_f32.cpp under the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd lab;patch < ../codes_for_ipynb/cnn_inference_f32.patch;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can check the source file using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lab/cnn_inference_f32.cpp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find related modification in below cnn_inference_f32.cpp, and the modifications for each step are wrapped up with \">>>>>>\" and \"<<<<<<\".\n",
    "\n",
    "### step1 : change engine::kind from CPU to GPU\n",
    "changing engine kind from cpu to gpu during engine instantiation.\n",
    "* Before patching : engine eng(engine::kind::cpu, 0);\n",
    "* After patching : engine eng(engine::kind::gpu, 0);\n",
    "\n",
    "### step 2 : implement a function to access GPU memory via SYCL buffer and its accessor\n",
    "You can refer to the below function write_to_dnnl_memory for that.\n",
    "overall, we use SYCL buffer and its accessor to access GPU memory.\n",
    "auto buffer = mem.get_sycl_buffer<uint8_t>();\n",
    "auto dst = buffer.get_access<cl::sycl::access::mode::write>();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+// ------ GPU code conversion --Step 2 >>>>>>\n",
    "+// Read from handle, write to memory\n",
    "+inline void write_to_dnnl_memory(void *handle, dnnl::memory &mem) {\n",
    "+\n",
    "+    dnnl::engine eng = mem.get_engine();\n",
    "+    size_t size = mem.get_desc().get_size();\n",
    "+\n",
    "+    bool is_cpu_sycl = (DNNL_CPU_RUNTIME == DNNL_RUNTIME_SYCL\n",
    "+            && eng.get_kind() == dnnl::engine::kind::cpu);\n",
    "+    bool is_gpu_sycl = (DNNL_GPU_RUNTIME == DNNL_RUNTIME_SYCL\n",
    "+            && eng.get_kind() == dnnl::engine::kind::gpu);\n",
    "+    if (is_cpu_sycl || is_gpu_sycl) {\n",
    "+\n",
    "+        auto buffer = mem.get_sycl_buffer<uint8_t>();\n",
    "+        auto dst = buffer.get_access<cl::sycl::access::mode::write>();\n",
    "+        uint8_t *dst_ptr = dst.get_pointer();\n",
    "+\n",
    "+        if (!dst_ptr || !handle) {\n",
    "+            std::cerr << \"memory is NULL\"\n",
    "+                      << \"\\n\";\n",
    "+            return;\n",
    "+        }\n",
    "+        for (size_t i = 0; i < size; ++i)\n",
    "+            dst_ptr[i] = ((uint8_t *)handle)[i];\n",
    "+        return;\n",
    "+    }\n",
    "+\n",
    "+    if (eng.get_kind() == dnnl::engine::kind::cpu) {\n",
    "+        uint8_t *dst = static_cast<uint8_t *>(mem.get_data_handle());\n",
    "+        if (!dst || !handle) {\n",
    "+            std::cerr << \"memory is NULL\"\n",
    "+                      << \"\\n\";\n",
    "+            return;\n",
    "+        }\n",
    "+        for (size_t i = 0; i < size; ++i)\n",
    "+            dst[i] = ((uint8_t *)handle)[i];\n",
    "+        return;\n",
    "+    }\n",
    "+\n",
    "+    assert(!\"not expected\");\n",
    "+}\n",
    "+//<<<<<< ------ GPU code conversion --Step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 3 : write user's data into GPU memory via the implemented function from Step 2\n",
    " For accessing user data in GPU memory, we can't use the host pointer to write data into that, but we use write_to_dnnl_memory function instead. Refer to the code snapshot below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "     auto user_src_memory = memory(\n",
    "-            { { conv1_src_tz }, dt::f32, tag::nchw }, eng, user_src.data());\n",
    "+            { { conv1_src_tz }, dt::f32, tag::nchw }, eng);\n",
    "+    write_to_dnnl_memory(user_src.data(), user_src_memory);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run with oneAPI DPC++ Compiler \n",
    "For this  AlexNet neural network topology for forward-pass inference sample on GPU, DPC++ is used as the compiler.\n",
    "The following section guides you how to build with DPC++ and run on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler  command and flags that will generate the exectuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh  --dnnl-configuration=cpu_dpcpp_gpu_dpcpp --force > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir dpcpp\n",
    "cd dpcpp\n",
    "cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL\n",
    "make cnn-inference-f32-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "\n",
    "By default, oneDNN Verbose log is disabled.\n",
    "You can unmark  #export DNNL_VERBOSE=1 to enable oneDNN verbose log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh  --dnnl-configuration=cpu_dpcpp_gpu_dpcpp --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "#export DNNL_VERBOSE=1\n",
    "./dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "echo \"########## Done with the run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dpcpp; chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable oneDNN Verbose log and check the engine kind for each operation\n",
    "gpu should be the engine kind for most of the operations, and you should be able to check the engine kind after \"dnnl_verbose,exec,\" for each operation.\n",
    "Check this [link](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) for a detailed explanation of oneDNN verbose log.\n",
    "\n",
    "Below is an example for oneDNN verbose log for convolution on GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnnl_verbose,exec,gpu,convolution,ocl:gen9:blocked,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd16b:f0,,alg:convolution_direct,mb1_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab, the developer learned the following:\n",
    "* How to port a oneDNN sample from CPU-only version to CPU&GPU version\n",
    "* How to program a simple convolutional neural network by using oneDNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
