{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile IntelÂ® oneAPI Deep Neural Network Library (oneDNN) Samples by using Verobse Mode and JIT DUMP inspection\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn how to use Verbose Mode to profile oneDNN samples on CPU & GPU\n",
    "* Learn how to inspect JIT Dump to profile oneDNN samples on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module shows the elapsed time percentage over different oneDNN primitives\n",
    "<img src=\"images/cpu.JPG\" style=\"float:left\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module also shows the elapsed time percentage over different oneDNN JIT or GPU kernels\n",
    "<img src=\"images/cpu_jit.JPG\" style=\"float:left\" width=400>\n",
    "<img src=\"images/gpu_kernel.JPG\" style=\"float:right\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Verbose Mode Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## prerequisites\n",
    "***\n",
    "### Step 1: Prepare the build/run environment\n",
    "oneDNN has four different configurations inside the Intel oneAPI toolkits. Each configuration is in a different folder under the oneDNN installation path, and each configuration supports a different compiler or threading library  \n",
    "\n",
    "Set the installation path of your oneAPI toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default path: /opt/intel/oneapi\n",
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isdir(os.environ['ONEAPI_INSTALL']) == False:\n",
    "    print(\"ERROR! wrong oneAPI installation path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n'     $ONEAPI_INSTALL/dnnl/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are four different folders under the oneDNN installation path, and each of those configurations supports different features. This tutorial will use the dpcpp configuration to showcase the verbose log for both CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get current platform information for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PlatformUtils\n",
    "plat_utils = PlatformUtils()\n",
    "plat_utils.dump_platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2: Preparing the samples code\n",
    "\n",
    "This exercise uses the cnn_inference_f32.cpp example from oneDNN installation path.\n",
    "\n",
    "The section below will copy the cnn_inference_f32.cpp file into the lab folder.  \n",
    "This section also copies the required header files and CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/cnn_inference_f32.cpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.hpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.h lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build and Run with the oneAPI DPC++ Compiler \n",
    "One of the oneDNN configurations supports the oneAPI DPC++ compiler, and it can run on different architectures by using DPC++.\n",
    "The following section shows you how to build with DPC++ and run on different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler **dpcpp** command and flags that will generate the exectuable.\n",
    "To enable use of the DPC++ compiler and the related SYCL runtime, some definitions must be passed as cmake arguments.\n",
    "Here are the related cmake arguments for the DPC++ configuration: \n",
    "\n",
    "   -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir dpcpp\n",
    "cd dpcpp\n",
    "cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL\n",
    "make cnn-inference-f32-cpp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or a local machine.\n",
    "\n",
    "#### Script - run.sh\n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "By default, the built program uses CPU as the execution engine, but the user can switch to GPU by specifying the input argument \"gpu\".\n",
    "The user can refer to run.sh below to run cnn-inference-f32-cpp on both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=0\n",
    "./dpcpp/out/cnn-inference-f32-cpp cpu\n",
    "./dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit **build.sh** and **run.sh** to the job queue.\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts either on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! rm -rf dpcpp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "## Enable Verbose Mode\n",
    "***\n",
    "In this section, we enable verbose mode on the built sample from the previous section, and users can see different results from CPU and GPU.  \n",
    "Refer to the [link](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) for detailed verbose mode information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the feature is enabled at build-time, you can use the DNNL_VERBOSE environment variable to turn verbose mode on and control the level of verbosity.\n",
    "\n",
    "|Environment variable|Value|Description|\n",
    "|:-----|:----|:-----|\n",
    "|DNNL_VERBOSE| 0 |no verbose output (default)|\n",
    "||1|primitive information at execution|\n",
    "||2|primitive information at creation and execution|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare run.sh and enable DNNL_VERBOSE as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=2 \n",
    "./dpcpp/out/cnn-inference-f32-cpp cpu >>log_cpu_f32_vb2.csv 2>&1\n",
    "./dpcpp/out/cnn-inference-f32-cpp gpu >>log_gpu_f32_vb2.csv 2>&1\n",
    "\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit **build.sh** and **run.sh** to the job queue.\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts either on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Verbose Logs\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: List out all oneDNN verbose logs\n",
    "users should see two verbose logs listed in the table below.\n",
    "\n",
    "|Log File Name | Description |\n",
    "|:-----|:----|\n",
    "|log_cpu_f32_vb2.csv| log for cpu run |\n",
    "|log_cpu_f32_vb2.csv| log for gpu run|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames= os.listdir (\".\") \n",
    "result = []\n",
    "keyword = \".csv\"\n",
    "for filename in filenames: \n",
    "    #if os.path.isdir(os.path.join(os.path.abspath(\".\"), filename)): \n",
    "    if filename.find(keyword) != -1:\n",
    "        result.append(filename)\n",
    "result.sort()\n",
    "\n",
    "index =0 \n",
    "for folder in result:\n",
    "    print(\" %d : %s \" %(index, folder))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Pick a verbose log by putting its index value below\n",
    "Users can pick either cpu or gpu log for analysis.   \n",
    "Once users finish Step 2 to Step 8 for one log file, they can go back to step 2 and select another log file for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FdIndex=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: browse the content of selected verbose log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = result[FdIndex]\n",
    "with open(logfile) as f:\n",
    "    log = f.read()\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Parse verbose log and get the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = result[FdIndex]\n",
    "print(logfile)\n",
    "from profiling.profile_utils import oneDNNUtils, oneDNNLog\n",
    "onednn = oneDNNUtils()\n",
    "log1 = oneDNNLog()\n",
    "log1.load_log(logfile)\n",
    "data = log1.data\n",
    "exec_data = log1.exec_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Time breakdown for exec type\n",
    "The exec type includes exec and create. \n",
    "\n",
    "|exec type | Description |  \n",
    "|:-----|:----|  \n",
    "|exec | Time for primitives exection. Better to spend most of time on primitives execution. |  \n",
    "|create| Time for primitives creation. Primitives creation happens once. Better to spend less time on primitive creation. |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(data,\"exec\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Time breakdown for primitives type\n",
    "The primitives type includes convolution, reorder, sum, etc.  \n",
    "For this simple convolution net example, convolution and inner product primitives are expected to spend most of time.  \n",
    "However, the exact time percentage of different primitivies may vary among different architectures.    \n",
    "Users can easily identify top hotpots of primitives executions with this time breakdown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data,\"type\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Time breakdown for JIT kernel type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oneDNN uses just-in-time compilation (JIT) to generate optimal code for some functions based on input parameters and instruction set supported by the system.   \n",
    "Therefore, users can see different JIT kernel type among different CPU and GPU architectures.  \n",
    "For example, users can see avx_core_vnni JIT kernel if the workload uses VNNI instruction on Cascake Lake platform.  \n",
    "Users can also see different OCL kernels among different Intel GPU generations.  \n",
    "Moreover, users can identify the top hotspots of JIT kernel executions with this time breakdown.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data,\"jit\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:  Time breakdown for algorithm type\n",
    "oneDNN also supports different algorithms.  \n",
    "Users can identify the top hotspots of algorthm executions with this time breakdown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data,\"alg\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Time breakdown for architecture type\n",
    "The supported architectures include CPU and GPU.  \n",
    "For this simple net sample, we don't split computation among CPU and GPU,    \n",
    "so users should see either 100% CPU time or 100% GPU time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(data,\"arch\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Inspecting JIT Code\n",
    "\n",
    "In this section, we dump JIT code  on the built sample from the previous section, and users can see different results from CPU.    \n",
    "Refer to the [link](https://oneapi-src.github.io/oneDNN/dev_guide_inspecting_jit.html) for detailed JIT Dump information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the feature is enabled at build-time, you can use the DNNL_JIT_DUMP environment variable to inspect JIT code.\n",
    "\n",
    "|Environment variable|Value|Description|\n",
    "|:-----|:----|:-----|\n",
    "|DNNL_JIT_DUMP | 0 |JIT dump is disabled (default)|\n",
    "||any other value|JIT dump is enabled|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Prepare run.sh and enable DNNL_JIT_DUMP as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# disable verbose log\n",
    "export DNNL_VERBOSE=0\n",
    "# enable JIT Dump\n",
    "export DNNL_JIT_DUMP=1 \n",
    "./dpcpp/out/cnn-inference-f32-cpp cpu\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Submitting ***run.sh** to the job queue\n",
    "Now we can submit **run.sh** to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Move all JIT Dump files into the jitdump folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir jitdump;mv *.bin jitdump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: List out all oneDNN JIT Dump files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames= os.listdir (\"jitdump\") \n",
    "result = []\n",
    "keyword = \".bin\"\n",
    "for filename in filenames: \n",
    "    #if os.path.isdir(os.path.join(os.path.abspath(\".\"), filename)): \n",
    "    if filename.find(keyword) != -1:\n",
    "        result.append(filename)\n",
    "result.sort()\n",
    "\n",
    "index =0 \n",
    "for folder in result:\n",
    "    print(\" %d : %s \" %(index, folder))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Pick a JIT Dump file by putting its index value below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FdIndex=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: export JIT Dump file to environment variable JITFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = result[FdIndex]\n",
    "os.environ[\"JITFILE\"] = logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: disassembler JIT Dump file to view the code\n",
    "\n",
    "> NOTE: If the oneDNN sample uses VNNI instruction, users should be able to see \"vpdpbusd\" instruction from the JIT Dump file  \n",
    "\n",
    "> NOTE: If the oneDNN sample uses BF16 instruction, users should see usage of vdpbf16ps or vcvtne2ps2bf16 in the JIT dump file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: For disassembler vdpbf16ps and vcvtne2ps2bf16 instructions, users must use objdump with v2.34 or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!objdump -D -b binary -mi386:x86-64 jitdump/$JITFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab the developer learned the following:\n",
    "* how to use Verbose Mode to profile different oneDNN samples on CPU and GPU\n",
    "* how to inspect JIT Dump to profile oneDNN samples on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
