{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Intel® oneAPI Deep Neural Network Library (oneDNN) Samples by using Intel® VTune™ Profiler and oneDNN ITT Tagging feature\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn how to use VTune™ Profiler to profile oneDNN samples on CPU & GPU\n",
    "* Learn how to use oneDNN ITT Tagging feature to profile oneDNN samples on primitives level\n",
    "* Learn how to identify performance bottlenecks by VTune profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# VTune Profiling Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prerequisites\n",
    "***\n",
    "### Step 1: Prepare the build/run environment\n",
    "oneDNN has four different configurations inside the Intel oneAPI toolkits. Each configuration is in a different folder under the oneDNN installation path, and each configuration supports a different compiler or threading library  \n",
    "\n",
    "Set the installation path of your oneAPI toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isdir(os.environ['ONEAPI_INSTALL']) == False:\n",
    "    print(\"ERROR! wrong oneAPI installation path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n'     $ONEAPI_INSTALL/dnnl/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are four different folders under the oneDNN installation path, and each of those configurations supports different features. This tutorial will show you how to compile and run against different oneDNN configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lab;mkdir lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get current platform information for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PlatformUtils\n",
    "plat_utils = PlatformUtils()\n",
    "plat_utils.dump_platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2: Preparing the performance profiling sample\n",
    "\n",
    "This exercise uses the performance_profiling.cpp example from oneDNN installation path.\n",
    "> NOTE: please refer to [oneDNN doc](https://oneapi-src.github.io/oneDNN/performance_profiling_cpp.html) for the details implementation of the performance_profiling.cpp.  \n",
    "\n",
    "The section below will copy the performance_profiling.cpp file into the lab folder.  \n",
    "This section also copies the required header files and CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/performance_profiling.cpp lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can browser source codes by running below section, and below section also remove comments for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cpp -fpreprocessed  -dD -E lab/performance_profiling.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, copy the required header files and CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.hpp lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.h lab/\n",
    "!cp $ONEAPI_INSTALL/dnnl/latest/cpu_dpcpp_gpu_dpcpp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch examples to enlarge runtime for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd lab;patch < ../codes_for_ipynb/add_loop.patch;cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The performance profiling sample support different memory format\n",
    "\n",
    "|supported memory format | command |Description|\n",
    "|:-----|:----|:-----|\n",
    "|naive| performance-profiling-cpp cpu naive |use plain format (ex: NCHW) for the convolution|\n",
    "|blocked|performance-profiling-cpp cpu blocked|use blocked format (ex: nChw16c) for the convolution|\n",
    "|fused|performance-profiling-cpp cpu fused||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3:  Build and Run with GNU Compiler and OpenMP \n",
    "One of the oneDNN configurations supports GNU compilers, but it can run only on CPU.\n",
    "The following section shows you how to build with G++ and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler command and flags that will generate the exectuable.\n",
    "The user must switch to the G++ oneDNN configurations by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "In order to use the G++ compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ configuration : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make performance-profiling-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the G++ oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_gomp/out/performance-profiling-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### OPTIONAL : replace $ONEAPI_INSTALL with set value in both build.sh and run.sh\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('build.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n",
    "file_utils.replace_string_in_file('run.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf cpu_gomp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling oneDNN Performance by VTune™\n",
    "***\n",
    "In this section, we will profile the performance profiling sample with both the naive data format and the blocked data format by using VTune™ and [ITT tagging feature](https://github.com/oneapi-src/oneDNN/tree/rfcs/rfcs/20201014-VTune-ITT-tagging) from oneDNN.  \n",
    "Users should identify different vectorization and memory bound ratio for each primitive among those two data formats.\n",
    "Therefore, users could understand how data format impacts the performance with those supportive data.\n",
    "\n",
    "We uses 3 different VTune™ profiling types in this tutorial.\n",
    "Users could refer to [VTune™ performance analysis](https://software.intel.com/content/www/us/en/develop/documentation/vtune-help/top/analyze-performance.html) for more profiling types.  \n",
    "Users could also refer to the [dev_guide_profilers](https://oneapi-src.github.io/oneDNN/dev_guide_profilers.html) for oneDNN related VTune™ profiling information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Different VTune™ Profiling Types\n",
    "\n",
    "|Profiling Type|collect augument |Description|\n",
    "|:-----|:----|:-----|\n",
    "|Hotspots| 0 |no verbose output (default)|\n",
    "|Microarchitecture|1|primitive information at execution|\n",
    "|Threading|2|primitive information at creation and execution|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Profile the performance profiling sample with naive data format\n",
    "Naive implementation executes 2D convolution followed by ReLU on the data in NCHW format. This implementation does not align with oneDNN best practices and results in suboptimal performance.   \n",
    "In this section, we will use identify those performance bottlenecks caused by naive data format. \n",
    "\n",
    "> NOTE: Please refer to this page : [understanding memory format](https://oneapi-src.github.io/oneDNN/dev_guide_understanding_memory_formats.html) for more details of different data formats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hotspots Profiling Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Top oneDNN primitive hotspots\n",
    "First, we want to know which primitive takes most of the time.  \n",
    "We will profile the sample by using profile.sh and then analyze the hotspot by using analyze.sh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - profile.sh\n",
    "the script **profile.sh** encapsulates the program for submission to the profiling job queue for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profile.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the profiling\"\n",
    "vtune -collect hotspots -q -no-summary -knob sampling-mode=hw -r dnnl-vtune ./cpu_gomp/out/performance-profiling-cpp cpu naive\n",
    "echo \"########## Done with the profiling\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we filter out the column \"CPU Time:Self\" from the VTune™ result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report hotspots -q -r dnnl-vtune -format csv -csv-delimiter ',' -group-by task -column 'CPU Time:Self' | head -n 10 > hotspot.csv\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('profile.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue.\n",
    "\n",
    "> NOTE - it is possible to execute any of the profile and analyze commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command qsub. If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 profile.sh;if [ -x \"$(command -v qsub)\" ]; then ./q profile.sh; else ./profile.sh; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Pie Chart to illustrate the CPU time percentage among different primitives\n",
    "We also show the absolute CPU time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('hotspot.csv', engine='python')\n",
    "if data.empty is False:\n",
    "    print(data)\n",
    "    if len(data.columns) >= 2:            \n",
    "        data.plot.pie(y=data.columns[1], labels=data.iloc[:,0], figsize=(8, 8), fontsize=20, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Microarchitecture Profiling Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2.1 Vectorization over oneDNN primitives\n",
    "Second, we want to know how well each primitive is vectorized, and microarchitecture profiling type provides those data. \n",
    "We will profile the sample by using profile.sh and then analyze how well it is vectorized by using analyze.sh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - profile.sh\n",
    "the script **profile.sh** encapsulates the program for submission to the profiling job queue for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profile.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the profiling\"\n",
    "vtune -collect uarch-exploration -knob sampling-interval=1 -data-limit=2000 -q -no-summary -r dnnl-vtune-ue ./cpu_gomp/out/performance-profiling-cpp cpu naive\n",
    "echo \"########## Done with the profiling\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we filter out the column \"FP Arithmetic\" from the VTune™ result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report hotspots -q -r dnnl-vtune-ue -format csv -csv-delimiter ',' -group-by task -column 'FP Arithmetic' | head -n 10 > fp.csv\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('profile.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 profile.sh;if [ -x \"$(command -v qsub)\" ]; then ./q profile.sh; else ./profile.sh; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Bar Chart to illustrate the Scalar and Vector ops percentage among different primitives\n",
    "For FP Arithmetic column, there are more sub columns under it.  \n",
    "We dump all the sub columns name in below section, and pick column 4 for FP vector information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('fp.csv', engine='python')\n",
    "if data.empty is False:\n",
    "     if len(data.columns) >= 5:            \n",
    "        i = 0\n",
    "        for col in data.columns:\n",
    "            print(\" column %d : %s \"%(i,col))\n",
    "            i += 1\n",
    "\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[4], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : users should be able to see only ~40% FP Vector ratio for convolution primitive.  \n",
    "It is suboptimal, and we should try to improve it to ~100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2.2 Memory Bound over oneDNN primitives\n",
    "\n",
    "Third, we want to know any memory problem for those primitives, and microarchitecture profiling type provides those data. \n",
    "We will profile the sample by using profile.sh and then analyze memory bound issues by using analyze.sh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we filter out the column \"Memory Bound\" from the VTune™ result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report hotspots -q -r dnnl-vtune-ue -format csv -csv-delimiter ',' -group-by task -column 'Memory Bound' | head -n 10 > memory.csv\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Bar Chart to identify any DRAM/L1/L2/L3 bound issues among different primitives\n",
    "For Memory Bound column, there are more sub columns under it.  \n",
    "Users can dump all the sub columns name in below section by unmarking the line of print().  \n",
    "We pick column 1,2,11,12,17 for different memory bound information.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('memory.csv', engine='python')\n",
    "if data.empty is False:\n",
    "    if len(data.columns) >= 18:            \n",
    "        i = 0\n",
    "        for col in data.columns:\n",
    "            #print(\" column %d : %s \"%(i,col))\n",
    "            i += 1\n",
    "\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[1], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[2], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[11], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[12], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[17], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : users should be able to see more DRAM problem than L1/L2/L3 bound problems for eltwise primitive.  \n",
    "In general, we want to reduce all memory bound issues, but better to have less DRAM bound than L1/L2/L3 bound.  \n",
    "If you face more DRAM bound than L3 bound, it might mean that most of your data are not in L3 cache. \n",
    "We prefer to have data in L1/L2/L3 caches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Theading Profiling Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Thread Oversubscription\n",
    "Finally, we show case how to identify a thread oversubscription problem in VTune™, and Threading profiling type provides those data.  \n",
    "We will profile the sample by using profile.sh and then generate a summary output with oversubscription information by using analyze.sh. \n",
    "\n",
    "> NOTE : we make the thread oversubscription problem by setting OpenMP thread number to a very big value. Therefore, VTune™ will identify thread oversubscription problem caused by this wrong setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profile.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the profiling\"\n",
    "export OMP_NUM_THREADS=200 \n",
    "vtune -collect threading -data-limit=2000 -q -no-summary -r dnnl-vtune-th ./cpu_gomp/out/performance-profiling-cpp cpu naive\n",
    "echo \"########## Done with the profiling\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we just generate a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report summary -r dnnl-vtune-th --format html -report-output summary.html\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('profile.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dnnl-vtune-th; chmod 755 q;chmod 755 profile.sh;if [ -x \"$(command -v qsub)\" ]; then ./q profile.sh; else ./profile.sh; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Summary page from Thread Profiling\n",
    "Please check below summary report.  \n",
    "You should be able to see the total thread count which is the value of OMP_NUM_THREADS.  \n",
    "You should also see how long this workload has thread oversubscription issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Profile the performance profiling sample with blocked data format\n",
    "Blocked format implementation executes the same operations sequence on the blocked format optimized for convolution performance. This implementation uses format_tag=ANY to create a convolution memory descriptor to determine the data format optimal for the convolution implementation. It then propagates the blocked format to the non-intensive ReLU. This implementation results in better overall performance than the naive implementation.  \n",
    "In this section, we will use identify those performance improvements including better vectorization when users change data format from naive to blocked.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hotspots Profiling Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Top oneDNN primitive hotspots\n",
    "First, we want to know which primitive takes most of the time.  \n",
    "We will profile the sample by using profile.sh and then analyze the hotspot by using analyze.sh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profile.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the profiling\"\n",
    "vtune -collect hotspots -q -no-summary -knob sampling-mode=hw -r dnnl-vtune-b ./cpu_gomp/out/performance-profiling-cpp cpu blocked\n",
    "echo \"########## Done with the profiling\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we filter out the column \"CPU Time:Self\" from the VTune™ result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report hotspots -q -r dnnl-vtune-b -format csv -csv-delimiter ',' -group-by task -column 'CPU Time:Self' | head -n 10 > hotspot_b.csv\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('profile.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 profile.sh;if [ -x \"$(command -v qsub)\" ]; then ./q profile.sh; else ./profile.sh; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Pie Chart to illustrate the CPU time percentage among different primitives\n",
    "We also show the absolute CPU time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('hotspot_b.csv', engine='python')\n",
    "if data.empty is False:\n",
    "    print(data)\n",
    "    if len(data.columns) >= 2:                \n",
    "        data.plot.pie(y=data.columns[1], labels=data.iloc[:,0], figsize=(8, 8), fontsize=20, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Microarchitecture Profiling Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2.1 Vectorization over oneDNN primitives\n",
    "Second, we want to know how well each primitive is vectorized, and microarchitecture profiling type provides those data. \n",
    "We will profile the sample by using profile.sh and then analyze how well it is vectorized by using analyze.sh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile profile.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the profiling\"\n",
    "vtune -collect uarch-exploration -knob sampling-interval=1 -data-limit=2000 -q -no-summary -r dnnl-vtune-ue-b ./cpu_gomp/out/performance-profiling-cpp cpu blocked\n",
    "echo \"########## Done with the profiling\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we filter out the column \"FP Arithmetic\" from the VTune result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report hotspots -q -r dnnl-vtune-ue-b -format csv -csv-delimiter ',' -group-by task -column 'FP Arithmetic' | head -n 10 > fp_b.csv\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('profile.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 profile.sh;if [ -x \"$(command -v qsub)\" ]; then ./q profile.sh; else ./profile.sh; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Bar Chart to illustrate the Scalar and Vector ops percentage among different primitives\n",
    "For FP Arithmetic column, there are more sub columns under it.  \n",
    "We just pick column 4 for FP vector information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('fp_b.csv', engine='python')\n",
    "if data.empty is False:\n",
    "    if len(data.columns) >= 5:            \n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[4], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > NOTE : users should be able to see 100% FP Vector ratio for convolution primitive.  \n",
    "It is optimal because we switch data format from naive to blocked, so data format indeed helps on vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2.2 Memory Bound over oneDNN primitives\n",
    "\n",
    "Third, we want to know any memory problem for those primitives, and microarchitecture profiling type provides those data. \n",
    "We will profile the sample by using profile.sh and then analyze memory bound issues by using analyze.sh. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - analyze.sh\n",
    "the script **analyze.sh** encapsulates the program for submission to the analyzing job queue for execution.  \n",
    "In below section, we filter out the column \"Memory Bound\" from the VTune™ result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile analyze.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the analyzing\"\n",
    "vtune -report hotspots -q -r dnnl-vtune-ue-b -format csv -csv-delimiter ',' -group-by task -column 'Memory Bound' | head -n 10 > memory_b.csv\n",
    "echo \"########## Done with the analyzing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : replace ONEAPI_INSTALL with set value in both profile.sh and analyze.sh.\n",
    "> NOTE : this step is mandatory if you run the notebook on DevCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import FileUtils\n",
    "file_utils = FileUtils()\n",
    "file_utils.replace_string_in_file('analyze.sh','$ONEAPI_INSTALL', os.environ['ONEAPI_INSTALL'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting profile.sh and analyze.sh to the job queue\n",
    "Now we can submit the profile.sh and analyze.sh to the job queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 q;chmod 755 analyze.sh;if [ -x \"$(command -v qsub)\" ]; then ./q analyze.sh; else ./analyze.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Bar Chart to identify any DRAM/L1/L2/L3 bound issues among different primitives\n",
    "For Memory Bound column, there are more sub columns under it.  \n",
    "Users can dump all the sub columns name in below section by unmarking the line of print().  \n",
    "We pick column 1,2,11,12,17 for different memory bound information.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('memory_b.csv', engine='python')\n",
    "if data.empty is False:\n",
    "    if len(data.columns) >= 18:            \n",
    "        #print(data.columns)\n",
    "\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[1], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[2], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[11], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[12], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);\n",
    "        data.plot.bar(x=data.columns[0], y=data.columns[17], rot=0, title=\"\", ylim=(0,100), figsize=(8,5),fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab the developer learned the following:\n",
    "\n",
    "* Use VTune™ Profiler to profile oneDNN samples with different profiling types.\n",
    "* Use oneDNN ITT Tagging feature to profile oneDNN samples on each primitive defined as a Task in a VTune™ result.\n",
    "* Identify performance bottlenecks by VTune™ profiling on primitive level.\n",
    "* Understand the performance impact among different data formats for oneDNN workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
