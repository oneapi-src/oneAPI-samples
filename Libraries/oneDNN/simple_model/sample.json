{
 "guid": "389BBED3-456D-4092-B6D8-DDF782865D66",
 "name": "oneDNN CNN FP32 Inference",
 "categories": ["Toolkit/IntelÂ® oneAPI Base Toolkit/oneDNN"],
 "description": "Run a simple CNN on both Intel CPU and GPU with sample C++ codes.",
 "toolchain": ["dpcpp","gcc","icc"],
 "languages": [{"cpp":{}}],
 "dependencies": ["oneDNN", "tbb","compiler|icc"],
 "os": ["linux"],
 "builder": ["ide","cmake"],
 "targetDevice": ["CPU", "GPU"],
 "ciTests": {
	"linux": [{
		"env": ["source /opt/intel/oneapi/setvars.sh --dnnl-configuration=cpu_dpcpp_gpu_dpcpp --force" ],
		"id": "infer",
		"steps": [
			"mkdir build",
      		        "cd build",
           		"cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp",
           		"make cnn-inference-f32-cpp",
			"./out/cnn-inference-f32-cpp cpu",
			"SYCL_BE=PI_OPENCL ./out/cnn-inference-f32-cpp gpu"
		 ]
	}]

 }
}
