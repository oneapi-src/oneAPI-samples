{
 "guid": "389BBED3-456D-4092-B6D8-DDF782865D66",
 "name": "Simple Model",
 "categories": ["Toolkit/oneAPI Libraries/oneDNN"],
 "description": "Run a simple CNN on both IntelÂ® CPU and GPU with sample C++ codes",
 "toolchain": ["dpcpp","gcc","icc"],
 "languages": [{"cpp":{}}],
 "dependencies": ["oneDNN", "tbb","compiler|icc"],
 "os": ["linux", "windows"],
 "builder": ["cmake"],
 "targetDevice": ["CPU", "GPU"],
 "ciTests": {
	"linux": [{
		"env": ["source /opt/intel/oneapi/setvars.sh --dnnl-configuration=cpu_dpcpp_gpu_dpcpp --force" ],
		"id": "infer",
		"steps": [
			"mkdir build",
      		        "cd build",
           		"cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp",
           		"make cnn-inference-f32-cpp",
			"./bin/cnn-inference-f32-cpp cpu",
			"SYCL_BE=PI_OPENCL ./bin/cnn-inference-f32-cpp gpu"
		 ]
	}],
	"windows": [
	    {
		"id": "infer cpu",
		"steps": [
			"set CC=clang",
			"set CXX=clang++",
			"mkdir build_cpu",
                        "cd build_cpu",
                        "cmake -G Ninja ..",
                        "cmake --build .",
                        "bin\\cnn-inference-f32-cpp.exe"
		 ]
	    },
	    {
		"id": "infer gpu",
		"steps": [
			"set CC=clang",
			"set CXX=clang++",
                        "mkdir build_gpu",
                        "cd build_gpu",
                        "cmake -G Ninja ..",
                        "cmake --build .",
                        "bin\\cnn-inference-f32-cpp.exe gpu"
		 ]
	    }
	]

 }
}
