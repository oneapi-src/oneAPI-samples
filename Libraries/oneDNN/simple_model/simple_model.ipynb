f{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.1 - port a Intel® oneAPI Deep Neural Network Library (oneDNN)  sample from CPU to GPU  - oneDNN CNN FP32 Inference\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn how to port a oneDNN sample from a CPU-only version to a CPU&GPU version by using DPC++\n",
    "* Learn how to program a simple convolutional neural network by using oneDNN\n",
    "* Learn how to collect VTune™ Amplifier data for CPU and GPU runs and compare performance results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise : Porting oneDNN application from CPU to GPU \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : introduce oneDNN configurations inside Intel® oneAPI toolkits\n",
    "oneDNN has four different configurations inside the Intel oneAPI toolkits. Each configuration is in a different folder under the oneDNN installation path, and each configuration supports different compilers or threading libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the installation path of your Intel oneAPI toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n'    $ONEAPI_INSTALL/oneDNN/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 4 different folders under the oneDNN installation path, and each of those configurations supports different features. This tutorial will make use of two configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir lab;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2 : scanning the cnn_inference_f32.cpp code which only supports CPU\n",
    "\n",
    "This C++ API example demonstrates how to build an AlexNet neural network topology for forward-pass inference, and it can run only on CPU.\n",
    "You can find a detailed code explanation at this [link](https://oneapi-src.github.io/oneDNN/cnn_inference_f32_cpp.html)\n",
    "\n",
    "There is a cnn_inference_f32.cpp, which has a CPU-only implementation.\n",
    "Let us copy into the lab folder, and use it as the base of the lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp codes_for_ipynb/cnn_inference_f32.cpp lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user could check the source file using the following command, but we recommened to use the detailed code explanation at this [link](https://oneapi-src.github.io/oneDNN/cnn_inference_f32_cpp.html) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lab/cnn_inference_f32.cpp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, copy the required CMake file into the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_gomp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3:   Build and Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run with GNU Compiler and OpenMP \n",
    "For this CPU-only AlexNet neural network topology for forward-pass inference sample, the GNU compiler is used.\n",
    "The following section guides you how to build with G++ and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler  command and flags that will generate the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp  --force > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make cnn-inference-f32-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the Intel DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the G++ oneDNN configuration by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "\n",
    "By default, oneDNN Verbose log is disabled.\n",
    "You can unmark  #export DNNL_VERBOSE=1 to enable oneDNN verbose log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp  --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "# unmark below line to enable oneDNN verbose log\n",
    "#export DNNL_VERBOSE=1\n",
    "./cpu_gomp/out/cnn-inference-f32-cpp\n",
    "echo \"########## Done with the run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!rm -rf cpu_gomp; chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable oneDNN Verbose log and check the engine kind for each operation\n",
    "cpu should be the engine kind for most of the operations, and you should be able to check the engine kind after \"dnnl_verbose,exec,\" for each operation.\n",
    "Check this [link](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) for a detailed explanation of oneDNN verbose log.\n",
    "\n",
    "Below is an example for oneDNN verbose log for convolution on CPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb1_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0,0.458008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze performance with VTune Amplifier\n",
    "Use the VTune™ Amplifier command line to analyze performance and display the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do CPU profiling first. \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh  --dnnl-configuration=cpu_gomp  --force\n",
    "type=hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir $(pwd)/vtune_data $(pwd)/cpu_gomp/out/cnn-inference-f32-cpp\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data -format html -report-output $(pwd)/summary.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display VTune Amplifier summary report generated in HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do GPU profiling \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling type is changed from hotspots to gpu-hotspots in below script to do basic GPU profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh  --dnnl-configuration=cpu_gomp  --force\n",
    "type=gpu-hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir $(pwd)/vtune_data_data $(pwd)/cpu_gomp/out/cnn-inference-f32-cpp\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data_data -format html -report-output $(pwd)/summary_gpu.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display VTune Amplifier summary report generated in HTML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the VTune Amplifier summary page, the GPU is stalled/idle all the time. this sample doesn't utilize GPU at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary_gpu.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5 : Modifying the cnn_inference_f32.cpp code to support both CPU and GPU\n",
    "\n",
    "In this session, we will convert the above cnn_inference_f32.cpp to support both CPU and GPU and compile the sample with DPC++ instead of G++.\n",
    "\n",
    "There are three steps to do the code conversion from CPU to GPU for this sample.\n",
    "\n",
    "* Step 1 : change engine::kind from CPU to GPU\n",
    "* Step 2 : implement a function to access GPU memory via SYCL buffer and its accessor\n",
    "* Step 3 : write user's data into GPU memory via the implemented function from Step 2\n",
    "\n",
    "There is a cnn_inference_f32.patch file inside the src folder. It contains all the changes for porting CPU to GPU against the CPU-only version of cnn_inference_f32.cpp.\n",
    "First we must patch the cnn_inference_f32.cpp under the lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd lab;patch < ../codes_for_ipynb/cnn_inference_f32.patch;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can check the source file using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lab/cnn_inference_f32.cpp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find related modification in below cnn_inference_f32.cpp, and the modifications for each step are wrapped up with \">>>>>>\" and \"<<<<<<\".\n",
    "\n",
    "### step1 : change engine::kind from CPU to GPU\n",
    "changing engine kind from cpu to gpu during engine instantiation.\n",
    "* Before patching : engine eng(engine::kind::cpu, 0);\n",
    "* After patching : engine eng(engine::kind::gpu, 0);\n",
    "\n",
    "### step 2 : implement a function to access GPU memory via SYCL buffer and its accessor\n",
    "You can refer to the below function write_to_dnnl_memory for that.\n",
    "overall, we use SYCL buffer and its accessor to access GPU memory.\n",
    "auto buffer = mem.get_sycl_buffer<uint8_t>();\n",
    "auto dst = buffer.get_access<cl::sycl::access::mode::write>();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+// ------ GPU code conversion --Step 2 >>>>>>\n",
    "+// Read from handle, write to memory\n",
    "+inline void write_to_dnnl_memory(void *handle, dnnl::memory &mem) {\n",
    "+\n",
    "+    dnnl::engine eng = mem.get_engine();\n",
    "+    size_t size = mem.get_desc().get_size();\n",
    "+\n",
    "+    bool is_cpu_sycl = (DNNL_CPU_RUNTIME == DNNL_RUNTIME_SYCL\n",
    "+            && eng.get_kind() == dnnl::engine::kind::cpu);\n",
    "+    bool is_gpu_sycl = (DNNL_GPU_RUNTIME == DNNL_RUNTIME_SYCL\n",
    "+            && eng.get_kind() == dnnl::engine::kind::gpu);\n",
    "+    if (is_cpu_sycl || is_gpu_sycl) {\n",
    "+\n",
    "+        auto buffer = mem.get_sycl_buffer<uint8_t>();\n",
    "+        auto dst = buffer.get_access<cl::sycl::access::mode::write>();\n",
    "+        uint8_t *dst_ptr = dst.get_pointer();\n",
    "+\n",
    "+        if (!dst_ptr || !handle) {\n",
    "+            std::cerr << \"memory is NULL\"\n",
    "+                      << \"\\n\";\n",
    "+            return;\n",
    "+        }\n",
    "+        for (size_t i = 0; i < size; ++i)\n",
    "+            dst_ptr[i] = ((uint8_t *)handle)[i];\n",
    "+        return;\n",
    "+    }\n",
    "+\n",
    "+    if (eng.get_kind() == dnnl::engine::kind::cpu) {\n",
    "+        uint8_t *dst = static_cast<uint8_t *>(mem.get_data_handle());\n",
    "+        if (!dst || !handle) {\n",
    "+            std::cerr << \"memory is NULL\"\n",
    "+                      << \"\\n\";\n",
    "+            return;\n",
    "+        }\n",
    "+        for (size_t i = 0; i < size; ++i)\n",
    "+            dst[i] = ((uint8_t *)handle)[i];\n",
    "+        return;\n",
    "+    }\n",
    "+\n",
    "+    assert(!\"not expected\");\n",
    "+}\n",
    "+//<<<<<< ------ GPU code conversion --Step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 3 : write user's data into GPU memory via the implemented function from Step 2\n",
    " For accessing user data in GPU memory, we can't use the host pointer to write data into that, but we use write_to_dnnl_memory function instead. Refer to the code snapshot below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "     auto user_src_memory = memory(\n",
    "-            { { conv1_src_tz }, dt::f32, tag::nchw }, eng, user_src.data());\n",
    "+            { { conv1_src_tz }, dt::f32, tag::nchw }, eng);\n",
    "+    write_to_dnnl_memory(user_src.data(), user_src_memory);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run with oneAPI DPC++ Compiler \n",
    "For this  AlexNet neural network topology for forward-pass inference sample on GPU, DPC++ is used as the compiler.\n",
    "The following section guides you how to build with DPC++ and run on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler  command and flags that will generate the exectuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh  --dnnl-configuration=cpu_dpcpp_gpu_dpcpp --force > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir dpcpp\n",
    "cd dpcpp\n",
    "cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL\n",
    "make cnn-inference-f32-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "\n",
    "By default, oneDNN Verbose log is disabled.\n",
    "You can unmark  #export DNNL_VERBOSE=1 to enable oneDNN verbose log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh  --dnnl-configuration=cpu_dpcpp_gpu_dpcpp --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "#export DNNL_VERBOSE=1\n",
    "./dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "echo \"########## Done with the run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dpcpp; chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable oneDNN Verbose log and check the engine kind for each operation\n",
    "gpu should be the engine kind for most of the operations, and you should be able to check the engine kind after \"dnnl_verbose,exec,\" for each operation.\n",
    "Check this [link](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html) for a detailed explanation of oneDNN verbose log.\n",
    "\n",
    "Below is an example for oneDNN verbose log for convolution on GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dnnl_verbose,exec,gpu,convolution,ocl:gen9:blocked,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb16a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd16b:f0,,alg:convolution_direct,mb1_ic3oc96_ih227oh55kh11sh4dh0ph0_iw227ow55kw11sw4dw0pw0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze performance with VTune Amplifier\n",
    "Use the VTune Amplifier command line to analyze performace and display the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do CPU profiling first. \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_dpcpp_gpu_dpcpp  --force\n",
    "type=hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir vtune_data $(pwd)/dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data -format html -report-output $(pwd)/summary.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display VTune Amplifier summary report generated in HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do GPU profiling \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force\n",
    "type=gpu-hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data_gpu\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir $(pwd)/vtune_data_gpu $(pwd)/dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data_gpu -format html -report-output $(pwd)/summary_gpu.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display VTune Amplifier summary report generated in HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary_gpu.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the supported profiling types from VTune Amplifier.\n",
    "\n",
    "* type=hotspots\n",
    "* type=memory-consumption\n",
    "* type=uarch-exploration\n",
    "* type=memory-access\n",
    "* type=threading\n",
    "* type=hpc-performance\n",
    "* type=system-overview\n",
    "* type=graphics-rendering\n",
    "* type=io\n",
    "* type=fpga-interaction\n",
    "* type=gpu-offload\n",
    "* type=gpu-hotspots\n",
    "* type=throttling\n",
    "* type=platform-profiler\n",
    "* type=cpugpu-concurrency\n",
    "* type=tsx-exploration\n",
    "* type=tsx-hotspots\n",
    "* type=sgx-hotspots\n",
    "\n",
    "For details of VTune Amplifier usage, refer to https://software.intel.com/en-us/oneapi/vtune-profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Offload Analysis with Advisor\n",
    "Use Advisor command line to do offload analysis and display the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advisor Command-Line for collecting and reporting \"offload\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile advisor_offload.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force\n",
    "rm -rf advisor_offload\n",
    "advixe-python $APM/collect.py advisor_offload --config gen9 -- $(pwd)/dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "advixe-python $APM/analyze.py advisor_offload --config gen9 --out-dir ./advisor_offload/report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 advisor_offload.sh; if [ -x \"$(command -v qsub)\" ]; then ./q advisor_offload.sh; else ./advisor_offload.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Advisor \"offload\" report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./advisor_offload/report/report.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Advisor Roofline Analysis\n",
    "This sections shows how to collect and generate a roofline report using Intel Advisor. Below is an Advisor-generated \"roofline\" report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advisor Command-Line for collecting and reporting \"roofline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile advisor_roofline.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force\n",
    "export ADVIXE_EXPERIMENTAL=gpu-profiling\n",
    "advixe-cl –collect=survey --enable-gpu-profiling --project-dir=./advisor_roofline --search-dir src:r=. -- $(pwd)/dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "advixe-cl –collect=tripcounts --stacks --flop --enable-gpu-profiling --project-dir=./advisor_roofline --search-dir src:r=. -- $(pwd)/dpcpp/out/cnn-inference-f32-cpp gpu\n",
    "advixe-cl --report=roofline --gpu --project-dir=./advisor_roofline --report-output=./advisor_roofline/roofline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 advisor_roofline.sh; if [ -x \"$(command -v qsub)\" ]; then ./q advisor_roofline.sh; else ./advisor_roofline.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Advisor \"roofline\" report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./advisor_roofline/roofline.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab, the developer learned the following:\n",
    "* How to port a oneDNN sample from CPU-only version to CPU&GPU version\n",
    "* How to program a simple convolutional neural network by using oneDNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
