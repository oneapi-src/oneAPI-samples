{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.1 - port an Intel® oneAPI Collective Communications Library (oneCCL) sample from CPU to GPU   -  CCL Allreduce  \n",
    "\n",
    "## Learning Objectives\n",
    "In this module, the developer will:\n",
    "* Learn different oneCCL configurations inside the Intel® oneAPI toolkit\n",
    "* Learn how to compile a oneCCL sample with different configurations via batch jobs on the Intel® DevCloud for oneAPI or in local environments\n",
    "* Learn how to program oneCCL with a simple sample\n",
    "* Learn how to port a oneCCL sample from CPU-only version to CPU&GPU version by using DPC++\n",
    "* Learn how to collect VTune™ Amplifier data for CPU and GPU runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# CCL Allreduce CPU to GPU porting Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : introduce oneCCL configurations inside oneAPI toolkits\n",
    "oneCCL has two different configurations inside the oneAPI toolkits. Both lib and include folders under the oneCCL installation path contain two different configurations, and each configuration supports a different compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the installation path of your oneAPI toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ONEAPI_INSTALL=/opt/intel/inteloneapi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!printf '%s\\n'    $ONEAPI_INSTALL/ccl/latest/lib/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two different folders under the oneCCL installation path, and each of those configurations supports different features. \n",
    "This tutorial will guide you on how to compile and run against different oneCCL configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a lab folder for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2 : Editing the cpu_allreduce_cpp_test.cpp code which only supports CPU\n",
    "\n",
    "This C++ API example demonstrates how to build a global reduction operation by using the sum function, and it can run only on CPU.\n",
    "You can find a detailed allreduce API explanation at this [link](https://intel.github.io/oneccl/spec/communication_primitives.html#allreduce)\n",
    "\n",
    "\n",
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "The first line of the cell contains the command **%%writefile ' lab/cpu_allreduce_cpp_test.cpp'** This tells the input cell to save the contents of the cell into the file name ' cpu_allreduce_cpp_test.cpp'  As you edit the cell and run it, it will save your changes into that file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/cpu_allreduce_cpp_test.cpp\n",
    "#include <iostream>\n",
    "#include <stdio.h>\n",
    "#include \"ccl.hpp\"\n",
    "\n",
    "#define COUNT 128\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    int i = 0;\n",
    "    int size = 0;\n",
    "    int rank = 0;\n",
    "\n",
    "    auto sendbuf = new int[COUNT];\n",
    "    auto recvbuf = new int[COUNT];\n",
    "\n",
    "    auto comm = ccl::environment::instance().create_communicator();\n",
    "    auto stream = ccl::environment::instance().create_stream();\n",
    "\n",
    "    rank = comm->rank();\n",
    "    size = comm->size();\n",
    "\n",
    "    /* initialize sendbuf */\n",
    "    for (i = 0; i < COUNT; i++) {\n",
    "        sendbuf[i] = rank;\n",
    "    }\n",
    "\n",
    "    /* modify sendbuf */\n",
    "    for (i = 0; i < COUNT; i++) {\n",
    "        sendbuf[i] += 1;\n",
    "    }\n",
    "\n",
    "    /* invoke ccl_allreduce */\n",
    "    comm->allreduce(sendbuf,\n",
    "                   recvbuf,\n",
    "                   COUNT,\n",
    "                   ccl::reduction::sum,\n",
    "                   nullptr, /* attr */\n",
    "                   stream)->wait();\n",
    "\n",
    "    /* check correctness of recvbuf */\n",
    "    for (i = 0; i < COUNT; i++) {\n",
    "        if (recvbuf[i] != size * (size + 1) / 2) {\n",
    "           recvbuf[i] = -1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* print out the result of the test */\n",
    "    if (rank == 0) {\n",
    "        for (i = 0; i < COUNT; i++) {\n",
    "            if (recvbuf[i] == -1) {\n",
    "                cout << \"FAILED\" << endl;\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "        if (i == COUNT) {\n",
    "            cout << \"PASSED\" << endl;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, copy the required CMake file into lab folder. The top half of CMakeList.txt handles CPU-only samples, and the bottom half handles DPC++ samples with CPU and GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/CMakeLists.txt\n",
    "#cmake_minimum_required (VERSION 2.8)\n",
    "#project(CCL_SAMPLES)\n",
    "set(CCL_TEST_INCLUDE_DIR \"$ENV{PWD}/../include\")\n",
    "set(CMAKE_INSTALL_PREFIX \"$ENV{PWD}/_install\")\n",
    "if(${CMAKE_CXX_COMPILER_ID} STREQUAL \"GNU\")\n",
    "    file(GLOB sources \"cpu_*.c\" \"cpu_*.cpp\")\n",
    "    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CLANG_FLAGS} -std=c++11\")\n",
    "    set(CCL_INCLUDE_DIR \"$ENV{CCL_ROOT}/include/cpu_icc\")\n",
    "    set(CCL_LIB_DIR \"$ENV{CCL_ROOT}/lib/cpu_icc\")\n",
    "    foreach(src ${sources})\n",
    "        include_directories(${CCL_INCLUDE_DIR})\n",
    "        include_directories(${CCL_TEST_INCLUDE_DIR})\n",
    "        link_directories(${CCL_LIB_DIR})\n",
    "        get_filename_component(executable ${src} NAME_WE)\n",
    "        add_executable(${executable} ${src})\n",
    "        target_link_libraries(${executable} PUBLIC rt)\n",
    "        target_link_libraries(${executable} PUBLIC m)\n",
    "        target_link_libraries(${executable} PRIVATE ccl)\n",
    "        target_link_libraries(${executable} PUBLIC pthread dl stdc++)\n",
    "        install(TARGETS ${executable} RUNTIME DESTINATION \"${CMAKE_INSTALL_PREFIX}\")\n",
    "    endforeach()\n",
    "endif()\n",
    "\n",
    "if(${CMAKE_CXX_COMPILER_ID} STREQUAL \"Clang\")\n",
    "    set(CCL_INCLUDE_DIRS \"${CCL_INCLUDE_DIRS} $ENV{SYCL_BUNDLE_ROOT}/include\")\n",
    "    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsycl -std=c++11\")\n",
    "    file(GLOB sources \"sycl_*.c\" \"sycl_*.cpp\")\n",
    "    set(CCL_INCLUDE_DIR \"$ENV{CCL_ROOT}/include/cpu_gpu_dpcpp\")\n",
    "    set(CCL_LIB_DIR \"$ENV{CCL_ROOT}/lib/cpu_gpu_dpcpp\")\n",
    "    foreach(src ${sources})\n",
    "        include_directories(${CCL_INCLUDE_DIR})\n",
    "        include_directories(${CCL_TEST_INCLUDE_DIR})\n",
    "        link_directories(${CCL_LIB_DIR})\n",
    "        get_filename_component(executable ${src} NAME_WE)\n",
    "        add_executable(${executable} ${src})\n",
    "        target_link_libraries(${executable} PUBLIC rt)\n",
    "        target_link_libraries(${executable} PUBLIC m)\n",
    "        target_link_libraries(${executable} PRIVATE ccl)\n",
    "        target_link_libraries(${executable} PRIVATE OpenCL)\n",
    "        target_link_libraries(${executable} PRIVATE sycl)\n",
    "        install(TARGETS ${executable} RUNTIME DESTINATION \"${CMAKE_INSTALL_PREFIX}\")\n",
    "    endforeach()\n",
    "endif()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3:  Build and Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run with GNU Compiler and OpenMP\n",
    "The global reduction operations by using sum function sample uses the GNU compiler for this CPU. The following section guides you on how to build with G++ and run on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler  command and flags that will generate the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_icc --force > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++\n",
    "make cpu_allreduce_cpp_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the Intel DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the g++ oneCCL configuration by inputting a custom configuration \"--ccl-configuration=cpu_icc\" when running \"source setvars.sh\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_icc --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_gomp/out/cpu_allreduce_cpp_test\n",
    "echo \"########## Done with the run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!rm -rf cpu_gomp; chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze performance with VTune Amplifier\n",
    "Use the VTune Amplifier command line to analyze performance and display the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do CPU profiling first\n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_icc --force \n",
    "type=hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir $(pwd)/vtune_data $(pwd)/cpu_gomp/out/cpu_allreduce_cpp_test\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data -format html -report-output $(pwd)/summary.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DisplayVTune Amplifier Summary\n",
    "Display VTune Amplifier summary report generated in HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do GPU profiling \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling type is changed from hotspots to gpu-hotspots in below script to do basic GPU profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_icc --force \n",
    "type=gpu-hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir $(pwd)/vtune_data $(pwd)/cpu_gomp/out/cpu_allreduce_cpp_test\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data -format html -report-output $(pwd)/summary-gpu.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display the VTune Amplifier summary report generated in HTML format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the VTune Amplifier summary page, the GPU is stalled/idle all the time. This sample does not utilize GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary-gpu.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5 : Modifying the cpu_allreduce_cpp_test.cpp code which supports both CPU and GPU\n",
    "\n",
    "In this session, we will convert the above sycl_allreduce_cpp_test.cpp to support both CPU and GPU and compile the sample with DPC++ instead of g++.\n",
    "\n",
    "There are several steps to complete the code conversion from CPU to GPU for this sample.\n",
    "\n",
    "* Step 0 : Define inline functions to create sycl queue with the selected selector\n",
    "* Step 1 : Declare the sycl queue and sycl buffers\n",
    "* Step 2 : Use the inline functions in Step 0 to create the sycl queue\n",
    "* Step 3 : Access sycl buffer via its accessor on both the host and target side \n",
    "* Step 3.1 : Initialize sycl buffer and its acccessor on the host side\n",
    "* Step 3.2 : Modify sycl buffer via its accessor on the target device side \n",
    "* Step 3.3 : Check sycl buffer's correctness on the target device side \n",
    "* Step 3.4 : Check sycl buffer's correctness on the host side\n",
    "\n",
    "You can find related modifications below in sycl_allreduce_cpp_test.cpp, and the modifications for each step are wrapped up with \">>>>>>\" and \"<<<<<<\".\n",
    "\n",
    "**_NOTE:_** Host Accessors: The constructor for a host accessor waits for all kernels that modify the same buffer (or\n",
    "image) in any queues to complete and then copies data back to host memory before the constructor returns.\n",
    "Any command groups with requirements to the same memory object cannot execute until the host accessor\n",
    "is destroyed. **Therefore, we must have { } for Step 3.1**\n",
    "\n",
    "There are two files in this DPC++ allreduce sample:\n",
    "* sycl_base.hpp\n",
    "* sycl_allreduce_cpp_test.cpp\n",
    "\n",
    "sycl_base.hpp contains inline functions to create sycl queue with the selected selector, and main program is in sycl_allreduce_cpp_test.cpp.\n",
    "\n",
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "The first line of the cell contains the command **%%writefile ' lab/sycl_base.hpp' '** This tells the input cell to save the contents of the cell into the file name '  lsycl_base.hpp'  As you edit the cell and run it, it will save your changes into that file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lab/sycl_base.hpp\n",
    "header file for inline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sycl_base.hpp\n",
    "#include <iostream>\n",
    "#include <stdio.h>\n",
    "\n",
    "// ------ GPU code conversion --Step 0 >>>>>>\n",
    "// Define inline functions to create sycl queue with the selected selector\n",
    "#include <CL/sycl.hpp>\n",
    "#include \"ccl.hpp\"\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "using namespace cl::sycl;\n",
    "using namespace cl::sycl::access;\n",
    "\n",
    "inline bool has_gpu()\n",
    "{\n",
    "    std::vector<cl::sycl::device> devices = cl::sycl::device::get_devices();\n",
    "    for (const auto& device : devices)\n",
    "    {\n",
    "        if (device.is_gpu())\n",
    "        {\n",
    "            return true;\n",
    "        }\n",
    "    }\n",
    "    return false;\n",
    "}\n",
    "\n",
    "inline int create_sycl_queue(int argc, char **argv, cl::sycl::queue &queue)\n",
    "{\n",
    "    unique_ptr<cl::sycl::device_selector> selector;\n",
    "    if (argc == 2)\n",
    "    {\n",
    "        if (strcmp(argv[1], \"cpu\") == 0)\n",
    "        {\n",
    "            selector.reset(new cl::sycl::cpu_selector());\n",
    "        }\n",
    "        else if (strcmp(argv[1], \"gpu\") == 0)\n",
    "        {\n",
    "            if (has_gpu()) \n",
    "            {\n",
    "                selector.reset(new cl::sycl::gpu_selector());\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                selector.reset(new cl::sycl::default_selector());\n",
    "                cout << \"GPU is unavailable, default_selector has been created instead of gpu_selector.\" << std::endl;\n",
    "            }\n",
    "        }\n",
    "        else if (strcmp(argv[1], \"host\") == 0)\n",
    "        {\n",
    "            selector.reset(new cl::sycl::host_selector());\n",
    "        }\n",
    "        else if (strcmp(argv[1], \"default\") == 0)\n",
    "        {\n",
    "            selector.reset(new cl::sycl::host_selector());\n",
    "               cout << \"Accelerator is unavailable for multiprocessing, host_selector has been created instead of default_selector.\" << std::endl;\n",
    "         }\n",
    "        else\n",
    "        {\n",
    "            cerr << \"Please provide device type: cpu | gpu | host | default \" << std::endl;\n",
    "            return -1;\n",
    "        }\n",
    "        queue = cl::sycl::queue(*selector);\n",
    "        cout << \"Provided device type \" << argv[1] << \"\\nRunning on \"\n",
    "                  << queue.get_device().get_info<cl::sycl::info::device::name>()\n",
    "                  << \"\\n\";\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        cerr << \"Please provide device type: cpu | gpu | host | default \" << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "                \n",
    "//<<<<<< ------ GPU code conversion --Step 0     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lab/sycl_allreduce_cpp_test.cpp\n",
    "Implementation of SYCL allreduce functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "The first line of the cell contains the command **%%writefile ' lab/sycl_allreduce_cpp_test.cpp' '** This tells the input cell to save the contents of the cell into the file name ' sycl_allreduce_cpp_test.cpp'  As you edit the cell and run it, it will save your changes into that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/sycl_allreduce_cpp_test.cpp\n",
    "// ------ GPU code conversion --Step 0 >>>>>>\n",
    "#include \"sycl_base.hpp\"\n",
    "//<<<<<< ------ GPU code conversion --Step 0     \n",
    "#define COUNT 128\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    int i = 0;\n",
    "    int size = 0;\n",
    "    int rank = 0;\n",
    "\n",
    "    // ------ GPU code conversion --Step 1 >>>>>>\n",
    "    // Declare the sycl queue and sycl buffers\n",
    "    cl::sycl::queue q;\n",
    "    cl::sycl::buffer<int, 1> sendbuf(COUNT);\n",
    "    cl::sycl::buffer<int, 1> recvbuf(COUNT);\n",
    "    //<<<<<< ------ GPU code conversion --Step 1    \n",
    "    \n",
    "    // ------ GPU code conversion --Step 2 >>>>>>\n",
    "    // Use inline functions in Step 0 to create the sycl queue\n",
    "    if (create_sycl_queue(argc, argv, q) != 0) {\n",
    "        return -1;\n",
    "    }\n",
    "    //<<<<<< ------ GPU code conversion --Step 2\n",
    "    \n",
    "    auto comm = ccl::environment::instance().create_communicator();\n",
    "    auto stream = ccl::environment::instance().create_stream();\n",
    "\n",
    "    rank = comm->rank();\n",
    "    size = comm->size();\n",
    "\n",
    "    /* initialize sendbuf and recvbuf*/\n",
    "    // ------ GPU code conversion --Step 3.1 >>>>>>\n",
    "    {\n",
    "        //  open buffers and initialize them on the CPU side \n",
    "        auto host_acc_sbuf = sendbuf.get_access<mode::write>();\n",
    "        auto host_acc_rbuf = recvbuf.get_access<mode::write>();\n",
    "        for (i = 0; i < COUNT; i++) {\n",
    "            host_acc_sbuf[i] = rank;\n",
    "            host_acc_rbuf[i] = -1;\n",
    "        }\n",
    "    }\n",
    "    //<<<<<< ------ GPU code conversion --Step 3.1\n",
    "\n",
    "    /* modify sendbuf */\n",
    "    // ------ GPU code conversion --Step 3.2 >>>>>>\n",
    "    // open sendbuf and modify it on the target device side \n",
    "    q.submit([&](handler& cgh){\n",
    "       auto dev_acc_sbuf = sendbuf.get_access<mode::write>(cgh);\n",
    "       cgh.parallel_for<class allreduce_test_sbuf_modify>(range<1>{COUNT}, [=](item<1> id) {\n",
    "           dev_acc_sbuf[id] += 1;\n",
    "       });\n",
    "    });\n",
    "    //<<<<<< ------ GPU code conversion --Step 3.2\n",
    "    \n",
    "    /* invoke ccl_allreduce */\n",
    "    comm->allreduce(sendbuf,\n",
    "                   recvbuf,\n",
    "                   COUNT,\n",
    "                   ccl::reduction::sum,\n",
    "                   nullptr, /* attr */\n",
    "                   stream)->wait();\n",
    "\n",
    "    \n",
    "    \n",
    "    /* check correctness of recvbuf */\n",
    "    // ------ GPU code conversion --Step 3.3 >>>>>>\n",
    "    // open recvbuf and check its correctness on the target device side \n",
    "    q.submit([&](handler& cgh){\n",
    "       auto dev_acc_rbuf = recvbuf.get_access<mode::write>(cgh);\n",
    "       cgh.parallel_for<class allreduce_test_rbuf_check>(range<1>{COUNT}, [=](item<1> id) {\n",
    "           if (dev_acc_rbuf[id] != size*(size+1)/2) {\n",
    "               dev_acc_rbuf[id] = -1;\n",
    "           }\n",
    "       });\n",
    "    });\n",
    "    //<<<<<< ------ GPU code conversion --Step 3.3\n",
    "    \n",
    "    /* print out the result of the test */\n",
    "    if (rank == 0) {\n",
    "        // ------ GPU code conversion --Step 3.4 >>>>>>\n",
    "        // open buffers and validate them on the CPU side \n",
    "        auto host_acc_rbuf_new = recvbuf.get_access<mode::read>();\n",
    "        for (i = 0; i < COUNT; i++) {\n",
    "            if (host_acc_rbuf_new[i] == -1) {\n",
    "        //<<<<<< ------ GPU code conversion --Step 3.4\n",
    "                cout << \"FAILED\" << std::endl;\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "        if (i == COUNT) {\n",
    "            cout << \"PASSED\" << std::endl;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run with the DPC++ Compiler\n",
    "For this global reduction operation sample on GPU and CPU, DPC++ is used as the compiler.\n",
    "The following section guides you how to build with DPC++ and run on GPU and CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler  command and flags that will generate the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_gpu_dpcpp --force > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir dpcpp\n",
    "cd dpcpp\n",
    "cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp\n",
    "make sycl_allreduce_cpp_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, execute your program on the DevCloud or in local environments.\n",
    "\n",
    "#### Script - run.sh\n",
    "The script **run.sh** encapsulates the program for submission to the job queue for execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_gpu_dpcpp --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./dpcpp/out/sycl_allreduce_cpp_test gpu\n",
    "echo \"########## Done with the run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts both on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dpcpp; chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze performance with VTune Amplifier\n",
    "Use the VTune Amplifier command line to analyze performace and display the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do CPU profiling first. \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_gpu_dpcpp --force\n",
    "type=hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir vtune_data $(pwd)/dpcpp/out/sycl_allreduce_cpp_test cpu\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data -format html -report-output $(pwd)/summary.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display the VTune Amplifier summary report generated in HTML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do GPU profiling \n",
    "The script vtune_collect.sh encapsulates the profiling command and flags that will generate the VTune Amplifier profiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vtune_collect.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --ccl-configuration=cpu_gpu_dpcpp --force\n",
    "type=gpu-hotspots\n",
    "\n",
    "rm -r $(pwd)/vtune_data\n",
    "\n",
    "echo \"VTune Collect $type\"\n",
    "vtune -collect $type -result-dir $(pwd)/vtune_data $(pwd)/dpcpp/out/sycl_allreduce_cpp_test gpu\n",
    "\n",
    "\n",
    "echo \"VTune Summary Report\"\n",
    "vtune -report summary -result-dir $(pwd)/vtune_data -format html -report-output $(pwd)/summary-gpu.html\n",
    "echo \"Done profiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run VTune Amplifier to Collect Hotspots and Generate Report\n",
    "Collect VTune Amplifier data and generate report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 vtune_collect.sh; if [ -x \"$(command -v qsub)\" ]; then ./q vtune_collect.sh; else ./vtune_collect.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VTune Amplifier Summary\n",
    "Display the VTune Amplifier summary report generated in HTML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='summary-gpu.html', width=960, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the supported profiling types from VTune Amplifier.\n",
    "\n",
    "* type=hotspots\n",
    "* type=memory-consumption\n",
    "* type=uarch-exploration\n",
    "* type=memory-access\n",
    "* type=threading\n",
    "* type=hpc-performance\n",
    "* type=system-overview\n",
    "* type=graphics-rendering\n",
    "* type=io\n",
    "* type=fpga-interaction\n",
    "* type=gpu-offload\n",
    "* type=gpu-hotspots\n",
    "* type=throttling\n",
    "* type=platform-profiler\n",
    "* type=cpugpu-concurrency\n",
    "* type=tsx-exploration\n",
    "* type=tsx-hotspots\n",
    "* type=sgx-hotspots\n",
    "\n",
    "For details of VTune Amplifier usage, please refer to https://software.intel.com/en-us/oneapi/vtune-profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab the developer will have learned the following:\n",
    "* Know different oneCCL configurations inside oneAPI toolkit\n",
    "* Know how to compile a oneCCL sample with different configurations via batch jobs on the Intel oneAPI DevCloud or in local environments\n",
    "* Know how to program oneCCL with a simple sample\n",
    "* Know how to port a oneCCL sample from CPU-only version to CPU&GPU version by using DPC++\n",
    "* Know how to collect VTune Amplifier data for CPU and GPU runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
