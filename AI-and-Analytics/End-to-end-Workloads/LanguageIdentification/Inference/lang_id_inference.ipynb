{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Run Options   \n",
    "inference_custom.py is for running inference on user-specified data, whereas inference_commonVoice.py will run on the testing dataset from \n",
    "the CommonVoice dataset. Both scripts use the following input arguments. Some options are only supported on inference_custom.py:   \n",
    "-p : datapath  \n",
    "-d : duration of the wave sample, default is 3  \n",
    "-s : size of sample waves, default is 100   \n",
    "--vad : enable VAD model to detect active speech (inference_custom.py only)  \n",
    "--ipex : run inference with optimizations from Intel® Extension for PyTorch (IPEX)  \n",
    "--bf16 : run inference with auto-mixed precision featuring Bfloat16  \n",
    "--int8_model : Run inference with the INT8 model generated from Intel® Neural Compressor (INC)  \n",
    "--ground_truth_compare : enable comparison of prediction labels to ground truth values (inference_custom.py only)  \n",
    "--verbose : prints additional debug information, such as latency  \n",
    "\n",
    "The VAD option will identify speech segments in the audio file and construct a new WAV file containing only the speech segments. This improves the quality of speech data used as input into the language identification model.  \n",
    "\n",
    "The Intel® Extension for PyTorch (IPEX) option will apply optimizations to the pretrained model. There should be performance improvements in terms of latency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference_commonVoice.py for CommonVoice Test Data\n",
    "This will run inference on the trained model to see how well it performs on the Common Voice test data generated from the preprocessing scripts.  \n",
    "\n",
    "python inference_commonVoice.py -p DATAPATH  \n",
    "\n",
    "An output file test_data_accuracy.csv will give the summary of the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_commonVoice.py -p ${COMMON_VOICE_PATH}/processed_data/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference_custom.py for Custom Data  \n",
    "To run inference on custom data, you must specify a folder with .wav files and pass the path in as an argument. You can do so by creating a folder named `data_custom` and then copy 1 or 2 .wav files from your test dataset into it. .mp3 files will NOT work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select audio clips from audio files for prediction\n",
    "python inference_custom.py -p DATAPATH -d DURATION -s SIZE\n",
    "\n",
    "An output file `output_summary.csv` will give the summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 50 3sec samples from each WAV file under the folder \"data_custom\"\n",
    "!python inference_custom.py -p data_custom -d 3 -s 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select audio clips from audio files after applying Voice Activity Detection (VAD)  \n",
    "python inference_custom.py -p DATAPATH -d DURATION -s SIZE --vad  \n",
    "\n",
    "An output file output_summary.csv will give the summary of the results. Note that the audio input into the VAD model must be sampled at 16kHz. The code already performs this conversion for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_custom.py -p data_custom -d 3 -s 50 --vad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations with Intel® Extension for PyTorch (IPEX) \n",
    "python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose  \n",
    "\n",
    "This will apply ipex.optimize to the model(s) and TorchScript. You can also add the --bf16 option along with --ipex to run in the BF16 data type, supported on 4th Gen Intel® Xeon® Scalable processors and newer.\n",
    "\n",
    "Note that the *--verbose* option is required to view the latency measurements.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with Intel® Neural Compressor (INC)\n",
    "To improve inference latency, Intel® Neural Compressor (INC) can be used to quantize the trained model from FP32 to INT8 by running quantize_model.py. The *-datapath* argument can be used to specify a custom evaluation dataset but by default it is set to `$COMMON_VOICE_PATH/processed_data/dev` which was generated from the data preprocessing scripts in the `Training` folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python quantize_model.py -p ./lang_id_commonvoice_model -datapath $COMMON_VOICE_PATH/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After quantization, the model will be stored in lang_id_commonvoice_model_INT8 and neural_compressor.utils.pytorch.load will have to be used to load the quantized model for inference. If self.language_id is the original model and data_path is the path to the audio file:\n",
    "\n",
    "```\n",
    "from neural_compressor.utils.pytorch import load\n",
    "model_int8 = load(\"./lang_id_commonvoice_model_INT8\", self.language_id)\n",
    "signal = self.language_id.load_audio(data_path)\n",
    "prediction = self.model_int8(signal)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is integrated into inference_custom.py. You can now run inference on your data using this INT8 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_custom.py -p data_custom -d 3 -s 50 --vad --int8_model --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Comparing Predictions with Ground Truth\n",
    "\n",
    "You can choose to modify audio_ground_truth_labels.csv to include the name of the audio file and expected audio label (like, en for English), then run inference_custom.py with the --ground_truth_compare option. By default, this is disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "If the model appears to be giving the same output regardless of input, try running clean.sh to remove the RIR_NOISES and speechbrain \n",
    "folders so they can be re-pulled.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed6ae0d06e7bec0fef5f1fb38f177ceea45508ce95c68ed2f49461dd6a888a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
