{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Run Options   \n",
    "inference_custom.py is for running inference on user-specified data, whereas inference_commonVoice.py will run on the testing dataset from \n",
    "the CommonVoice dataset. Both scripts use the following input arguments. Some options are only supported on inference_custom.py:   \n",
    "-p : datapath  \n",
    "-d : duration of the wave sample, default is 3  \n",
    "-s : size of sample waves, default is 100   \n",
    "--vad : enable VAD model to detect active speech (inference_custom.py only)  \n",
    "--ipex : run inference with optimizations from Intel Extension for PyTorch  \n",
    "--bf16 : run inference with auto-mixed precision featuring Bfloat16  \n",
    "--int8_model : Run inference with the INT8 model generated from IntelÂ® Neural Compressor  \n",
    "--ground_truth_compare : enable comparison of prediction labels to ground truth values (inference_custom.py only)  \n",
    "--verbose : prints additional debug information, such as latency  \n",
    "\n",
    "The VAD option will identify speech segments in the audio file and construct a new WAV file containing only the speech segments. This improves the quality of speech data used as input into the language identification model.  \n",
    "\n",
    "The IPEX (Intel Extension for PyTorch) option will apply optimizations to the pretrained model. There should be performance improvements in terms of latency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference_commonVoice.py for CommonVoice Test Data\n",
    "This will run inference on the trained model to see how well it performs on the Common Voice test data generated from the preprocessing scripts.  \n",
    "\n",
    "python inference_commonVoice.py -p DATAPATH  \n",
    "\n",
    "An output file test_data_accuracy.csv will give the summary of the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_commonVoice.py -p /data/commonVoice/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference_custom.py for Custom Data  \n",
    "To generate an overall results output summary, the audio_ground_truth_labels.csv file needs to be modified with the name of the audio file and expected audio label (i.e. en for English). By default, this is disabled but if desired, the *--ground_truth_compare* can be used. To run inference on custom data, you must specify a folder with WAV files and pass the path in as an argument.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select audio clips from audio files for prediction\n",
    "python inference_custom.py -p DATAPATH -d DURATION -s SIZE\n",
    "\n",
    "An output file output_summary.csv will give the summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 50 3sec samples from each WAV file under the folder \"data_custom\"\n",
    "!python inference_custom.py -p data_custom -d 3 -s 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select audio clips from audio files after applying Voice Activity Detection (VAD)  \n",
    "python inference_custom.py -p DATAPATH -d DURATION -s SIZE --vad  \n",
    "\n",
    "An output file output_summary.csv will give the summary of the results. Note that the audio input into the VAD model must be sampled at 16kHz. The code already performs this conversion for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_custom.py -p data_custom -d 3 -s 50 --vad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations with Intel Extension for PyTorch (IPEX)  \n",
    "python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose  \n",
    "\n",
    "Note that the *--verbose* option is required to view the latency measurements.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_custom.py -p data_custom -d 3 -s 50 --vad --ipex --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with Neural Compressor\n",
    "To improve inference latency, Neural Compressor can be used to quantize the trained model from FP32 to INT8 by running quantize_model.py. The *-datapath* argument can be used to specify a custom evaluation dataset but by default it is set to */data/commonVoice/dev* which was generated from the data preprocessing scripts in the *Training* folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python quantize_model.py -p ./lang_id_commonvoice_model -datapath $COMMON_VOICE_PATH/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After quantization, the model will be stored in *lang_id_commonvoice_model_INT8* and *neural_compressor.utils.pytorch.load* will have to be used to load the quantized model for inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "If the model appears to be giving the same output regardless of input, try running clean.sh to remove the RIR_NOISES and speechbrain \n",
    "folders so they can be re-pulled.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed6ae0d06e7bec0fef5f1fb38f177ceea45508ce95c68ed2f49461dd6a888a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
