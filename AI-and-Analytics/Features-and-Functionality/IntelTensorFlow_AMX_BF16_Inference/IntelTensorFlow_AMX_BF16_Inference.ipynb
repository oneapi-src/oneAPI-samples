{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81375dd4",
   "metadata": {},
   "source": [
    "# Intel TensorFlow Intel® AMX BF16 Inference\n",
    "\n",
    "This notebook performs the following steps:\n",
    "\n",
    "- Enable auto-mixed precision with few code changes for faster inference.\n",
    "- Image Classification task using [TensorFlow Hub's](https://www.tensorflow.org/hub) ResNet50v1.5 pretrained model.\n",
    "- Export the optimized model in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f93536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import PIL.Image as Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from copy import deepcopy\n",
    "print(\"We are using Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b469092",
   "metadata": {},
   "source": [
    "### Identifying supported ISA\n",
    "\n",
    "We identify the underlying supported ISA to determine whether to enable auto-mixed precision to leverage higher performance benefits for training and inference as accelerated by the 4th Gen Intel® Xeon® scalable processor (codenamed Sapphire Rapids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if hardware supports Intel® AMX\n",
    "\n",
    "from cpuinfo import get_cpu_info\n",
    "info = get_cpu_info()\n",
    "flags = info['flags']\n",
    "amx_supported = False\n",
    "for flag in flags:\n",
    "    if \"amx\" in flag:\n",
    "        amx_supported = True\n",
    "        print(\"Intel® AMX is supported on current hardware. Code sample can be run.\\n\")\n",
    "if not amx_supported:\n",
    "    print(\"Intel® AMX is not supported on current hardware. Code sample cannot be run.\\n\")\n",
    "    sys.exit(\"Intel® AMX is not supported on current hardware. Code sample cannot be run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d69fcb",
   "metadata": {},
   "source": [
    "If the message \"Intel® AMX is not supported on current hardware. Code sample cannot be run.\" is printed above, the hardware being used does not support Intel® AMX. Therefore, this code sample cannot proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ab7c1",
   "metadata": {},
   "source": [
    "### Image Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdc171",
   "metadata": {},
   "source": [
    "In this section, we use [TensorFlow Hub's](https://www.tensorflow.org/hub) pretrained [ResNet50v1.5 pretrained model](https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/5) trained on the ImageNet dataset and fine-tuned on TensorFlow Flower dataset.\n",
    "\n",
    "Source: https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c7aad",
   "metadata": {},
   "source": [
    "Loading the data in a *tf.data.Dataset* format.<br />\n",
    "We use a Batch Size of 512 images each of shape 224 x 224 x 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "data_root = tf.keras.utils.get_file(\n",
    "  'flower_photos',\n",
    "  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "   untar=True)\n",
    "\n",
    "batch_size = 512\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_root),\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_root),\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = np.array(train_ds.class_names)\n",
    "print(\"The flower dataset has \" + str(len(class_names)) + \" classes: \", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e1333",
   "metadata": {},
   "source": [
    "Image Pre-processing (Normalization between 0 and 1) and using buffered prefetching to avoid I/O blocking issues.\n",
    "\n",
    "Reference: https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-trained fp_32 model\n",
    "fp32_model = tf.keras.models.load_model('models/my_saved_model_fp32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e31875",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ee4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "fp32_history = fp32_model.evaluate(val_ds)\n",
    "fp32_inference_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d721553",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in val_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d45604",
   "metadata": {},
   "source": [
    "### Enabling auto-mixed precision with `tf.config` API\n",
    "\n",
    "In this section, we show how to enable the auto-mixed precision using the `tf.config` API. Enabling this API will automatically convert the pre-trained model to use the bfloat16 datatype for computation resulting in an increased inference throughput on the latest Intel® Xeon® scalable processor.\n",
    "\n",
    "\n",
    "_Note: We only enable the auto-mixed precision if the underlying system is the 4th Gen Intel® Xeon® scalable processor (codenamed Sapphire Rapids)_\n",
    "\n",
    "BF16 with AVX512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e121dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model as the bf16 model with AVX512 to compare inference time\n",
    "os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_BF16\"\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision_onednn_bfloat16':True})\n",
    "bf16_model_noAmx = tf.keras.models.load_model('models/my_saved_model_fp32')\n",
    "\n",
    "bf16_model_noAmx_export_path = \"models/my_saved_model_bf16_noAmx\"\n",
    "bf16_model_noAmx.save(bf16_model_noAmx_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd713c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "bf16_noAmx_history = bf16_model_noAmx.evaluate(val_ds)\n",
    "bf16_noAmx_inference_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44811d06",
   "metadata": {},
   "source": [
    "BF16 with Intel® AMX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model as the bf16 model with Intel® AMX to compare inference time\n",
    "os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AMX_BF16\"\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision_onednn_bfloat16':True})\n",
    "bf16_model_withAmx = tf.keras.models.load_model('models/my_saved_model_fp32')\n",
    "\n",
    "bf16_model_withAmx_export_path = \"models/my_saved_model_bf16_with_amx\"\n",
    "bf16_model_withAmx.save(bf16_model_withAmx_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "bf16_withAmx_history = bf16_model_withAmx.evaluate(val_ds)\n",
    "bf16_withAmx_inference_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43352f8",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "The following cells below will summarize the inference time for all three cases and display graphs to show the performance speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67266015",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary\")\n",
    "print(\"FP32 inference time: %.3f\" %fp32_inference_time)\n",
    "print(\"BF16 with AVX512 inference time: %.3f\" %bf16_noAmx_inference_time)\n",
    "print(\"BF16 with Intel® AMX inference time: %.3f\" %bf16_withAmx_inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.title(\"Resnet50 Inference Time\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Inference Time (seconds)\")\n",
    "plt.bar([\"FP32\", \"BF16 with AVX512\", \"BF16 with Intel® AMX\"], [fp32_inference_time, bf16_noAmx_inference_time, bf16_withAmx_inference_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_bf16_noAMX_from_fp32 = fp32_inference_time / bf16_noAmx_inference_time\n",
    "print(\"BF16 with AVX512 is %.2fX faster than FP32\" %speedup_bf16_noAMX_from_fp32)\n",
    "speedup_bf16_withAMX_from_fp32 = fp32_inference_time / bf16_withAmx_inference_time\n",
    "print(\"BF16 with Intel® AMX is %.2fX faster than FP32\" %speedup_bf16_withAMX_from_fp32)\n",
    "speedup_bf16_withAMX_from_bf16 = bf16_noAmx_inference_time / bf16_withAmx_inference_time\n",
    "print(\"BF16 with Intel® AMX is %.2fX faster than BF16 with AVX512\" %speedup_bf16_withAMX_from_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705734a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Intel® AMX Speedup\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.bar([\"FP32\", \"BF16 with AVX512\", \"BF16 with Intel® AMX\"], [1, speedup_bf16_noAMX_from_fp32, speedup_bf16_withAMX_from_fp32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9538",
   "metadata": {},
   "source": [
    "The following cell will summarize the inference accuracy for all three cases and illustrate the use of Intel® AMX BF16 with auto-mixed precision during inference will not influence the inference accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86790e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_inference_accuracy = fp32_history[1]\n",
    "bf16_noAmx_inference_accuracy = bf16_noAmx_history[1]\n",
    "bf16_withAmx_inference_accuracy = bf16_withAmx_history[1]\n",
    "plt.figure()\n",
    "plt.title(\"Resnet50 Inference Accuracy\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Inference Accuracy\")\n",
    "plt.bar([\"FP32\", \"BF16 with AVX512\", \"BF16 with Intel® AMX\"], [fp32_inference_accuracy, bf16_noAmx_inference_accuracy, bf16_withAmx_inference_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[CODE_SAMPLE_COMPLETED_SUCCESFULLY]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
