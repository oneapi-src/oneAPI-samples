{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81375dd4",
   "metadata": {},
   "source": [
    "# Intel TensorFlow AMX BF16 Inference\n",
    "\n",
    "This notebook performs the following steps:\n",
    "\n",
    "- Enable auto-mixed precision with few code changes for faster inference.\n",
    "- Image Classification task using [TensorFlow Hub's](https://www.tensorflow.org/hub) ResNet50v1.5 pretrained model.\n",
    "- Export the optimized model in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f93536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import PIL.Image as Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from copy import deepcopy\n",
    "print(\"We are using Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b469092",
   "metadata": {},
   "source": [
    "### Identifying supported ISA\n",
    "\n",
    "We identify the underlying supported ISA to determine whether to enable auto-mixed precision to leverage higher performance benefits for training and inference as accelerated by the 4th Gen Intel® Xeon® scalable processor (codenamed Sapphire Rapids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if hardware supports AMX\n",
    "\n",
    "from cpuinfo import get_cpu_info\n",
    "info = get_cpu_info()\n",
    "flags = info['flags']\n",
    "amx_supported = False\n",
    "for flag in flags:\n",
    "    if \"amx\" in flag:\n",
    "        amx_supported = True\n",
    "        print(\"AMX is supported on current hardware. Code sample can be run.\\n\")\n",
    "if not amx_supported:\n",
    "    print(\"AMX is not supported on current hardware. Code sample cannot be run.\\n\")\n",
    "    sys.exit(\"AMX is not supported on current hardware. Code sample cannot be run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d69fcb",
   "metadata": {},
   "source": [
    "If the message \"AMX is not supported on current hardware. Code sample cannot be run.\" is printed above, the hardware being used does not support AMX. Therefore, this code sample cannot proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ab7c1",
   "metadata": {},
   "source": [
    "### Image Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdc171",
   "metadata": {},
   "source": [
    "In this section, we use [TensorFlow Hub's](https://www.tensorflow.org/hub) pretrained [ResNet50v1.5 pretrained model](https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/5) trained on the ImageNet dataset and fine-tuned on TensorFlow Flower dataset.\n",
    "\n",
    "Source: https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c7aad",
   "metadata": {},
   "source": [
    "Loading the data in a *tf.data.Dataset* format.<br />\n",
    "We use a Batch Size of 512 images each of shape 224 x 224 x 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "data_root = tf.keras.utils.get_file(\n",
    "  'flower_photos',\n",
    "  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "   untar=True)\n",
    "\n",
    "batch_size = 512\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_root),\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_root),\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = np.array(train_ds.class_names)\n",
    "print(\"The flower dataset has \" + str(len(class_names)) + \" classes: \", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e1333",
   "metadata": {},
   "source": [
    "Image Pre-processing (Normalization between 0 and 1) and using buffered prefetching to avoid I/O blocking issues.\n",
    "\n",
    "Reference: https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-trained fp_32 model\n",
    "fp32_model = tf.keras.models.load_model('models/my_saved_model_fp32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e31875",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ee4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "fp32_history = fp32_model.evaluate(val_ds)\n",
    "fp32_inference_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d721553",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in val_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d45604",
   "metadata": {},
   "source": [
    "### Enabling auto-mixed precision with `tf.config` API\n",
    "\n",
    "In this section, we show how to enable the auto-mixed precision using the `tf.config` API. Enabling this API will automatically convert the pre-trained model to use the bfloat16 datatype for computation resulting in an increased inference throughput on the latest Intel® Xeon® scalable processor.\n",
    "\n",
    "\n",
    "_Note: We only enable the auto-mixed precision if the underlying system is the 4th Gen Intel® Xeon® scalable processor (codenamed Sapphire Rapids)_\n",
    "\n",
    "BF16 with AVX512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e121dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model as the bf16 model with AVX512 to compare inference time\n",
    "os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_BF16\"\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision_onednn_bfloat16':True})\n",
    "bf16_model_noAmx = tf.keras.models.load_model('models/my_saved_model_fp32')\n",
    "\n",
    "bf16_model_noAmx_export_path = \"models/my_saved_model_bf16_noAmx\"\n",
    "bf16_model_noAmx.save(bf16_model_noAmx_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd713c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "bf16_noAmx_history = bf16_model_noAmx.evaluate(val_ds)\n",
    "bf16_noAmx_inference_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44811d06",
   "metadata": {},
   "source": [
    "BF16 with AMX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model as the bf16 model with AMX to compare inference time\n",
    "os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AMX_BF16\"\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision_onednn_bfloat16':True})\n",
    "bf16_model_withAmx = tf.keras.models.load_model('models/my_saved_model_fp32')\n",
    "\n",
    "bf16_model_withAmx_export_path = \"models/my_saved_model_bf16_with_amx\"\n",
    "bf16_model_withAmx.save(bf16_model_withAmx_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "bf16_withAmx_history = bf16_model_withAmx.evaluate(val_ds)\n",
    "bf16_withAmx_inference_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43352f8",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "The following cells below will summarize the inference time for all three cases and display graphs to show the performance speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67266015",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary\")\n",
    "print(\"FP32 inference time: %.3f\" %fp32_inference_time)\n",
    "print(\"BF16 with AVX512 inference time: %.3f\" %bf16_noAmx_inference_time)\n",
    "print(\"BF16 with AMX inference time: %.3f\" %bf16_withAmx_inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.title(\"Resnet50 Inference Time\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Inference Time (seconds)\")\n",
    "plt.bar([\"FP32\", \"BF16 with AVX512\", \"BF16 with AMX\"], [fp32_inference_time, bf16_noAmx_inference_time, bf16_withAmx_inference_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_bf16_noAMX_from_fp32 = fp32_inference_time / bf16_noAmx_inference_time\n",
    "print(\"BF16 with AVX512 is %.2fX faster than FP32\" %speedup_bf16_noAMX_from_fp32)\n",
    "speedup_bf16_withAMX_from_fp32 = fp32_inference_time / bf16_withAmx_inference_time\n",
    "print(\"BF16 with AMX is %.2fX faster than FP32\" %speedup_bf16_withAMX_from_fp32)\n",
    "speedup_bf16_withAMX_from_bf16 = bf16_noAmx_inference_time / bf16_withAmx_inference_time\n",
    "print(\"BF16 with AMX is %.2fX faster than BF16 with AVX512\" %speedup_bf16_withAMX_from_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705734a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"AMX Speedup\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.bar([\"FP32\", \"BF16 with AVX512\", \"BF16 with AMX\"], [1, speedup_bf16_noAMX_from_fp32, speedup_bf16_withAMX_from_fp32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9538",
   "metadata": {},
   "source": [
    "The following cell will summarize the inference accuracy for all three cases and illustrate the use of AMX BF16 with auto-mixed precision during inference will not influence the inference accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86790e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_inference_accuracy = fp32_history[1]\n",
    "bf16_noAmx_inference_accuracy = bf16_noAmx_history[1]\n",
    "bf16_withAmx_inference_accuracy = bf16_withAmx_history[1]\n",
    "plt.figure()\n",
    "plt.title(\"Resnet50 Inference Accuracy\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Inference Accuracy\")\n",
    "plt.bar([\"FP32\", \"BF16 with AVX512\", \"BF16 with AMX\"], [fp32_inference_accuracy, bf16_noAmx_inference_accuracy, bf16_withAmx_inference_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[CODE_SAMPLE_COMPLETED_SUCCESFULLY]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
