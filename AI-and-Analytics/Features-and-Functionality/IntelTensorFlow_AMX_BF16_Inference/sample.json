{
    "guid": "0A887217-5621-4C8D-9418-17558088698B",
    "name": "IntelÂ® Extension for TensorFlow* BF16 Inference",
    "categories": ["Toolkit/oneAPI AI And Analytics/Features and Functionality"],
    "description": "This sample illustrates how to inference a TensorFlow model using Advanced Matrix Extensions Bfloat16",
    "builder": ["cli"],
    "languages": [{"python":{}}],
    "os":["linux"],
    "targetDevice": ["CPU"],
    "cpuInstructionSets": ["AVX512", "AMX"],
    "ciTests": {
      "linux": [
      {
          "env": [
            "source /intel/oneapi/intelpython/bin/activate",
            "conda activate tensorflow",
            "pip install uv",
            "uv python pin $(which python)",
            "uv venv --system-site-packages",
            "uv sync",
            "uv run ipython kernel install --user --name tensorflow"
          ],
          "id": "intel amx bf16 inference",
          "steps": [
              "uv run python Intel_TensorFlow_AMX_BF16_Inference.py",
              "uv run jupyter nbconvert --ExecutePreprocessor.enabled=True --ExecutePreprocessor.kernel_name=tensorflow2 --to notebook IntelTensorFlow_AMX_BF16_Inference.ipynb"
           ]
      }
       ]
    },
    "expertise": "Code Optimization"
  }
  
