{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f9200e-7830-4ee5-8637-e67b5df57eac",
   "metadata": {},
   "source": [
    "# PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb565f-ef03-40cb-9182-5b2b752331e8",
   "metadata": {},
   "source": [
    "This code sample will train a ResNet50 model using the CIFAR10 dataset while using Intel's Extension for PyTorch (IPEX). The model will be trained using FP32 and BF16 precision, including the use of Intel's Advanced Matrix Extensions (AMX) on BF16. AMX is supported on BF16 and INT8 data types starting with the 4th Generation of Xeon Scalable Processors, Sapphire Rapids. The training time will be compared, showcasing the speedup of AMX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a0285-162b-4435-96cb-1d9ab2bbfe8a",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751acd58-3cc0-42c3-8f59-17136a1518b7",
   "metadata": {},
   "source": [
    "Ensure the PyTorch kernel is activated before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41ce52-c94c-4bdf-a528-0e0200fd5501",
   "metadata": {},
   "source": [
    "## Imports, Dataset, Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4eedf0-5c7c-49d3-be15-f46b4988d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17246f67-0059-4b5f-afe8-a105d767b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and constants\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "DOWNLOAD = True\n",
    "DATA = 'datasets/cifar10/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771f165",
   "metadata": {},
   "source": [
    "## Identify Supported ISA  \n",
    "We identify the underlying supported ISA to determine whether AMX is supported. The 4th Gen Intel® Xeon® Scalable Processor (codenamed Sapphire Rapids) or newer must be used to run this sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c339a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if hardware supports AMX\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import version_check\n",
    "from cpuinfo import get_cpu_info\n",
    "info = get_cpu_info()\n",
    "flags = info['flags']\n",
    "amx_supported = False\n",
    "for flag in flags:\n",
    "    if \"amx\" in flag:\n",
    "        amx_supported = True\n",
    "        break\n",
    "if not amx_supported:\n",
    "    print(\"AMX is not supported on current hardware. Code sample cannot be run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f461d",
   "metadata": {},
   "source": [
    "If the message \"AMX is not supported on current hardware. Code sample cannot be run.\" is printed above, the hardware being used does not support AMX. Therefore, this code sample cannot proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd66ee-aac5-4a60-8f66-417612d4d3af",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "The function below will train the Resnet50 model based on the whether AMX should be enabled, and whether to use FP32 or BF16 data type. The environment variable `ONEDNN_MAX_CPU_ISA` is used to enable or disable AMX. For more information, refer to the [oneDNN documentation on CPU Dispatcher Control](https://oneapi-src.github.io/oneDNN/dev_guide_cpu_dispatcher_control.html). To use BF16 in operations, use the `torch.cpu.amp.autocast()` function to perform forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e21c9-aaa5-4f75-b00a-0d875cc0bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to run a test case\n",
    "\"\"\"\n",
    "def trainModel(train_loader, modelName=\"myModel\", amx=True, dataType=\"fp32\"):\n",
    "    \"\"\"\n",
    "    Input parameters\n",
    "        train_loader: a torch DataLoader object containing the training data\n",
    "        modelName: a string representing the name of the model\n",
    "        amx: set to False to disable AMX on BF16, default True otherwise\n",
    "        dataType: the data type for model parameters, supported values - fp32, bf16\n",
    "    Return value\n",
    "        training_time: the time in seconds it takes to train the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure environment variable\n",
    "    if not amx and \"bf16\" == dataType:\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_CORE_BF16\"\n",
    "    else:\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"DEFAULT\"\n",
    "\n",
    "    # Initialize the model \n",
    "    model = torchvision.models.resnet50()\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    model.train()\n",
    "    \n",
    "    # Optimize with BF16 or FP32 (default)\n",
    "    if \"bf16\" == dataType:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    num_batches = len(train_loader)\n",
    "    start_time = time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if \"bf16\" == dataType:\n",
    "            with torch.cpu.amp.autocast():   # Auto Mixed Precision\n",
    "                # Setting memory_format to torch.channels_last could improve performance with 4D input data. This is optional.\n",
    "                data = data.to(memory_format=torch.channels_last)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "        else:\n",
    "            # Setting memory_format to torch.channels_last could improve performance with 4D input data. This is optional.\n",
    "            data = data.to(memory_format=torch.channels_last)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if 0 == (batch_idx+1) % 50:\n",
    "            print(\"Batch %d/%d complete\" %(batch_idx+1, num_batches))\n",
    "    end_time = time()\n",
    "    training_time = end_time-start_time\n",
    "    print(\"Training took %.3f seconds\" %(training_time))\n",
    "    \n",
    "    # Save a checkpoint of the trained model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, 'checkpoint_%s.pth' %modelName)\n",
    "        \n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab47ca1-d8fd-4e69-bca6-cb29dba6596b",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The CIFAR10 dataset is used for this sample. Batch size will be set to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d9563-e40e-48b4-a311-a71a762b5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "torchvision.transforms.Resize((224, 224)),\n",
    "torchvision.transforms.ToTensor(),\n",
    "torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=DATA,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=DOWNLOAD,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac27dd2-cc07-4b96-90fb-53e5f08dbc4b",
   "metadata": {},
   "source": [
    "## Training with FP32 and BF16, including AMX\n",
    "Train the Resnet50 model in three different cases:\n",
    "1. FP32 (baseline)  \n",
    "2. BF16 without AMX  \n",
    "3. BF16 with AMX  \n",
    "\n",
    "The training time is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6caea-b9d9-4ad7-9a27-fe1f82449fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with FP32\")\n",
    "fp32_training_time = trainModel(train_loader, modelName=\"fp32\", dataType=\"fp32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aafe25-4f7d-42ad-92ed-3438bd78c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with BF16 without AMX\")\n",
    "bf16_noAmx_training_time = trainModel(train_loader, modelName=\"bf16_noAmx\", amx=False, dataType=\"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc8a70-509a-4714-8524-084f34e287c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with BF16 with AMX\")\n",
    "bf16_withAmx_training_time = trainModel(train_loader, modelName=\"bf16_withAmx\", dataType=\"bf16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d2e1e7-40db-4e94-9f04-0d812992169a",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "The following cells below will summarize the training times for all three cases and display graphs to show the performance speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3aa054-a36e-47e8-a646-ac939b74776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary\")\n",
    "print(\"FP32 training time: %.3f\" %fp32_training_time)\n",
    "print(\"BF16 without AMX training time: %.3f\" %bf16_noAmx_training_time)\n",
    "print(\"BF16 with AMX training time: %.3f\" %bf16_withAmx_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c69b95-3d3f-49c0-82ac-1de5bf16ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"ResNet Training Time\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Training Time (seconds)\")\n",
    "plt.bar([\"FP32\", \"BF16 no AMX\", \"BF16 with AMX\"], [fp32_training_time, bf16_noAmx_training_time, bf16_withAmx_training_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea6ae7",
   "metadata": {},
   "source": [
    "The training times for the 3 cases are printed out and shown in the figure above. Using BF16 should show significant reduction in training time. However, there is little to no change using AVX512 with BF16 and AMX with BF16 because the amount of computations required for one batch is too small with this dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f265bfc-1e75-455b-8d8a-08245325f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_from_fp32 = fp32_training_time / bf16_withAmx_training_time\n",
    "print(\"BF16 with AMX is %.2fX faster than FP32\" %speedup_from_fp32)\n",
    "speedup_from_bf16 = bf16_noAmx_training_time / bf16_withAmx_training_time\n",
    "print(\"BF16 with AMX is %.2fX faster than BF16 without AMX\" %speedup_from_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990591d-3f4b-40f9-85fc-6b9a7c8e2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"AMX Speedup\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.bar([\"FP32\", \"BF16 no AMX\"], [speedup_from_fp32, speedup_from_bf16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea2aeb",
   "metadata": {},
   "source": [
    "This figure shows the relative performance speedup of AMX compared to FP32 and BF16 with AVX512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01080",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This code sample shows how to enable and disable AMX during runtime, as well as the performance improvements using AMX BF16 for training on the ResNet50 model. Performance will vary based on your hardware and software versions. To see more performance improvement between AVX-512 BF16 and AMX BF16, increase the amount of required computations in one batch. This can be done by increasing the batch size with CIFAR10 or using another dataset. For even more speedup, consider using the Intel® Extension for PyTorch* [Launch Script](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/launch_script.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0877d6-e045-4091-b5e4-4dfcb6d04f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[CODE_SAMPLE_COMPLETED_SUCCESFULLY]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed6ae0d06e7bec0fef5f1fb38f177ceea45508ce95c68ed2f49461dd6a888a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
