{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f9200e-7830-4ee5-8637-e67b5df57eac",
   "metadata": {},
   "source": [
    "# PyTorch Training Optimizations with Advanced Matrix Extensions Bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb565f-ef03-40cb-9182-5b2b752331e8",
   "metadata": {},
   "source": [
    "This code sample will train a ResNet50 model using the CIFAR10 dataset while using Intel's Extension for PyTorch (IPEX). The model will be trained using FP32 and BF16 precision, including the use of Intel® Advanced Matrix Extensions (Intel® AMX) on BF16. Intel® AMX is supported on BF16 and INT8 data types starting with the 4th Generation of Xeon Scalable Processors, Sapphire Rapids. The training time will be compared, showcasing the speedup of Intel® AMX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a0285-162b-4435-96cb-1d9ab2bbfe8a",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751acd58-3cc0-42c3-8f59-17136a1518b7",
   "metadata": {},
   "source": [
    "Ensure the PyTorch kernel is activated before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41ce52-c94c-4bdf-a528-0e0200fd5501",
   "metadata": {},
   "source": [
    "## Imports, Dataset, Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4eedf0-5c7c-49d3-be15-f46b4988d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17246f67-0059-4b5f-afe8-a105d767b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and constants\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "DOWNLOAD = True\n",
    "DATA = 'datasets/cifar10/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771f165",
   "metadata": {},
   "source": [
    "## Identify Supported ISA  \n",
    "We identify the underlying supported ISA to determine whether Intel® AMX is supported. The 4th Gen Intel® Xeon® Scalable Processor (codenamed Sapphire Rapids) or newer must be used to run this sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c339a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if hardware supports Intel® AMX\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import version_check\n",
    "from cpuinfo import get_cpu_info\n",
    "info = get_cpu_info()\n",
    "flags = info['flags']\n",
    "amx_supported = False\n",
    "for flag in flags:\n",
    "    if \"amx\" in flag:\n",
    "        amx_supported = True\n",
    "        break\n",
    "if not amx_supported:\n",
    "    print(\"Intel® AMX is not supported on current hardware. Code sample cannot be run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f461d",
   "metadata": {},
   "source": [
    "If the message \"Intel® AMX is not supported on current hardware. Code sample cannot be run.\" is printed above, the hardware being used does not support Intel® AMX. Therefore, this code sample cannot proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd66ee-aac5-4a60-8f66-417612d4d3af",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "The function trainModel() will train the Resnet50 model based on the whether Intel® AMX should be enabled, and whether to use FP32 or BF16 data type. The environment variable `ONEDNN_MAX_CPU_ISA` is used to enable or disable Intel® AMX. **Note that this environment variable is only initialized once.** This means to run with Intel® AMX and VNNI, there will need to be separate processes. The best practice is to set this environment variable before running your script. For more information, refer to the [oneDNN documentation on CPU Dispatcher Control](https://www.intel.com/content/www/us/en/develop/documentation/onednn-developer-guide-and-reference/top/performance-profiling-and-inspection/cpu-dispatcher-control.html). \n",
    "\n",
    "To use BF16 in operations, use the `torch.cpu.amp.autocast()` function to perform forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7486b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e21c9-aaa5-4f75-b00a-0d875cc0bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to run a test case\n",
    "\"\"\"\n",
    "def trainModel(train_loader, modelName=\"myModel\", amx=True, dataType=\"fp32\"):\n",
    "    \"\"\"\n",
    "    Input parameters\n",
    "        train_loader: a torch DataLoader object containing the training data\n",
    "        modelName: a string representing the name of the model\n",
    "        amx: set to False to disable Intel® AMX on BF16, default True otherwise\n",
    "        dataType: the data type for model parameters, supported values - fp32, bf16\n",
    "    Return value\n",
    "        training_time: the time in seconds it takes to train the model\n",
    "    \"\"\"\n",
    "\n",
    "    if amx:\n",
    "        # Set the environment variable to enable Intel® AMX\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_CORE_AMX\"\n",
    "    else:\n",
    "        # Set the environment variable to disable Intel® AMX\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_CORE_BF16\"\n",
    "\n",
    "    # Initialize the model \n",
    "    model = torchvision.models.resnet50()\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    model.train()\n",
    "    \n",
    "    # Optimize with BF16 or FP32 (default)\n",
    "    if \"bf16\" == dataType:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "\n",
    "    # Train the model\n",
    "    num_batches = len(train_loader)\n",
    "    start_time = time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if \"bf16\" == dataType:\n",
    "            with torch.cpu.amp.autocast():   # Auto Mixed Precision\n",
    "                # Setting memory_format to torch.channels_last could improve performance with 4D input data. This is optional.\n",
    "                data = data.to(memory_format=torch.channels_last)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "        else:\n",
    "            # Setting memory_format to torch.channels_last could improve performance with 4D input data. This is optional.\n",
    "            data = data.to(memory_format=torch.channels_last)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "        if 0 == (batch_idx+1) % 50:\n",
    "            print(\"Batch %d/%d complete\" %(batch_idx+1, num_batches))\n",
    "    end_time = time()\n",
    "    training_time = end_time-start_time\n",
    "    print(\"Training took %.3f seconds\" %(training_time))\n",
    "    \n",
    "    # Save a checkpoint of the trained model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, 'checkpoint_%s.pth' %modelName)\n",
    "        \n",
    "    return training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab47ca1-d8fd-4e69-bca6-cb29dba6596b",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The CIFAR10 dataset is used for this sample. Batch size will be set to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d9563-e40e-48b4-a311-a71a762b5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "torchvision.transforms.Resize((224, 224)),\n",
    "torchvision.transforms.ToTensor(),\n",
    "torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=DATA,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=DOWNLOAD,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac27dd2-cc07-4b96-90fb-53e5f08dbc4b",
   "metadata": {},
   "source": [
    "## Training with FP32 and BF16, including Intel® AMX\n",
    "Train the Resnet50 model in three different cases:\n",
    "1. FP32 (baseline)  \n",
    "2. BF16 without Intel® AMX  \n",
    "3. BF16 with Intel® AMX  \n",
    "\n",
    "The training time is recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6caea-b9d9-4ad7-9a27-fe1f82449fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with FP32\")\n",
    "fp32_training_time = trainModel(train_loader, modelName=\"fp32\", dataType=\"fp32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aafe25-4f7d-42ad-92ed-3438bd78c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with BF16 with AVX512\")\n",
    "bf16_avx512_training_time = trainModel(train_loader, amx=False, modelName=\"bf16_withAmx\", dataType=\"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc8a70-509a-4714-8524-084f34e287c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model with BF16 with Intel® AMX\")\n",
    "bf16_amx_training_time = trainModel(train_loader, amx=True, modelName=\"bf16_noAmx\", dataType=\"bf16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d2e1e7-40db-4e94-9f04-0d812992169a",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "The following cells below will summarize the training times for all three cases and display graphs to show the performance speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3aa054-a36e-47e8-a646-ac939b74776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary\")\n",
    "print(\"FP32 training time: %.3f\" %fp32_training_time)\n",
    "print(\"BF16 with AVX512 training time: %.3f\" %bf16_avx512_training_time)\n",
    "print(\"BF16 with Intel® AMX training time: %.3f\" %bf16_amx_training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c69b95-3d3f-49c0-82ac-1de5bf16ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"ResNet Training Time\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Training Time (seconds)\")\n",
    "plt.bar([\"FP32\", \"BF16 w/AVX512\", \"BF16 w/Intel® AMX\"], [fp32_training_time, bf16_avx512_training_time, bf16_amx_training_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea6ae7",
   "metadata": {},
   "source": [
    "The training times for the 3 cases are printed out and shown in the figure above. Using BF16 should show significant reduction in training time. However, there is little to no change using AVX512 with BF16 and Intel® AMX with BF16 because the amount of computations required for one batch is too small with this dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f265bfc-1e75-455b-8d8a-08245325f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16_avx512_speedup_from_fp32 = fp32_training_time / bf16_avx512_training_time\n",
    "print(\"BF16 with AVX512 is %.2fX faster than FP32\" %bf16_avx512_speedup_from_fp32)\n",
    "bf16_amx_speedup_from_fp32 = fp32_training_time / bf16_amx_training_time\n",
    "print(\"BF16 with Intel® AMX is %.2fX faster than FP32\" %bf16_amx_speedup_from_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990591d-3f4b-40f9-85fc-6b9a7c8e2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Intel® AMX Speedup\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.bar([\"FP32\", \"BF16 w/AVX512\", \"BF16 w/Intel® AMX\"], [1, bf16_avx512_speedup_from_fp32, bf16_amx_speedup_from_fp32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea2aeb",
   "metadata": {},
   "source": [
    "This figure shows the relative performance speedup of Intel® AMX compared to FP32 and BF16 with AVX512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01080",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This code sample shows how to enable and disable Intel® AMX during runtime, as well as the performance improvements using Intel® AMX BF16 for training on the ResNet50 model. Performance will vary based on your hardware and software versions. To see more performance improvement between AVX-512 BF16 and Intel® AMX BF16, increase the batch size with CIFAR10 or use another dataset. For even more speedup, consider using the Intel® Extension for PyTorch (IPEX) [Launch Script](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/launch_script.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0877d6-e045-4091-b5e4-4dfcb6d04f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[CODE_SAMPLE_COMPLETED_SUCCESSFULLY]')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
