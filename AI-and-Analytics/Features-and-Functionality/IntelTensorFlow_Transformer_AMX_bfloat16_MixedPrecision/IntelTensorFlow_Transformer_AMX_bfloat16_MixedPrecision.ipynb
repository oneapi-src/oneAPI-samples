{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc048357-6e34-406b-9a6e-1caabfc3a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Copyright © 2023 Intel Corporation\n",
    "# \n",
    "# SPDX-License-Identifier: MIT\n",
    "# ============================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bdd8c5b-209e-4a7d-9d98-2b552c987039",
   "metadata": {},
   "source": [
    "# Tensorflow Transformer with AMX bfloat16 Mixed Precision Learning\n",
    "\n",
    "In this example we will be learning Transformer block for text classification using **IMBD dataset**. And then we will modify the code to use mixed precision learning with **bfloat16**. The example based on the [Text classification with Transformer Keras code example](https://keras.io/examples/nlp/text_classification_with_transformer/).\n",
    "\n",
    "To start this sample, make sure you have installed [Intel® oneAPI AI Analytics Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html) For more informations and istructions please follow [Get Started with the Intel® AI Analytics Toolkit](https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html).\n",
    "\n",
    "Also, we need to check if Jupyter Notebook runs on  4th Gen Intel® Xeon® Scalable Processors (Sapphire Rapids). The code below will return the specific architecture the Notebook is running on. If it returns SPR you are ready to go with the rest of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpuinfo import get_cpu_info\n",
    "\n",
    "info = get_cpu_info()\n",
    "flags = info['flags']\n",
    "arch_list = ['SPR', 'CPX',\"ICX|CLX\", \"SKX\", \"BDW|CORE|ATOM\"]\n",
    "isa_list = [['amx_bf16', 'amx_int8', 'amx_tile'],['avx512_bf16'],['avx512_vnni'],['avx512'],['avx2']]\n",
    "index = len(arch_list) - 1\n",
    "for flag in flags:\n",
    "    for idx, isa_sublist in enumerate(isa_list):\n",
    "        for isa in isa_sublist:\n",
    "            if isa in flag:\n",
    "                if idx < index:\n",
    "                    index = idx\n",
    "arch = arch_list[index]\n",
    "\n",
    "print(arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a48da-eecc-4a94-99d9-31aa121543c9",
   "metadata": {},
   "source": [
    "Let's start by downloading the sample from the keras.io github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5b57c-8caf-4117-87b2-d16f674fa859",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/keras-team/keras-io/master/examples/nlp/text_classification_with_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22554015-3cd4-4654-af42-2afcc801ff75",
   "metadata": {},
   "source": [
    "## Existing example explanation\n",
    "The example implements a Transformer block as a layer. Transformer block consists of layers of Self Attention, feed-forward (i.e., Dense) and Normalization. Example uses the `TransformerBlock` provided by `keras`.\n",
    "\n",
    "Later it implements embedding layer. There are two seperate embedding layers:\n",
    "* one for tokens,\n",
    "* one for token index (positions).\n",
    "In Transformer-based networks, we need to include positional information of the tokens in the embeddings. There is used the `TokenAndPositionEmbedding` provided in `keras`.\n",
    "\n",
    "In next step **IMDB dataset** is download. It contains 50,000 movie reviews with 2 classes (positive and negative). There is provided a set of 25,000 texts for training and 25,000 for testing. Only top 20,000 words as a vocabulary size and only first 200 words of each movie review are considered in the example.\n",
    "\n",
    "The following step is to create classifier. Transformer layer outputs one vector for each time step of our input sequence. Here, there is taken the mean across all time steps and use a feed forward network on top of it to classify text.\n",
    "\n",
    "At the last step is to train and evaluate the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072ddd8-eb38-473c-bc03-ea442c7fac0a",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "\n",
    "To show benefits in performance by using bfloat16 mixed precision learning let's measure the time needed to learn the model. We need to apply code which creates a variable to store the times before and after learning, and then prints the difference to the standard output availabe in the prepared patch file `time.patch`. Let's look on the prepared file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b05169-bb12-4a34-b12a-cf9f06102db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./patch/time.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b353598-9354-4765-ae63-45a8efd59860",
   "metadata": {},
   "source": [
    "So let's apply time measure to the keras.io sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790fc0-d1ce-4be7-87cb-a29dbdeddfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!patch text_classification_with_transformer.py ./patch/time.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf9ca2-2683-4072-8925-1caa24f906d4",
   "metadata": {},
   "source": [
    "#### Run sample\n",
    "The script `job.sh` encapsulates the program for subbmission to the job queue for execution. \n",
    "\n",
    "To collect information about how much of an application runtime is spent executing oneDNN primitives and which of those take the mosf time we are using oneDNN verbose mode:\n",
    "\n",
    "* `ONEDNN_VERBOSE=1` - to enable primitive information at execution primitive information at creation and execution,\n",
    "* `ONEDNN_VERBOSE_TIMESTAMP=1` - to display timestamp.\n",
    "\n",
    "The whole output of the program will be saved in dedicated logs file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aacff6-abb2-4dde-916e-7039b7a0e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job.sh\n",
    "#!/bin/bash\n",
    "\n",
    "mkdir logs\n",
    "\n",
    "wget https://raw.githubusercontent.com/IntelAI/models/master/benchmarks/common/platform_util.py\n",
    "\n",
    "echo \"########## Executing the run\"\n",
    "\n",
    "source /opt/intel/oneapi/setvars.sh\n",
    "source activate tensorflow\n",
    "\n",
    "ONEDNN_VERBOSE_TIMESTAMP=1 ONEDNN_VERBOSE=1 python ./text_classification_with_transformer.py > ./logs/dnn_logs.txt\n",
    "\n",
    "echo \"########## Done with the run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69270656-e90a-4096-a847-93cd439b0978",
   "metadata": {},
   "source": [
    "#### Submitting job.sh to the job queue\n",
    "\n",
    "Now we can submit `job.sh` to the job queue.\n",
    "\n",
    "**NOTE - it is possible to any of the run commands in local environments.**\n",
    "\n",
    "To enable users to run their scripts either on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command qsub. If the check fails, it is assumed that run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889d206-b583-4364-a460-6eacb803417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export property=spr; chmod 755 q; chmod 755 job.sh; if [ -x \"$(command -v qsub)\" ]; then ../../q job.sh; else ./job.sh; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71f6a9f6-e196-49ef-ba14-fb2c47b2a2c4",
   "metadata": {},
   "source": [
    "## Modification for mixed precision learning using bfloat16\n",
    "\n",
    "To use bfloat16 mixed precision learning we need to add the following line:\n",
    "\n",
    "```python\n",
    "tf.config.optimizer.set_experimental_options({'auto_mixed_precision_onednn_bfloat16':True})\n",
    "```\n",
    "\n",
    "available in the prepared patch file `mixed_precision.patch` and the rest of the code should stay the same. So let's take a look what's in the prepared file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566a13a-1adf-4ca6-b0e6-92b8244a698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./patch/mixed_precision.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b9414-ba1c-46b5-984d-f6aa7945fbb3",
   "metadata": {},
   "source": [
    "And apply it to the downloaded example from the keras.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2b212-d6ac-4b2f-8e66-cf9b7a39f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!patch text_classification_with_transformer.py ./patch/mixed_precision.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704b7b1-b097-4767-b4aa-3cca1e3f05b2",
   "metadata": {},
   "source": [
    "#### Run sample and submit script to the job queue\n",
    "Let's use script similar to `job.sh` that we prepared already and submit updated version of the text classification sample with bfloat16 mixed precision learning. We will only change file for saving logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b0700-11b3-4bab-a32b-65519044f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job_mixed.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"########## Executing the run\"\n",
    "\n",
    "source /opt/intel/oneapi/setvars.sh\n",
    "source activate tensorflow\n",
    "\n",
    "ONEDNN_VERBOSE_TIMESTAMP=1 ONEDNN_VERBOSE=1 python ./text_classification_with_transformer.py > ./logs/dnn_logs_mixed.txt\n",
    "\n",
    "echo \"########## Done with the run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8bdda3-5d2b-41cc-881b-fc5033360c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export property=spr; chmod 755 q; chmod 755 job_mixed.sh; if [ -x \"$(command -v qsub)\" ]; then ../../q job_mixed.sh; else ./job_mixed.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7830de3-8a83-4d16-8353-bd15aff929e4",
   "metadata": {},
   "source": [
    "### Performance comparison\n",
    "\n",
    "Now let's parse `job.sh` outputs to compare the learning times of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930a178-b326-436f-a52f-ff90aea7849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./logs/dnn_logs.txt | grep \"time: \"; cat ./logs/dnn_logs_mixed.txt | grep \"time: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0914c66-4a60-4356-9338-b122ab02a083",
   "metadata": {},
   "source": [
    "There are shown times of learning text classification sample. First of times is for **float32 learning**, and second is time of model learning using the **bfloat16 mixed precision**.\n",
    "As we can see time for using bfloat16 mixed precision learning is better than with float32 learning process, which shows the performance improvement with AMX and bfloat16 usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613dd0a-361c-471b-b47a-9718e69f4edb",
   "metadata": {},
   "source": [
    "## ISA Comparison\n",
    "The section below compares and analyzes different ISA upon JIT Kernel usage and CPU instruction usage.\n",
    "\n",
    "Those comparisons can be conducted on the same CPU microarchitecture with the help of oneDNN CPU dispatcher control."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bd0bdf9",
   "metadata": {},
   "source": [
    "### AMX run\n",
    "\n",
    "First, we will run the same example on the maximum available CPU ISA, i.e., on AMX by setting `DNNL_MAX_CPU_ISA` to `AMX_BF16` and also pointing to the corresponding file where the statistics of the execution of the example `./logs/log_cpu_bf16_amx.csv` will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924e956-e529-4066-82c0-1e3e3181186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_amx.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"########## Executing the run\"\n",
    "\n",
    "source activate tensorflow\n",
    "\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=2 \n",
    "# enable JIT Dump\n",
    "export DNNL_JIT_DUMP=1\n",
    "\n",
    "DNNL_MAX_CPU_ISA=AMX_BF16 python ./text_classification_with_transformer.py cpu >> ./logs/log_cpu_bf16_amx.csv 2>&1\n",
    "\n",
    "echo \"########## Done with the run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec5188-53ac-457d-840e-2f6971d7c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export property=spr; chmod 755 q; chmod 755 run_amx.sh; if [ -x \"$(command -v qsub)\" ]; then ../../q run_amx.sh; else ./run_amx.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b452168",
   "metadata": {},
   "source": [
    "### AVX512 BF16 run\n",
    "\n",
    "Next, we will run this example on maximum by setting the maximum CPU ISA to AVX512 BF16, setting `DNNL_MAX_CPU_ISA` to `AVX512_CORE_BF16` and also pointing to the appropriate file to save the statistics of the example `./logs/log_cpu_bf16_avx512_bf16.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0771d6-33ed-4666-b6e3-6f0ff97a5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"########## Executing the run\"\n",
    "\n",
    "source activate tensorflow\n",
    "\n",
    "# enable verbose log\n",
    "export DNNL_VERBOSE=2 \n",
    "# enable JIT Dump\n",
    "export DNNL_JIT_DUMP=1\n",
    "\n",
    "DNNL_MAX_CPU_ISA=AVX512_CORE_BF16 python ./text_classification_with_transformer.py cpu >> ./logs/log_cpu_bf16_avx512_bf16.csv 2>&1\n",
    "\n",
    "echo \"########## Done with the run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b106f-e3e6-46fc-9a98-f8a89cf6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export property=spr; chmod 755 q; chmod 755 run.sh; if [ -x \"$(command -v qsub)\" ]; then ../../q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e17283-2383-4f30-845f-7cc571122001",
   "metadata": {},
   "source": [
    "#### oneDNN Verbose Log JIT Kernel Time BreakDown\n",
    "\n",
    "oneDNN uses just-in-time compilation (JIT) to generate optimal code for some functions based on input parameters and instruction set supported by the system.\n",
    "Therefore, users can see different JIT kernel type among different first selected ISA and second selected ISA.\n",
    "\n",
    "To decrypt oneDNN verbose output we are using created profiling tool - `profile_utils.py` file. Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c496afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/oneapi-src/oneAPI-samples/master/Libraries/oneDNN/tutorials/profiling/profile_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e898",
   "metadata": {},
   "source": [
    "We can parse verbose log and get the data back now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292df02-1563-4f96-b312-998c5a72d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from profile_utils import oneDNNUtils, oneDNNLog\n",
    "onednn = oneDNNUtils()\n",
    "\n",
    "logfile1 = './logs/log_cpu_bf16_avx512_bf16.csv'\n",
    "log1 = oneDNNLog()\n",
    "log1.load_log(logfile1)\n",
    "exec_data1 = log1.exec_data\n",
    "\n",
    "logfile2 = './logs/log_cpu_bf16_amx.csv'\n",
    "log2 = oneDNNLog()\n",
    "log2.load_log(logfile2)\n",
    "exec_data2 = log2.exec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ba513-5583-44c2-af42-f3dd21083ee3",
   "metadata": {},
   "source": [
    "##### JIT Kernel Type Time breakdown for AVX512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd3164-e94e-4067-804d-8010ec196565",
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data1,\"jit\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04da518-6bf5-41fd-9e53-dd63229fe514",
   "metadata": {},
   "source": [
    "##### JIT Kernel Type Time breakdown for AMX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b259d-1488-4927-8308-fef5c7232436",
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data2,\"jit\",\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35925923-142f-43a7-b482-f058ac426ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('idp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f954598d05c49c7b2a3e840295d971716fdeffd17843be95d2639682acd709ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
