# `Intel Extension for PyTorch Getting Started` Sample

torch-ccl holds PyTorch bindings maintained by Intel for the Intel® oneAPI Collective Communications Library (oneCCL).

Intel® oneCCL (collective communications library) is a library for efficient distributed deep learning training that implements such collectives like allreduce, allgather, and alltoall. For more information on oneCCL, please refer to the oneCCL documentation.

For comprehensive instructions regarding distributed training with oneCCL in PyTorch, go to the following github repos:
* [PyTorch and oneCCL](https://github.com/intel/torch-ccl) 
* [PyTorch](https://github.com/intel/optimized-models/tree/master/pytorch/distributed)

| Optimized for                       | Description
|:---                               |:---
| OS                                | Linux* Ubuntu* 18.04
| Hardware                          | Skylake with GEN9 or newer
| Software                          | Intel Extension for PyTorch;
| What you will learn               | How to perform distributed training with oneCCL in PyTorch
| Time to complete                  | 60 minutes


## Purpose

From this sample code, you will learn how to perform distributed training with oneCCL in PyTorch.

The code will be running on the CPU.

## Key Implementation Details 

The code includes how to perform distributed training with oneCCL in PyTorch.
 
## License  

Code samples are licensed under the MIT license. See
[License.txt](https://github.com/oneapi-src/oneAPI-samples/blob/master/License.txt) for details.

Third party program Licenses can be found here: [third-party-programs.txt](https://github.com/oneapi-src/oneAPI-samples/blob/master/third-party-programs.txt)

## Building the `torch-ccl Getting Started` Sample

### On a Linux* System 

Please follow instructions [here](https://github.com/intel/optimized-models/tree/master/pytorch/distributed#distributed-training-with-oneccl-in-pytorch).

## Running the Sample

Please follow instructions [here](https://github.com/intel/optimized-models/tree/master/pytorch/distributed#run-scripts--cpu-affinity).
