# `Intel Extension for PyTorch Getting Started` Sample

torch-ccl holds PyTorch bindings maintained by Intel for the Intel® oneAPI Collective Communications Library (oneCCL).

Intel® oneCCL (collective commnications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the oneCCL documentation.

For comprehensive instructions regarding distributed training with oneCCL in PyTorch, go to https://github.com/intel/torch-ccl and https://github.com/intel/optimized-models/tree/master/pytorch/distributed.

| Optimized for                       | Description
|:---                               |:---
| OS                                | Linux* Ubuntu* 18.04
| Hardware                          | Skylake with GEN9 or newer
| Software                          | Intel Extension for PyTorch;
| What you will learn               | How to perform distributed training with oneCCL in PyTorch
| Time to complete                  | 60 minutes


## Purpose

From this sample code, you will learn how to perform distributed training with oneCCL in PyTorch.

The code will be running on CPU.

## Key Implementation Details 

The code includes how to perform distributed training with oneCCL in PyTorch.
 
## License  

This code sample is licensed under MIT license. 


## Building the `torch-ccl Getting Started` Sample

### Running Samples In DevCloud

N/A

### On a Linux* System 

Please follow instructions [here](https://github.com/intel/optimized-models/tree/master/pytorch/distributed#distributed-training-with-oneccl-in-pytorch).

## Running the Sample

Please follow instructions [here](https://github.com/intel/optimized-models/tree/master/pytorch/distributed#run-scripts--cpu-affinity).
