{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Copyright Â© 2022 Intel Corporation\n",
    "# \n",
    "# SPDX-License-Identifier: MIT\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple k-NN classification with numba_dpex IDP optimization\n",
    "\n",
    "This sample shows how to recieve the same accuracy of the k-NN model classification by using numpy, numba and numba_dpex. The computetaion are performed using wine dataset.\n",
    "\n",
    "Let's start with general inports used in the whole sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Then, let's download the dataset and prepare it for future computations.\n",
    "\n",
    "We are using the wine dataset available in the sci-kit learn library. For our purposes, we will be using only 2 features: alcohol and malic_acid.\n",
    "\n",
    "So first we need to load the dataset and create DataFrame from it. Later we will limit the DataFrame to just target and 2 classes we choose for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine()\n",
    "# Convert loaded dataset to DataFrame \n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "df['target'] = pd.Series(data.target)\n",
    "\n",
    "# Limit features to 2 selected for this problem \n",
    "df = df[['target', 'alcohol', 'malic_acid']]\n",
    "\n",
    "# Show top 5 values from the limited dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are planning to compare the results of the numpy, namba and IDP numba_dpex so we need to make sure that the results are reproducible. We can do this through the use of a random seed function that initializes a random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to prepare the dataset for training and testing. To do this, we randomly divided the downloaded wine dataset into a training set (containing 90% of the data) and a test set (containing 10% of the data). \n",
    "\n",
    "In addition, we take from both sets (training and test) data *X* (features) and label *y* (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we are using 10% of the data for the testing purpose\n",
    "train_sample_idx = np.random.choice(df.index, size=int(df.shape[0]*0.9), replace=False)\n",
    "train_data, test_data = df.iloc[train_sample_idx], df.drop(train_sample_idx)\n",
    "\n",
    "# get features and label from train/test data\n",
    "X_train, y_train = train_data.drop('target', axis=1), train_data['target']\n",
    "X_test, y_test = test_data.drop('target', axis=1), test_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy k-NN\n",
    "\n",
    "Now, it's time to implenet the first version of k-NN function using NumPy.\n",
    "\n",
    "First, let's create simple euqlidesian distance function. We are taking positions form the provided vectors, counting the squares of the individual differences between the positions, and then drawing the root of their sum for the whole vectors (remember that the vectors must be of equal length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(vector1, vector2):\n",
    "    dist = [(a - b)**2 for a, b in zip(vector1, vector2)]\n",
    "    dist = math.sqrt(sum(dist))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the k-nearest neighbour algorithm itself.\n",
    "\n",
    "1. We are starting by defining a container for predictions the same size as a test set.\n",
    "2. Then, for each row in the test set, we calculate distances between then and every training record.\n",
    "3. We are sorting training datasets based on calculated distances\n",
    "4. Choose k of the first elements in the sorted training list.\n",
    "5. We are counting labels appearances\n",
    "6. The most common label is set as a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, X_test, k):\n",
    "    # 1. Prepare container for predictions\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, row in X_test.iterrows():\n",
    "        # 2. Calculate distances\n",
    "        inputs = X_train.copy()\n",
    "        inputs['distance'] = inputs.apply(distance, vector2=row, axis=1)\n",
    "        inputs = pd.concat([inputs, y_train], axis=1)\n",
    "        \n",
    "        # 3. Sort based on distance\n",
    "        inputs = inputs.sort_values('distance', ascending=True)\n",
    "        \n",
    "        # 4. Choose k if the first elements\n",
    "        neighbors = inputs.head(k)\n",
    "        classes = neighbors['target'].tolist()\n",
    "        \n",
    "        # 5. Count labels appearances\n",
    "        majority_count = Counter(classes)\n",
    "        \n",
    "        # 6. Choose most common label\n",
    "        predictions[index] = majority_count.most_common(1).pop()[0]\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our prepared knn function on the wine dataset. Let's assume `k = 3`.\n",
    "The accuracy of the predicted labels is measured as the mean of the truly predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = KNN(3)\n",
    "predictions = knn(X_train, y_train, X_test, 3)\n",
    "true_values = y_test.to_numpy()\n",
    "accuracy = np.mean(predictions == true_values)\n",
    "print('Numpy accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Numba k-NN\n",
    "\n",
    "Now, let's move to the numba implementation of the k-NN algorithm. We will start the same, by defining the distance function and importing the necessary packages.\n",
    "\n",
    "For numba implementation, we are using the core functionality which is `numba.jit()` decorator.\n",
    "\n",
    "We are starting with defining the distance function. Like before it is a euclidean distance. For additional optimization we are using `np.linalg.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def euclidean_distance_numba(vector1, vector2):\n",
    "    dist = np.linalg.norm(vector1-vector2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the k-NN algorithm. Like before, there is `numba.jit()` decorator used. Other steps for the algorithm are the same as for the NumPy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def knn_numba(X_train, y_train, X_test, k):\n",
    "    # 1. Prepare container for predictions\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    for x in np.arange(X_test.shape[0]):\n",
    "                \n",
    "        # 2. Calculate distances\n",
    "        inputs = X_train.copy()\n",
    "        distances = np.zeros((inputs.shape[0], 1))\n",
    "        for i in np.arange(inputs.shape[0]):\n",
    "            distances[i] = euclidean_distance_numba(inputs[i], X_test[x])\n",
    "        \n",
    "        labels = y_train.copy()\n",
    "        labels = labels.reshape((labels.shape[0],1))\n",
    "\n",
    "        # add labels column\n",
    "        inputs = np.hstack((inputs, labels))\n",
    "        # add distance column\n",
    "        inputs = np.hstack((inputs, distances))\n",
    "\n",
    "        # 3. Sort based on distance\n",
    "        inputs = inputs[inputs[:,3].argsort()]\n",
    "        # 4. Choose k if the first elements\n",
    "        # 2nd columns contains classes, select first k values\n",
    "        neighbor_classes = inputs[:, 2][:k]\n",
    "\n",
    "        # 5. Count labels appearances\n",
    "        counter = {}\n",
    "        for item in neighbor_classes:\n",
    "            if item in counter:\n",
    "                counter[item] = counter.get(item) + 1\n",
    "            else:\n",
    "                counter[item] = 1\n",
    "        counter_sorted = sorted(counter)\n",
    "        \n",
    "        # 6. Choose most common label\n",
    "        predictions[x] = counter_sorted[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, as in the NumPy example, we are testing implemented method for the `k = 3`. \n",
    "\n",
    "The accuracy of the method is the same as in the NumPy implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn(3) using numba.jit() decorator\n",
    "predictions = knn_numba(X_train.values, y_train.values, X_test.values, 3)\n",
    "true_values = y_test.to_numpy()\n",
    "accuracy = np.mean(predictions == true_values)\n",
    "print('Numba accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba_dpex k-NN\n",
    "\n",
    "Numba_dpex implementation use `numba_dpex.kernel()` decorator. For more information about programming, SYCL kernels go to: https://intelpython.github.io/numba-dpex/latest/user_guides/kernel_programming_guide/index.html.\n",
    "\n",
    "Calculating distance is like in the NumPy example. We are using Euclidean distance. Later, we create the queue of the neighbours by the calculated distance and count in provided *k* votes for dedicated classes of neighbours.\n",
    "\n",
    "In the end, we are taking a class that achieves the maximum value of votes and setting it for the current global iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import   numba_dpex\n",
    "\n",
    "@numba_dpex.kernel\n",
    "def knn_numba_dpex(train, train_labels, test, k, predictions, votes_to_classes_lst):\n",
    "    i = numba_dpex.get_global_id(0)\n",
    "    queue_neighbors = numba_dpex.private.array(shape=(3, 2), dtype=np.float64)\n",
    "    \n",
    "    for j in range(k):\n",
    "        x1 = train[j][0]\n",
    "        x2 = test[i][0]\n",
    "\n",
    "        distance = 0.0\n",
    "        diff = x1 - x2\n",
    "        distance += diff * diff\n",
    "        dist = math.sqrt(distance)\n",
    "\n",
    "        queue_neighbors[j, 0] = dist\n",
    "        queue_neighbors[j, 1] = train_labels[j]\n",
    "        \n",
    "    for j in range(k):\n",
    "        new_distance = queue_neighbors[j, 0]\n",
    "        new_neighbor_label = queue_neighbors[j, 1]\n",
    "        index = j\n",
    "\n",
    "        while index > 0 and new_distance < queue_neighbors[index - 1, 0]:\n",
    "            queue_neighbors[index, 0] = queue_neighbors[index - 1, 0]\n",
    "            queue_neighbors[index, 1] = queue_neighbors[index - 1, 1]\n",
    "\n",
    "            index = index - 1\n",
    "\n",
    "            queue_neighbors[index, 0] = new_distance\n",
    "            queue_neighbors[index, 1] = new_neighbor_label\n",
    "\n",
    "    for j in range(k, len(train)):\n",
    "        x1 = train[j][0]\n",
    "        x2 = test[i][0]\n",
    "\n",
    "        distance = 0.0\n",
    "        diff = x1 - x2\n",
    "        distance += diff * diff\n",
    "        dist = math.sqrt(distance)\n",
    "        \n",
    "        if dist < queue_neighbors[k - 1][0]:\n",
    "            queue_neighbors[k - 1][0] = dist\n",
    "            queue_neighbors[k - 1][1] = train_labels[j]\n",
    "            new_distance = queue_neighbors[k - 1, 0]\n",
    "            new_neighbor_label = queue_neighbors[k - 1, 1]\n",
    "            index = k - 1\n",
    "\n",
    "            while index > 0 and new_distance < queue_neighbors[index - 1, 0]:\n",
    "                queue_neighbors[index, 0] = queue_neighbors[index - 1, 0]\n",
    "                queue_neighbors[index, 1] = queue_neighbors[index - 1, 1]\n",
    "\n",
    "                index = index - 1\n",
    "\n",
    "                queue_neighbors[index, 0] = new_distance\n",
    "                queue_neighbors[index, 1] = new_neighbor_label\n",
    "\n",
    "    votes_to_classes = votes_to_classes_lst[i]\n",
    "\n",
    "    for j in range(len(queue_neighbors)):\n",
    "        votes_to_classes[int(queue_neighbors[j, 1])] += 1\n",
    "\n",
    "    max_ind = 0\n",
    "    max_value = 0\n",
    "\n",
    "    for j in range(3):\n",
    "        if votes_to_classes[j] > max_value:\n",
    "            max_value = votes_to_classes[j]\n",
    "            max_ind = j\n",
    "\n",
    "    predictions[i] = max_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, like before, let's test the prepared k-NN function.\n",
    "\n",
    "In this case, we will need to provide the container for predictions: `pedictions_numba` and the container for votes per class: `votes_to_classes_lst` (the container size is 3, as we have 3 classes in our dataset).\n",
    "\n",
    "We are running a prepared k-NN function using `dctl.device_context()`, which allows us to select a divice. For more information, go to: https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/device_selection.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpctl\n",
    "\n",
    "predictions_numba = np.empty(len(X_test.values))\n",
    "# we have 3 classes\n",
    "votes_to_classes_lst = np.zeros((len(X_test.values), 3))\n",
    "with dpctl.device_context(\"opencl:cpu:0\"):\n",
    "    knn_numba_dpex[len(X_test.values), numba_dpex.DEFAULT_LOCAL_SIZE](X_train.values, y_train.values, X_test.values, 3, predictions_numba, votes_to_classes_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, let's measure the accuracy of the prepared implementation. It is measured as the number of well-assigned classes for the test set. The final result is the same for all: NumPy, numba and numba-dpex implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = y_test.to_numpy()\n",
    "accuracy = np.mean(predictions_numba == true_values)\n",
    "print('Numba_dpex accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (IntelÂ® oneAPI 2022.3)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f954598d05c49c7b2a3e840295d971716fdeffd17843be95d2639682acd709ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
