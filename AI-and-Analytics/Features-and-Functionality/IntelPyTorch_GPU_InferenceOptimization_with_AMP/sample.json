{
  "guid": "91B5893E-028B-47BC-9D84-395A60CC1B33",
  "name": "Intel PyTorch GPU Inference Optimization with AMP",
  "categories": ["Toolkit/oneAPI AI And Analytics/Features and Functionality"],
  "description": "This sample illustrates how to use AMP BFLOAT16 in PyTorch on Intel dGPU",
  "builder": ["cli"],
  "languages": [{"python":{}}],
  "os":["linux"],
  "targetDevice": ["GPU"],
  "gpuRequired": ["ats-m","dg2","pvc"],
  "ciTests": {
    "linux": [
    {
        "env": [
            "source /intel/oneapi/intelpython/bin/activate",
            "conda activate pytorch-gpu",
            "pip install uv",
            "uv python pin $(which python)",
            "uv venv --system-site-packages",
            "uv sync"
        ],
        "id": "intel pytorch gpu inference optimization with amp",
        "steps": [
            "uv run IntelPyTorch_GPU_InferenceOptimization_with_AMP.py"
         ]
    }
     ]
  },
  "expertise": "Code Optimization"
}

