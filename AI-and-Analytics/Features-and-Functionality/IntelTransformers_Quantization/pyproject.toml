[project]
name = "quantizing_transformer_model_using_intel_extension_for_transformers_itrex"
version = "0.1.0"
description = "This sample illustrates the process of quantizing the `Intel/neural-chat-7b-v3-3` language model. This model, a fine-tuned iteration of *Mistral-7B*, undergoes quantization utilizing Weight Only Quantization (WOQ) techniques provided by Intel® Extension for Transformers (ITREX)."
authors = [
    {name = "Copyright © 2023 Intel Corporation"}
]
license = {text = "MIT"}
readme = "README.md"
requires-python = ">=3.11,<3.12"
dependencies = [
    "accelerate==0.29.3",
    "datasets==2.9.0",
    "deepspeed==0.15.4",
    "intel-extension-for-pytorch==2.8.0",
    "intel-extension-for-transformers==1.4.2",
    "neural-compressor==3.4",
    "neural-speed==1.0",
    "numpy==1.26.4",
    "onnxruntime==1.19.2",
    "peft==0.10.0",
    "sentencepiece>=0.2.0",
    "setuptools>=75.8.2",
    "torch==2.8.0",
    "torchaudio==2.8.0",
    "torchvision==0.20.1",
    "transformers==4.53.0",
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "jupyter>=1.1.1",
]
