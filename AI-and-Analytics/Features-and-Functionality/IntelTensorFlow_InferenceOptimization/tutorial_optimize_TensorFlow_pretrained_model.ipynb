{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize TensorFlow pre-trained model for inference\n",
    "To get a good performance on your pre-trained model for inference, some inferece optimizations are required.   \n",
    "This tutorial will guide you how to optimize a pre-trained model for a better inference performance, and also \n",
    "analyze the model pb files before and after the inference optimizations.  \n",
    "Those optimizations includes:  \n",
    "* Converting variables to constants.\n",
    "* Removing training-only operations like checkpoint saving.\n",
    "* Stripping out parts of the graph that are never reached.\n",
    "* Removing debug operations like CheckNumerics.\n",
    "* Folding batch normalization ops into the pre-calculated weights.\n",
    "* Fusing common operations into unified versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a benchmark script\n",
    "In this tutorial, we re-use a benchmark script from Intel LPOT project, so we need to download the script first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/intel/lpot/master/examples/tensorflow/oob_models/tf_savemodel_benchmark.py -P scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a pre-trained model \"ssd_inception_v2\" from download.tensorflow.org\n",
    "This pre-trained model is an pb file without infernece optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz -P pre-trained-models ; tar zxvf pre-trained-models/ssd_inception_v2_coco_2018_01_28.tar.gz -C pre-trained-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dump all related output into log.txt\n",
    "This tutorial needs to analyze the log from benchmark script, so we dump all runtime log into log.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In jupyter notebook simple logging to console and file:\n",
    "\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename='log.txt'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch benchmark script to enable logging for throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd scripts;patch < enable_log.patch;cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch benchmark script to output the performance number into log.txt file.\n",
    "log.txt captures all INFO loggings, so we output throughput as a INFO logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Run original pre-trained model\n",
    "Let us run the downloaded pre-trained model from previous session without any optimization by using the benchmark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/tf_savemodel_benchmark.py --model_path pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model --num_iter 200 --num_warmup 10 --disable_optimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Parse the logfile for performance number\n",
    "We parse out the performance number from log.txt and save it for later performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.profile_utils import PerfPresenter\n",
    "perfp=PerfPresenter()\n",
    "Thoughput_list = []\n",
    "print(\"get throughput\")\n",
    "val = 'Throughput'\n",
    "index = 4\n",
    "line = perfp.read_throughput('log.txt', keyword=val, index=index)\n",
    "if line != None:\n",
    "    throughput=line\n",
    "    print(throughput)\n",
    "    Thoughput_list.append(float(throughput))\n",
    "else:\n",
    "    print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimize the pre-trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Optimize the model by using The Intel速 Low Precision Optimization Tool (Intel速 LPOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intel速 Low Precision Optimization Tool (Intel速 LPOT) is an open-source Python library that delivers a unified low-precision inference interface across multiple Intel-optimized Deep Learning (DL) frameworks on both CPUs and GPUs.\n",
    "LPOT also provides graph optimizations for fp32 pre-trained models with more optimizations (such as common subexpression elimination) than the TensorFlow optimization tool [optimize_for_inference] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py). Users could refer to [fp32 optimization](https://github.com/intel/lpot/blob/master/docs/graph_optimization.md#1-fp32-optimization) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lpot.experimental import Graph_Optimization\n",
    "graph_opt = Graph_Optimization()\n",
    "graph_opt.model = 'pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model'   # the path to saved_model dir\n",
    "output = graph_opt()\n",
    "output.save('pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model_lpot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Optimize the model by using TensorFlow Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a freeze_optimize_v2.py for inference optimization in this tutorial, and Intel is working on upstreaming this script to TensorFlow github.  \n",
    "The input of this script is the directory of original saved model, and output of this script is the directory of optimzed model.  \n",
    "Users don't need to change below command in this tutorial, but need to put related directories after \"--input_saved_model_dir\" and \"--output_saved_model_dir\" for other pre-trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/freeze_optimize_v2.py --input_saved_model_dir=pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model --output_saved_model_dir=pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run optimized model\n",
    "Let us run the optimized model by using the benchmark script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, pick one of the optimized models from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf optimized_model_lpot pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/tf_savemodel_benchmark.py --model_path pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model --num_iter 200 --num_warmup 10 --disable_optimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Parse the logfile for performance number\n",
    "We parse out the performance number from log.txt and save it for later performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.profile_utils import PerfPresenter\n",
    "perfp=PerfPresenter()\n",
    "print(\"get throughput\")\n",
    "val = 'Throughput'\n",
    "index = 4\n",
    "line = perfp.read_throughput('log.txt', keyword=val, index=index)\n",
    "if line!=None:\n",
    "    throughput=line\n",
    "    print(throughput)\n",
    "    Thoughput_list.append(float(throughput))\n",
    "else:\n",
    "    print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance Comparison\n",
    "We compare the performance difference between original saved model and optimized model, and show a speedup diagram accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(Thoughput_list)\n",
    "speedup = float(Thoughput_list[1])/float(Thoughput_list[0])\n",
    "print(\"Speedup : \", speedup)\n",
    "df = pd.DataFrame({'pretrained_model':['saved model', 'optimized model'], 'Speedup':[1, speedup]})\n",
    "ax = df.plot.bar( x='pretrained_model', y='Speedup', rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the pre-trained model PB files\n",
    "In this tutorial,we use tf_pb_utils.py to parse a pb file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ( Optional ) 1. Understand structures of a PB file\n",
    "This section is optional for this tutorial because we fully understand the structure of those pb files.  \n",
    "If you investigate into a new pb file, you might need to go through below section to understand the structure of pb file. \n",
    "\n",
    "The tf_pb_utils.py will parse op.type and op.name from a graph_def into a CSV file.  \n",
    "\n",
    "op.name might contains a layer structure.  \n",
    "\n",
    "Below is an example.  \n",
    "ex : FeatureExtractor\\InceptionV2\\InceptionV2\\Mixed_5c\\Branch_2\\Conv2d_0b_3x3\\Conv2D  \n",
    "The first layer is FeatureExtractor, and the second layer is InceptionV2. \n",
    "The last layer is Conv2D.  \n",
    "\n",
    "Here is another example.  \n",
    "ex : BoxPredictor_4\\BoxEncodingPredictor\\Conv2D  \n",
    "Even the last layer is Conv2D, it has different first and second layer.  \n",
    "Moreover, this Conv2D is not related to InceptionV2 layer, so we don't want to count this Conv2D as a inceptionV2 ops.  \n",
    "\n",
    "\n",
    "Therefore, we still need the layers information to focus on the ops important to us.  \n",
    "\n",
    "we parse op.type and op.name into a CSV file \"out.csv\", and below is a mapping table between CSV column and op.type & op.name. op.name[i] represnt the i layer of this op.name.   \n",
    "\n",
    "\n",
    "|op_type|op_name|op1|op2|op3|op4|op5|op6|\n",
    "|:-----|:----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|op.type| op.name[-1] |op.name[0] | op.name[1] | op.name[2] |op.name[3] |op.name[4] |op.name[5] |  \n",
    "\n",
    "\n",
    "Following two sub-sections will show you how to focus on op.type of a interested layer such as InceptionV2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ( Optional ) Find the column which contain the interested layer such as InceptionV2\n",
    "Below command will group rows by the values on the selected column from out.csv file.\n",
    "Check which column contains the interested layer.  \n",
    "Below is column 3 of ssd_inception_v2 case, it contains InceptionV2 as second row.\n",
    "\n",
    "     == Dump column : 3 ==  \n",
    "    op2  \n",
    "    BatchMultiClassNonMaxSuppression    5307  \n",
    "    InceptionV2                         1036  \n",
    "    0                                    263  \n",
    "    map                                   63  \n",
    "    Decode                                63  \n",
    "    ClassPredictor                        36  \n",
    "    BoxEncodingPredictor                  36  \n",
    "    Meshgrid_14                           34  \n",
    "    Meshgrid_1                            34  \n",
    "    Meshgrid_10                           34  \n",
    "    dtype: int64  \n",
    "\n",
    "\n",
    "\n",
    "Both indexs of column and row start from 0. \n",
    "Therefore, we could access second row by index 1.  \n",
    "By using column index 3 and row index 1, we could access InceptionV2 related op.name.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/tf_pb_utils.py pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model/saved_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze the original model PB file\n",
    "From previous section, we know we can know InceptionV2 related ops by access column index 3 and row index 1.  \n",
    "Therefore, we append \"-c 3 -r 1\" in the end of below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/tf_pb_utils.py pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model/saved_model.pb -c 3 -r 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Analyze the optimized model PB file\n",
    "From previous section, we know we can know InceptionV2 related ops by access column index 3 and row index 1.  \n",
    "Therefore, we append \"-c 3 -r 1\" in the end of below command.  \n",
    "\n",
    ">By comparing the diagrams, you could understand that FusedBatchNorm ops is replaced by BiasAdd ops after inference optimization, because inference optimizatoin folds batch normalization ops into the pre-calculated weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/tf_pb_utils.py pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model/saved_model.pb -c 3 -r 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel-tensorflow",
   "language": "python",
   "name": "intel-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
