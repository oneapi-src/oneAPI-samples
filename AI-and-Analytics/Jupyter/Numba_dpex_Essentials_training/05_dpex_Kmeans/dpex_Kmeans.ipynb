{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Algorithm Using Numba-dpex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "- [KMeans Algorithm](#KMeans-Algorithm)\n",
    "- _Code:_ [Implementation of Kmeans targeting CPU using Numba JIT](#Implementation-of-Kmeans-targeting-CPU-using-Numba-JIT)\n",
    "- _Code:_ [Implementation of Kmeans targeting GPU using Numba JIT](#Implementation-of-K-Means-targeting-GPU-using-Numba-JIT)\n",
    "- _Code:_ [Implementation of Kmeans targeting GPU using Kernels](#Implementation-of-Kmeans-targeting-GPU-using-Kernels)\n",
    "- _Code:_ [Implementation of Kmeans targeting GPU using atomics](#Implementation-of-Kmeans-targeting-GPU-using-atomics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Build a Numba implementation of K-means targeting CPU and GPU using Numba JIT\n",
    "* Build a Numba-dpex implementation of K-means on CPU and GPU using kernel approach\n",
    "* Build a Numba-dpex implementation of K-means on GPU using Atomics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numba-dpex\n",
    "\n",
    "Numba-dpex is a standalone extension to the Numba JIT compiler that adds SYCL programming capabilities to Numba. Numba-dpex is packaged as part of the IDP that comes with oneAPI base toolkit, and you don’t need to install any specific Conda packages. The support for SYCL is via SYCL runtime and other SYCL compilers are not supported by Numba-dpex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line parameters\n",
    "\n",
    "| Type | Default Value | Description |\n",
    "|:---|:---|:---|\n",
    "| --steps | 10 | Number of workload runs |\n",
    "| --step | 2  | Data growth factor on each iteration |\n",
    "| --size | 2 ** 28 | Initial data size |\n",
    "| --repeat | 1 | Iterations inside measured region |\n",
    "| --json | False | Output json data filename |\n",
    "| -d | 1 | Data Dimension |\n",
    "| --usm | False | Use USM Shared |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# K-Means Algorithm\n",
    "Kmeans is a clustering algorithm that partitions observations from a dataset into a requested number of geometric clusters of points closest to the cluster’s own center of mass. Using an initial estimate of the centroids, the algorithm iteratively updates the positions of the centroids until a fixed point.\n",
    "\n",
    "\n",
    "Kmeans is a simple and powerful ML algorithm to cluster data into similar groups. Its objective is to split a set of N observations into K clusters. This is achieved by minimizing inertia (i.e., the sum of squared Euclidian distances from observations to the cluster centers, or centroids). The algorithm is iterative, with two steps in each iteration:\n",
    "* For each observation, compute the distance from it to each centroid, and then reassign each observation to the cluster with the nearest centroid.\n",
    "* For each cluster, compute the centroid as the mean of observations assigned to this cluster.\n",
    "\n",
    "Repeat these steps until the number of iterations exceeds a predefined maximum or the algorithm converges (i.e., the difference between two consecutive inertias is less than a predefined threshold).\n",
    "Different methods are used to get initial centroids for the first iteration. The algorithm can select random observations as initial centroids or use more complex methods such as kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Kmeans targeting CPU using Numba JIT\n",
    "In the following example, we introduce a naive K-means implementation that targets a CPU using the Numba JIT.\n",
    "\n",
    "This is the decorator-based approach, where we offload data parallel code sections like parallel-for, and certain NumPy function calls. With the decorator method, a programmer needs to simply identify the most time-consuming parts of the program. If those parts can be parallelized, the programmer needs to annotate those sections using Numba-dpex, and can expect those code sections to execute on a GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/kmeans.py\n",
    "import base_kmeans\n",
    "import numpy\n",
    "import numba\n",
    "\n",
    "REPEAT = 1\n",
    "\n",
    "ITERATIONS = 30\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, parallel=True, fastmath=True)\n",
    "def groupByCluster(arrayP, arrayPcluster, arrayC, num_points, num_centroids):\n",
    "    for i0 in numba.prange(num_points):\n",
    "        minor_distance = -1\n",
    "        for i1 in range(num_centroids):\n",
    "            dx = arrayP[i0, 0] - arrayC[i1, 0]\n",
    "            dy = arrayP[i0, 1] - arrayC[i1, 1]\n",
    "            my_distance = numpy.sqrt(dx * dx + dy * dy)\n",
    "            if minor_distance > my_distance or minor_distance == -1:\n",
    "                minor_distance = my_distance\n",
    "                arrayPcluster[i0] = i1\n",
    "    return arrayPcluster\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, parallel=True, fastmath=True)\n",
    "def calCentroidsSum(\n",
    "    arrayP, arrayPcluster, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "):\n",
    "    for i in numba.prange(num_centroids):\n",
    "        arrayCsum[i, 0] = 0\n",
    "        arrayCsum[i, 1] = 0\n",
    "        arrayCnumpoint[i] = 0\n",
    "\n",
    "    for i in range(num_points):\n",
    "        ci = arrayPcluster[i]\n",
    "        arrayCsum[ci, 0] += arrayP[i, 0]\n",
    "        arrayCsum[ci, 1] += arrayP[i, 1]\n",
    "        arrayCnumpoint[ci] += 1\n",
    "\n",
    "    return arrayCsum, arrayCnumpoint\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True, parallel=True, fastmath=True)\n",
    "def updateCentroids(arrayC, arrayCsum, arrayCnumpoint, num_centroids):\n",
    "    for i in numba.prange(num_centroids):\n",
    "        arrayC[i, 0] = arrayCsum[i, 0] / arrayCnumpoint[i]\n",
    "        arrayC[i, 1] = arrayCsum[i, 1] / arrayCnumpoint[i]\n",
    "\n",
    "\n",
    "def kmeans(\n",
    "    arrayP, arrayPcluster, arrayC, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "):\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        groupByCluster(arrayP, arrayPcluster, arrayC, num_points, num_centroids)\n",
    "\n",
    "        calCentroidsSum(\n",
    "            arrayP, arrayPcluster, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "        )\n",
    "\n",
    "        updateCentroids(arrayC, arrayCsum, arrayCnumpoint, num_centroids)\n",
    "\n",
    "    return arrayC, arrayCsum, arrayCnumpoint\n",
    "\n",
    "\n",
    "def printCentroid(arrayC, arrayCsum, arrayCnumpoint):\n",
    "    for i in range(NUMBER_OF_CENTROIDS):\n",
    "        print(\n",
    "            \"[x={:6f}, y={:6f}, x_sum={:6f}, y_sum={:6f}, num_points={:d}]\".format(\n",
    "                arrayC[i, 0],\n",
    "                arrayC[i, 1],\n",
    "                arrayCsum[i, 0],\n",
    "                arrayCsum[i, 1],\n",
    "                arrayCnumpoint[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "def run_kmeans(\n",
    "    arrayP,\n",
    "    arrayPclusters,\n",
    "    arrayC,\n",
    "    arrayCsum,\n",
    "    arrayCnumpoint,\n",
    "    NUMBER_OF_POINTS,\n",
    "    NUMBER_OF_CENTROIDS,\n",
    "):\n",
    "\n",
    "    for i in range(REPEAT):\n",
    "        for i1 in range(NUMBER_OF_CENTROIDS):\n",
    "            arrayC[i1, 0] = arrayP[i1, 0]\n",
    "            arrayC[i1, 1] = arrayP[i1, 1]\n",
    "\n",
    "        arrayC, arrayCsum, arrayCnumpoint = kmeans(\n",
    "            arrayP,\n",
    "            arrayPclusters,\n",
    "            arrayC,\n",
    "            arrayCsum,\n",
    "            arrayCnumpoint,\n",
    "            NUMBER_OF_POINTS,\n",
    "            NUMBER_OF_CENTROIDS,\n",
    "        )\n",
    "\n",
    "    #     if i + 1 == REPEAT:\n",
    "    #         printCentroid(arrayC, arrayCsum, arrayCnumpoint)\n",
    "\n",
    "    # print(\"Iterations: {:d}\".format(ITERATIONS))\n",
    "    # print(\"Average Time: {:.4f} ms\".format(total))\n",
    "\n",
    "\n",
    "base_kmeans.run(\"Kmeans Numba\", run_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_kmeans_cpu.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_kmeans_cpu.sh; else ./run_kmeans_cpu.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of K-Means targeting GPU using Numba JIT\n",
    "\n",
    "In the following example, we introduce a naive K-Means implementation that targets a GPU using the Numba JIT, where we take an array representing M points in N dimensions, and return the M x M matrix of K-Means distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/kmeans_gpu.py\n",
    "\n",
    "import dpctl\n",
    "import numpy\n",
    "import base_kmeans_gpu\n",
    "import numba\n",
    "\n",
    "REPEAT = 1\n",
    "# defines total number of iterations for kmeans accuracy\n",
    "ITERATIONS = 30\n",
    "\n",
    "__njit = numba.jit(nopython=True, parallel=True, fastmath=True)\n",
    "\n",
    "# determine the euclidean distance from the cluster center to each point\n",
    "@__njit\n",
    "def groupByCluster(arrayP, arrayPcluster, arrayC, num_points, num_centroids):\n",
    "    # parallel for loop\n",
    "    for i0 in numba.prange(num_points):\n",
    "        minor_distance = -1\n",
    "        for i1 in range(num_centroids):\n",
    "            dx = arrayP[i0, 0] - arrayC[i1, 0]\n",
    "            dy = arrayP[i0, 1] - arrayC[i1, 1]\n",
    "            my_distance = numpy.sqrt(dx * dx + dy * dy)\n",
    "            if minor_distance > my_distance or minor_distance == -1:\n",
    "                minor_distance = my_distance\n",
    "                arrayPcluster[i0] = i1\n",
    "    return arrayPcluster\n",
    "\n",
    "\n",
    "# assign points to cluster\n",
    "@__njit\n",
    "def calCentroidsSum(\n",
    "    arrayP, arrayPcluster, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "):\n",
    "    # parallel for loop\n",
    "    for i in numba.prange(num_centroids):\n",
    "        arrayCsum[i, 0] = 0\n",
    "        arrayCsum[i, 1] = 0\n",
    "        arrayCnumpoint[i] = 0\n",
    "\n",
    "    for i in range(num_points):\n",
    "        ci = arrayPcluster[i]\n",
    "        arrayCsum[ci, 0] += arrayP[i, 0]\n",
    "        arrayCsum[ci, 1] += arrayP[i, 1]\n",
    "        arrayCnumpoint[ci] += 1\n",
    "\n",
    "    return arrayCsum, arrayCnumpoint\n",
    "\n",
    "\n",
    "# update the centriods array after computation\n",
    "@__njit\n",
    "def updateCentroids(arrayC, arrayCsum, arrayCnumpoint, num_centroids):\n",
    "    for i in numba.prange(num_centroids):\n",
    "        arrayC[i, 0] = arrayCsum[i, 0] / arrayCnumpoint[i]\n",
    "        arrayC[i, 1] = arrayCsum[i, 1] / arrayCnumpoint[i]\n",
    "\n",
    "\n",
    "def kmeans(\n",
    "    arrayP, arrayPcluster, arrayC, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "):\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        groupByCluster(arrayP, arrayPcluster, arrayC, num_points, num_centroids)\n",
    "\n",
    "        calCentroidsSum(\n",
    "            arrayP, arrayPcluster, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "        )\n",
    "\n",
    "        updateCentroids(arrayC, arrayCsum, arrayCnumpoint, num_centroids)\n",
    "\n",
    "    return arrayC, arrayCsum, arrayCnumpoint\n",
    "\n",
    "\n",
    "def run_kmeans(\n",
    "    arrayP,\n",
    "    arrayPclusters,\n",
    "    arrayC,\n",
    "    arrayCsum,\n",
    "    arrayCnumpoint,\n",
    "    NUMBER_OF_POINTS,\n",
    "    NUMBER_OF_CENTROIDS,\n",
    "):\n",
    "\n",
    "    with dpctl.device_context(base_kmeans_gpu.get_device_selector(is_gpu=True)):\n",
    "        for i in range(REPEAT):\n",
    "            for i1 in range(NUMBER_OF_CENTROIDS):\n",
    "                arrayC[i1, 0] = arrayP[i1, 0]\n",
    "                arrayC[i1, 1] = arrayP[i1, 1]\n",
    "\n",
    "            arrayC, arrayCsum, arrayCnumpoint = kmeans(\n",
    "                arrayP,\n",
    "                arrayPclusters,\n",
    "                arrayC,\n",
    "                arrayCsum,\n",
    "                arrayCnumpoint,\n",
    "                NUMBER_OF_POINTS,\n",
    "                NUMBER_OF_CENTROIDS,\n",
    "            )\n",
    "\n",
    "\n",
    "base_kmeans_gpu.run(\"Kmeans Numba\", run_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_kmeans_gpu.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_kmeans_gpu.sh; else ./run_kmeans_gpu.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Kmeans targeting GPU using Kernels\n",
    "\n",
    "## Writing Explicit Kernels in numba-dpex\n",
    "\n",
    "Writing a SYCL kernel using the `@numba_dpex.kernel` decorator has similar syntax to writing OpenCL kernels. As such, the numba-dpex module provides similar indexing and other functions as OpenCL. The indexing functions supported inside a `numba_dpex.kernel` are:\n",
    "\n",
    "* numba_dpex.get_local_id : Gets the local ID of the item\n",
    "* numba_dpex.get_local_size: Gets the local work group size of the device\n",
    "* numba_dpex.get_group_id : Gets the group ID of the item\n",
    "* numba_dpex.get_num_groups: Gets the number of gropus in a worksgroup\n",
    "\n",
    "Refer https://intelpython.github.io/numba-dpex/latest/user_guides/kernel_programming_guide/index.html for more details.\n",
    "\n",
    "In the following example we use the dpex-kernel approach for explicit kernel programming where, if the programmer wants to extract further performance from the offloaded code, the programmer can use the explicit kernel programming approach using dpex-kernels, and tune the GPU parameters where we take advantage of the work groups and the work items in a device using the kernel approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Kmeans targeting GPU using atomics\n",
    "Atomics allow multiple work-items for any cross work-item communication via memory. SYCL atomics are similar to C++ atomics and make the access to resources protected by atomics guaranteed to be executed as a single unit.\n",
    "\n",
    "Numba-dpex supports some of the atomic operations supported in SYCL. Those that are presently implemented are as follows:\n",
    "* add(ary, idx, val): Perform atomic ary[idx] += val. Returns the old value at the index location as if it is loaded atomically.\n",
    "\n",
    "* sub(ary, idx, val): Perform atomic ary[idx] -= val. Returns the old value at the index location as if it is loaded atomically.\n",
    "\n",
    "The following code shows the implementation of a reduction operation where every work-item is updating a global accumulator atomically\n",
    "\n",
    "Here’s an example of how to use atomics add in numba-dpex:\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/kmeans_kernel_atomic.py\n",
    "import dpctl\n",
    "import base_kmeans_gpu\n",
    "import numpy\n",
    "import numba_dpex as nb\n",
    "from numba_dpex import atomic\n",
    "\n",
    "REPEAT = 1\n",
    "ITERATIONS = 30\n",
    "\n",
    "atomic_add = atomic.add\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def groupByCluster(arrayP, arrayPcluster, arrayC, num_points, num_centroids):\n",
    "    idx = nb.get_global_id(0)\n",
    "    # if idx < num_points: # why it was removed??\n",
    "    minor_distance = -1\n",
    "    for i in range(num_centroids):\n",
    "        dx = arrayP[idx, 0] - arrayC[i, 0]\n",
    "        dy = arrayP[idx, 1] - arrayC[i, 1]\n",
    "        my_distance = numpy.sqrt(dx * dx + dy * dy)\n",
    "        if minor_distance > my_distance or minor_distance == -1:\n",
    "            minor_distance = my_distance\n",
    "            arrayPcluster[idx] = i\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def calCentroidsSum1(arrayCsum, arrayCnumpoint):\n",
    "    i = nb.get_global_id(0)\n",
    "    arrayCsum[i, 0] = 0\n",
    "    arrayCsum[i, 1] = 0\n",
    "    arrayCnumpoint[i] = 0\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def calCentroidsSum2(arrayP, arrayPcluster, arrayCsum, arrayCnumpoint):\n",
    "    i = nb.get_global_id(0)\n",
    "    ci = arrayPcluster[i]\n",
    "    atomic_add(arrayCsum, (ci, 0), arrayP[i, 0])\n",
    "    atomic_add(arrayCsum, (ci, 1), arrayP[i, 1])\n",
    "    atomic_add(arrayCnumpoint, ci, 1)\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def updateCentroids(arrayC, arrayCsum, arrayCnumpoint, num_centroids):\n",
    "    i = nb.get_global_id(0)\n",
    "    arrayC[i, 0] = arrayCsum[i, 0] / arrayCnumpoint[i]\n",
    "    arrayC[i, 1] = arrayCsum[i, 1] / arrayCnumpoint[i]\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def copy_arrayC(arrayC, arrayP):\n",
    "    i = nb.get_global_id(0)\n",
    "    arrayC[i, 0] = arrayP[i, 0]\n",
    "    arrayC[i, 1] = arrayP[i, 1]\n",
    "\n",
    "\n",
    "def kmeans(\n",
    "    arrayP, arrayPcluster, arrayC, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "):\n",
    "\n",
    "    copy_arrayC[num_centroids,](arrayC, arrayP)\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        groupByCluster[num_points,](\n",
    "            arrayP, arrayPcluster, arrayC, num_points, num_centroids\n",
    "        )\n",
    "\n",
    "        calCentroidsSum1[num_centroids,](\n",
    "            arrayCsum, arrayCnumpoint\n",
    "        )\n",
    "\n",
    "        calCentroidsSum2[num_points,](\n",
    "            arrayP, arrayPcluster, arrayCsum, arrayCnumpoint\n",
    "        )\n",
    "\n",
    "        updateCentroids[num_centroids,](\n",
    "            arrayC, arrayCsum, arrayCnumpoint, num_centroids\n",
    "        )\n",
    "\n",
    "    return arrayC, arrayCsum, arrayCnumpoint\n",
    "\n",
    "\n",
    "def run_kmeans(\n",
    "    arrayP,\n",
    "    arrayPclusters,\n",
    "    arrayC,\n",
    "    arrayCsum,\n",
    "    arrayCnumpoint,\n",
    "    NUMBER_OF_POINTS,\n",
    "    NUMBER_OF_CENTROIDS,\n",
    "):\n",
    "\n",
    "    with dpctl.device_context(base_kmeans_gpu.get_device_selector(is_gpu=True)):\n",
    "        for i in range(REPEAT):\n",
    "            arrayC, arrayCsum, arrayCnumpoint = kmeans(\n",
    "                arrayP,\n",
    "                arrayPclusters,\n",
    "                arrayC,\n",
    "                arrayCsum,\n",
    "                arrayCnumpoint,\n",
    "                NUMBER_OF_POINTS,\n",
    "                NUMBER_OF_CENTROIDS,\n",
    "            )\n",
    "\n",
    "\n",
    "base_kmeans_gpu.run(\"Kmeans Numba\", run_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_kmeans_atomic.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_kmeans_atomic.sh; else ./run_kmeans_atomic.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot GPU Results\n",
    "\n",
    "Below sample runs the Kmeans algorithm on the GPU and plots the first 10 centroids and the cluster of points each centroid is associated with.\n",
    "\n",
    "Here’s an example that runs the Kmeans algorithm using numba-dpex on a GPU and plots the centroids with the cluster of points:\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/kmeans_kernel_atomic_graph.py\n",
    "import dpctl\n",
    "import base_kmeans_gpu_graph\n",
    "import numpy\n",
    "import numba_dpex as nb\n",
    "from numba_dpex import atomic\n",
    "\n",
    "REPEAT = 1\n",
    "ITERATIONS = 30\n",
    "\n",
    "atomic_add = atomic.add\n",
    "\n",
    "@nb.kernel\n",
    "def groupByCluster(arrayP, arrayPcluster, arrayC, num_points, num_centroids):\n",
    "    idx = nb.get_global_id(0)\n",
    "    # if idx < num_points: # why it was removed??\n",
    "    minor_distance = -1\n",
    "    for i in range(num_centroids):\n",
    "        dx = arrayP[idx, 0] - arrayC[i, 0]\n",
    "        dy = arrayP[idx, 1] - arrayC[i, 1]\n",
    "        my_distance = numpy.sqrt(dx * dx + dy * dy)\n",
    "        if minor_distance > my_distance or minor_distance == -1:\n",
    "            minor_distance = my_distance\n",
    "            arrayPcluster[idx] = i\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def calCentroidsSum1(arrayCsum, arrayCnumpoint):\n",
    "    i = nb.get_global_id(0)\n",
    "    arrayCsum[i, 0] = 0\n",
    "    arrayCsum[i, 1] = 0\n",
    "    arrayCnumpoint[i] = 0\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def calCentroidsSum2(arrayP, arrayPcluster, arrayCsum, arrayCnumpoint):\n",
    "    i = nb.get_global_id(0)\n",
    "    ci = arrayPcluster[i]\n",
    "    atomic_add(arrayCsum, (ci, 0), arrayP[i, 0])\n",
    "    atomic_add(arrayCsum, (ci, 1), arrayP[i, 1])\n",
    "    atomic_add(arrayCnumpoint, ci, 1)\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def updateCentroids(arrayC, arrayCsum, arrayCnumpoint, num_centroids):\n",
    "    i = nb.get_global_id(0)\n",
    "    arrayC[i, 0] = arrayCsum[i, 0] / arrayCnumpoint[i]\n",
    "    arrayC[i, 1] = arrayCsum[i, 1] / arrayCnumpoint[i]\n",
    "\n",
    "\n",
    "@nb.kernel\n",
    "def copy_arrayC(arrayC, arrayP):\n",
    "    i = nb.get_global_id(0)\n",
    "    arrayC[i, 0] = arrayP[i, 0]\n",
    "    arrayC[i, 1] = arrayP[i, 1]\n",
    "\n",
    "\n",
    "def kmeans(\n",
    "    arrayP, arrayPcluster, arrayC, arrayCsum, arrayCnumpoint, num_points, num_centroids\n",
    "):\n",
    "\n",
    "    copy_arrayC[num_centroids,](arrayC, arrayP)\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        groupByCluster[num_points,](\n",
    "            arrayP, arrayPcluster, arrayC, num_points, num_centroids\n",
    "        )\n",
    "\n",
    "        calCentroidsSum1[num_centroids,](\n",
    "            arrayCsum, arrayCnumpoint\n",
    "        )\n",
    "\n",
    "        calCentroidsSum2[num_points,](\n",
    "            arrayP, arrayPcluster, arrayCsum, arrayCnumpoint\n",
    "        )\n",
    "\n",
    "        updateCentroids[num_centroids,](\n",
    "            arrayC, arrayCsum, arrayCnumpoint, num_centroids\n",
    "        )\n",
    "\n",
    "    return arrayC, arrayCsum, arrayCnumpoint\n",
    "\n",
    "\n",
    "def run_kmeans(\n",
    "    arrayP,\n",
    "    arrayPclusters,\n",
    "    arrayC,\n",
    "    arrayCsum,\n",
    "    arrayCnumpoint,\n",
    "    NUMBER_OF_POINTS,\n",
    "    NUMBER_OF_CENTROIDS,\n",
    "):\n",
    "\n",
    "    with dpctl.device_context(base_kmeans_gpu_graph.get_device_selector(is_gpu=True)):\n",
    "        for i in range(REPEAT):\n",
    "            arrayC, arrayCsum, arrayCnumpoint = kmeans(\n",
    "                arrayP,\n",
    "                arrayPclusters,\n",
    "                arrayC,\n",
    "                arrayCsum,\n",
    "                arrayCnumpoint,\n",
    "                NUMBER_OF_POINTS,\n",
    "                NUMBER_OF_CENTROIDS,\n",
    "            )\n",
    "\n",
    "\n",
    "base_kmeans_gpu_graph.run(\"Kmeans Numba\", run_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_kmeans_atomic_graph.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_kmeans_atomic_graph.sh; else ./run_kmeans_atomic_graph.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### View the results\n",
    "Select the cell below and click run ▶ to view the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "def read_dictionary(fn):\n",
    "    import joblib\n",
    "    # Load data (deserialize)\n",
    "    with open(fn, 'rb') as handle:\n",
    "        dictionary = joblib.load(handle)\n",
    "    return dictionary\n",
    "resultsDict = read_dictionary('resultsDict.dat')\n",
    "limit = 10\n",
    "\n",
    "arrayP = resultsDict['arrayP']\n",
    "arrayPclusters = resultsDict['arrayPclusters']\n",
    "# print(arrayP.shape)\n",
    "# print(arrayPclusters.shape)\n",
    "#y = resultsDict['y']\n",
    "xC = resultsDict['xC']\n",
    "yC = resultsDict['yC']\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# columns = ['x', 'y']\n",
    "# df = pd.DataFrame(X, columns = columns)\n",
    "# df['color'] = resultsDict['dbscan.labels_']\n",
    "# colors = { 0: 'magenta', 1: 'lime', -1: 'b' }\n",
    "\n",
    "cluster0 = np.where(arrayPclusters==0)\n",
    "cluster1 = np.where(arrayPclusters==1)\n",
    "cluster2 = np.where(arrayPclusters==2)\n",
    "cluster3 = np.where(arrayPclusters==3)\n",
    "cluster4 = np.where(arrayPclusters==4)\n",
    "cluster5 = np.where(arrayPclusters==5)\n",
    "cluster6 = np.where(arrayPclusters==6)\n",
    "cluster7 = np.where(arrayPclusters==7)\n",
    "cluster8 = np.where(arrayPclusters==8)\n",
    "cluster9 = np.where(arrayPclusters==9)\n",
    "# arrayP[cluster0]\n",
    "# arrayP[cluster1]\n",
    "# arrayP[cluster2]\n",
    "# arrayP[cluster3]\n",
    "plt.style.use('default')\n",
    "\n",
    "plt.scatter(x=arrayP[cluster0][:,0], y=arrayP[cluster0][:,1], c='gold',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster1][:,0], y=arrayP[cluster1][:,1], c='magenta',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster2][:,0], y=arrayP[cluster2][:,1], c='cyan',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster3][:,0], y=arrayP[cluster3][:,1], c='green',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster4][:,0], y=arrayP[cluster4][:,1], c='y',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster5][:,0], y=arrayP[cluster5][:,1], c='r',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster6][:,0], y=arrayP[cluster6][:,1], c='purple',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster7][:,0], y=arrayP[cluster7][:,1], c='blue',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster8][:,0], y=arrayP[cluster8][:,1], c='green',alpha=0.08)\n",
    "plt.scatter(x=arrayP[cluster9][:,0], y=arrayP[cluster9][:,1], c='orange',alpha=0.08)\n",
    "#plt.scatter(x=x, y=y, c='lime',alpha=0.05)\n",
    "\n",
    "plt.scatter(x=xC[:10], y=yC[:10],s=75,  c='r', edgecolor=\"k\")\n",
    "plt.title('Kmeans centroids')\n",
    "\n",
    "#plt.grid()\n",
    "plt.gcf().set_size_inches((16, 8))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Advsior Roofline Report\n",
    "\n",
    "A Roofline chart is a visual representation of application performance in relation to hardware limitations, including memory bandwidth and computational peaks.  Intel Advisor includes an automated Roofline tool that measures and plots the chart on its own, so all you need to do is read it.\n",
    "\n",
    "The chart can be used to identify not only where bottlenecks exist, but what’s likely causing them, and which ones will provide the most speedup if optimized.\n",
    "\n",
    "The Survey is usually the first analysis you want to run with Intel® Advisor. The survey is mainly used to time your application as well as the different loops and functions. \n",
    "\n",
    "The second step is to run the trip count analysis. This step uses instrumentation to count how many iterations you are running in each loops. Adding the option -flop will also provide the precise number of operations executed in each of your code sections.\n",
    "\n",
    "Execute the following line to display the roofline results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the Survey analysis with the --profile-gpu option\n",
    "\n",
    "```\n",
    "advisor --collect=survey --profile-gpu -run-pass-thru=--no-altstack -project-dir=roofline --search-dir src:r=. python lab/kmeans_kernel_atomic.py --steps 1 --size 16384 --repeat 5 --json result_gpu.json --usm -d 3\n",
    "```\n",
    "* Run the Trip Counts and FLOP analysis with --profile-gpu option:\n",
    "\n",
    "```\n",
    "advisor --collect=tripcounts --profile-gpu --project-dir=roofline \"--search-dir src:r=.\" --flop --no-trip-counts python lab/kmeans_kernel_atomic.py --steps 1 --size 16384 --repeat 5 --json result_gpu.json --usm -d 3\n",
    "```\n",
    "* Generate a GPU Roofline report:\n",
    "\n",
    "```\n",
    "advisor --report=roofline --gpu --project-dir=roofline --report-output=roofline/roofline.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Advisor Roofline Report\n",
    "\n",
    "Execute the following line to display the roofline results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run lab/mm_basic_roofline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Vtune reports\n",
    "Below exercises we use VTune™  analyzer as a way to see what is going on with each implementation. The information was the high-level hotspot generated from the collection and rendered in an HTML iframe. Depending on the options chosen, many of the VTune analyzer's performance collections can be rendered via HTML pages. The below vtune scripts collect GPU offload and GPU hotspots information.\n",
    "\n",
    "#### Learn more about VTune\n",
    "​\n",
    "There is extensive training on VTune, click [here](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/vtune-profiler.html#gs.2xmez3) to get deep dive training.\n",
    "\n",
    "```\n",
    "vtune -run-pass-thru=--no-altstack -collect=gpu-offload -result-dir=vtune_dir python lab/kmeans_kernel_atomic.py --steps 1 --size 16384 --repeat 5 --json result_gpu.json\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -run-pass-thru=--no-altstack -collect=gpu-hotspots -result-dir=vtune_hotspots_dir_new python lab/kmeans_kernel_atomic.py --steps 1 --size 16384 --repeat 5 --json result_gpu.json\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -report summary -result-dir vtune_dir -format html -report-output output.html\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -report summary -result-dir vtune_hotspots_dir -format html -report-output output_hotspots.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run lab/mm_basic_vtune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this module you will have learned the following:\n",
    "* Numba implementation of K-Means targeting a CPU and GPU using Numba JIT\n",
    "* Numba-dpex implementation of K-Means on a CPU and GPU using the kernel approach\n",
    "* Numba-dpex  implementation of K-means on a GPU using Atomics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.1)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.09px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
