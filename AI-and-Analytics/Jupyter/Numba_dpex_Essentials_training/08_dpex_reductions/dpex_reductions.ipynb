{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reductions and Local memory in numba-dpex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning Objectives\n",
    "- Use ND-range to show improvement in parallelism over the basic implementation.\n",
    "- Use local memory to avoid repeated global memory access \n",
    "- Understand the usage of group barriers to synchronize all work-items and perform reduction\n",
    "- Use atomic operation to perform reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reductions\n",
    "A __reduction produces a single value by combining multiple values__ in an unspecified order, using an operator that is both associative and commutative (e.g. addition). Only the final value resulting from a reduction is of interest to the programmer.\n",
    "\n",
    "A very common example is calculating __sum__ by adding a bunch of values. other examples are maximum and minumum\n",
    "\n",
    "Parallelizing reductions can be tricky because of the nature of computation and accelerator hardware. Let's look at code examples showing how reduction can be performed on GPU using kernel invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduction_kernel.py\n",
    "# SPDX-FileCopyrightText: 2020 - 2023 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import math\n",
    "\n",
    "import dpctl\n",
    "import numpy as np\n",
    "\n",
    "import numba_dpex as ndpx\n",
    "\n",
    "\n",
    "@ndpx.kernel\n",
    "def sum_reduction_kernel(A, R, stride):\n",
    "    i = ndpx.get_global_id(0)\n",
    "    # sum two element\n",
    "    R[i] = A[i] + A[i + stride]\n",
    "    # store the sum to be used in nex iteration\n",
    "    A[i] = R[i]\n",
    "\n",
    "\n",
    "def sum_reduce(A):\n",
    "    \"\"\"Size of A should be power of two.\"\"\"\n",
    "    total = len(A)\n",
    "    # max size will require half the size of A to store sum\n",
    "    R = np.array(np.random.random(math.ceil(total / 2)), dtype=A.dtype)\n",
    "\n",
    "    # Use the environment variable SYCL_DEVICE_FILTER to change the default device.\n",
    "    # See https://github.com/intel/llvm/blob/sycl/sycl/doc/EnvironmentVariables.md#sycl_device_filter.\n",
    "    device = dpctl.select_default_device()\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "\n",
    "    with dpctl.device_context(device):\n",
    "        while total > 1:\n",
    "            global_size = total // 2\n",
    "            sum_reduction_kernel[ndpx.Range(global_size)](A, R, global_size)\n",
    "            total = total // 2\n",
    "\n",
    "    return R[0]\n",
    "\n",
    "\n",
    "def test_sum_reduce():\n",
    "    # This test will only work for size = power of two\n",
    "    N = 2048\n",
    "    #assert N % 2 == 0\n",
    "\n",
    "    A = np.array(np.random.random(N), dtype=np.float32)\n",
    "    A_copy = A.copy()\n",
    "\n",
    "    actual = sum_reduce(A)\n",
    "    expected = A_copy.sum()\n",
    "\n",
    "    print(\"Actual:  \", actual)\n",
    "    print(\"Expected:\", expected)\n",
    "\n",
    "    #assert expected - actual < 1e-2\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum_reduce()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_reduction_kernel.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_reduction_kernel.sh; else ./run_reduction_kernel.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## ND-Range Kernels\n",
    "\n",
    "Naive parallel kernel do not allow for performance optimizations at a hardware level. In these next two kernels we will utilize ND-Range kernels as a way to expresses parallelism enabling low level performance tuning by providing access to both global and local memory and mapping executions to compute units on hardware. The entire iteration space is divided into smaller groups called work-groups, work-items are organized into these work-groups and are scheduled on a single compute unit on the hardware.  Workgroup size must divide the entire ND-range size exactly in each dimension.  These sizes can all vary by hardware platform and by using the device queries below a developer can identify what is possible.  The workload must be considered to find the best mix of these values.\n",
    "\n",
    "<img src=\"Assets/ndrange-subgroup.png\">\n",
    "\n",
    "The grouping of kernel executions into work-groups allows control of resource usage and load balance work distribution. The functionality of nd_range kernels is exposed via nd_range and nd_item classes. nd_range class represents a grouped execution range using global execution range and the local execution range of each work-group. nd_item class represents an individual instance of a kernel function and allows you to query for work-group range and index.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Assets/ndrange.png\">\n",
    "\n",
    "The work-group size depends on the accelerator hardware capability, so we set this size using command-line argument. Some hardware requre the matrix size to divide equally by the work-group size, we will use work-group size of 16x16 (256) by default which works for all the accelerator hardware we will be using to test, we will eventually use different work-group sizes to see how it impacts the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shared Local Memory (SLM) Implementation\n",
    "\n",
    "In a parallel algorithm, there is a high degree of reuse, so instead of loading values from global memory each time we can load the values into local memory and perform the computation.  This will reduce the latency of accessing the data values.  The difference between this implementation and the ND-range implementation is the reading is done from global memory in the case of ND-range each time and in this implementation they dat is loaded into local memory and then computed.  \n",
    "\n",
    "When a work-group begins, the contents of its local memory are uninitialized, and local memory does not persist after a work-group finishes executing. Because of these properties, local memory may only be used for temporary storage while a work-group is executing.  For other devices though, such as many GPU devices, there are dedicated resources for local memory, and on these devices, communicating via local memory should perform better than communicating via global memory.\n",
    "\n",
    "In SYCL’s memory model, local memory is a contiguous region of memory allocated per work group and is visible to all the work items in that group. Local memory is device-only and cannot be accessed from the host. From the perspective offers the device, the local memory is exposed as a contiguous array of a specific types. The maximum available local memory is hardware-specific. The SYCL local memory concept is analogous to CUDA’s shared memory concept.\n",
    "\n",
    "Numba-dpex provides a special function numba_dpex.local.array to allocate local memory for a kernel. To simplify kernel development and accelerate communication between work-items in a work-group, SYCL defines a special local memory space specifically for communication between work-items in a work-group.\n",
    "\n",
    "Local Address Space refers to memory objects that need to be allocated in local memory pool and are shared by all work-items of a work-group. Numba-dpex does not support passing arguments that are allocated in the local address space to @numba_dpex.kernel. Users are allowed to allocate static arrays in the local address space inside the @numba_dpex.kernel. In the example below numba_dpex.local.array(shape, dtype) is the API used to allocate a static array in the local address space:\n",
    "These are used to compute an intermediate result which does not use global memory for repeated access for computation. \n",
    "\n",
    "Also notice that we used a barrier that helps to synchronize all of the work-items in the work-group. \n",
    "\n",
    "<img src=\"Assets/localmem.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Barrier\n",
    "\n",
    "When local accessor data is shared, work-group barriers are often required for work-item synchronization.\n",
    "\n",
    "The `group_barrier` function synchronizes how each work-item views the state of memory. This type of synchronization operation is known as enforcing memory consistency or fencing memory. It ensures that the results of memory operations performed before the barrier are visible to other work-items after the\n",
    "barrier.\n",
    "\n",
    "A `group_barrier` is usually required right after a local accessor is modified by a work-item so that it is synchronized for all work-items before the local accessor can be accessed.\n",
    "\n",
    "Below is an example of how to use local memory with barriers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local Memory sample\n",
    "The following DPC++ code shows a local-memory implementation of matrix multiplication: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/local_memory_kernel.py\n",
    "##==============================================================\n",
    "## Copyright © Intel Corporation\n",
    "##\n",
    "## SPDX-License-Identifier: Apache-2.0\n",
    "## =============================================================\n",
    "\n",
    "import dpctl\n",
    "import numpy as np\n",
    "from numba import float32\n",
    "\n",
    "import numba_dpex as dpex\n",
    "\n",
    "\n",
    "def no_arg_barrier_support():\n",
    "    \"\"\"\n",
    "    This example demonstrates the usage of numba_dpex's ``barrier``\n",
    "    intrinsic function. The ``barrier`` function is usable only inside\n",
    "    a ``kernel`` and is equivalent to OpenCL's ``barrier`` function.\n",
    "    \"\"\"\n",
    "\n",
    "    @dpex.kernel\n",
    "    def twice(A):\n",
    "        i = dpex.get_global_id(0)\n",
    "        d = A[i]\n",
    "        # no argument defaults to global mem fence\n",
    "        dpex.barrier()\n",
    "        A[i] = d * 2\n",
    "\n",
    "    N = 10\n",
    "    arr = np.arange(N).astype(np.float32)\n",
    "    print(arr)\n",
    "\n",
    "    # Use the environment variable SYCL_DEVICE_FILTER to change the default device.\n",
    "    # See https://github.com/intel/llvm/blob/sycl/sycl/doc/EnvironmentVariables.md#sycl_device_filter.\n",
    "    device = dpctl.select_default_device()\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "\n",
    "    with dpctl.device_context(device):\n",
    "        twice[N, dpex.DEFAULT_LOCAL_SIZE](arr)\n",
    "\n",
    "    # the output should be `arr * 2, i.e. [0, 2, 4, 6, ...]`\n",
    "    print(arr)\n",
    "\n",
    "\n",
    "def local_memory():\n",
    "    \"\"\"\n",
    "    This example demonstrates the usage of numba-dpex's `local.array`\n",
    "    intrinsic function. The function is used to create a static array\n",
    "    allocated on the devices local address space.\n",
    "    \"\"\"\n",
    "    blocksize = 10\n",
    "\n",
    "    @dpex.kernel\n",
    "    def reverse_array(A):\n",
    "        lm = dpex.local.array(shape=10, dtype=float32)\n",
    "        i = dpex.get_global_id(0)\n",
    "\n",
    "        # preload\n",
    "        lm[i] = A[i]\n",
    "        # barrier local or global will both work as we only have one work group\n",
    "        dpex.barrier(dpex.LOCAL_MEM_FENCE)  # local mem fence\n",
    "        # write\n",
    "        A[i] += lm[blocksize - 1 - i]\n",
    "\n",
    "    arr = np.arange(blocksize).astype(np.float32)\n",
    "    print(arr)\n",
    "\n",
    "    # Use the environment variable SYCL_DEVICE_FILTER to change the default device.\n",
    "    # See https://github.com/intel/llvm/blob/sycl/sycl/doc/EnvironmentVariables.md#sycl_device_filter.\n",
    "    device = dpctl.select_default_device()\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "\n",
    "    with dpctl.device_context(device):\n",
    "        reverse_array[blocksize, dpex.DEFAULT_LOCAL_SIZE](arr)\n",
    "\n",
    "    # the output should be `orig[::-1] + orig, i.e. [9, 9, 9, ...]``\n",
    "    print(arr)\n",
    "\n",
    "\n",
    "def main():\n",
    "    no_arg_barrier_support()\n",
    "    local_memory()\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_local_memory.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_local_memory.sh; else ./run_local_memory.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reductions using Local memory and local barriers\n",
    "One popular way of doing a reduction operation on GPUs is to create a number of work-groups and do a tree reduction in each work-group. In the kernel shown below, each work-item in the work-group participates in a reduction network to eventually sum up all the elements in that work-group.\n",
    "\n",
    "All the intermediate results from the work-groups are then summed up by doing a serial reduction (if this intermediate set of results is large enough then we can do few more round(s) of tree reductions). This tree reduction algorithm takes advantage of the very fast synchronization operations among the work-items in a work-group. The performance of this kernel is highly dependent on the efficiency of the kernel launches, because a large number of kernels are launched\n",
    "\n",
    "The following code shows a local-memory implementation of Reductions: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/reduce_local_memory.py\n",
    "# SPDX-FileCopyrightText: 2020 - 2023 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import dpctl\n",
    "import dpctl.tensor as dpt\n",
    "from numba import int32\n",
    "\n",
    "import numba_dpex as ndpx\n",
    "\n",
    "\n",
    "@ndpx.kernel\n",
    "def sum_reduction_kernel(A, partial_sums):\n",
    "    \"\"\"\n",
    "    The example demonstrates a reduction kernel implemented as a ``kernel``\n",
    "    function.\n",
    "    \"\"\"\n",
    "    local_id = ndpx.get_local_id(0)\n",
    "    global_id = ndpx.get_global_id(0)\n",
    "    group_size = ndpx.get_local_size(0)\n",
    "    group_id = ndpx.get_group_id(0)\n",
    "\n",
    "    local_sums = ndpx.local.array(64, int32)\n",
    "\n",
    "    # Copy from global to local memory\n",
    "    local_sums[local_id] = A[global_id]\n",
    "\n",
    "    # Loop for computing local_sums : divide workgroup into 2 parts\n",
    "    stride = group_size // 2\n",
    "    while stride > 0:\n",
    "        # Waiting for each 2x2 addition into given workgroup\n",
    "        ndpx.barrier(ndpx.LOCAL_MEM_FENCE)\n",
    "\n",
    "        # Add elements 2 by 2 between local_id and local_id + stride\n",
    "        if local_id < stride:\n",
    "            local_sums[local_id] += local_sums[local_id + stride]\n",
    "\n",
    "        stride >>= 1\n",
    "\n",
    "    if local_id == 0:\n",
    "        partial_sums[group_id] = local_sums[0]\n",
    "\n",
    "\n",
    "def sum_reduce(A):\n",
    "    global_size = len(A)\n",
    "    work_group_size = 64\n",
    "    # nb_work_groups have to be even for this implementation\n",
    "    nb_work_groups = global_size // work_group_size\n",
    "\n",
    "    partial_sums = dpt.zeros(nb_work_groups, dtype=A.dtype, device=A.device)\n",
    "\n",
    "    gs = ndpx.Range(global_size)\n",
    "    ls = ndpx.Range(work_group_size)\n",
    "    sum_reduction_kernel[ndpx.NdRange(gs, ls)](A, partial_sums)\n",
    "\n",
    "    final_sum = 0\n",
    "    # calculate the final sum in HOST\n",
    "    for i in range(nb_work_groups):\n",
    "        final_sum += int(partial_sums[i])\n",
    "\n",
    "    return final_sum\n",
    "\n",
    "\n",
    "def test_sum_reduce():\n",
    "    N = 1024\n",
    "    device = dpctl.select_default_device()\n",
    "    A = dpt.ones(N, dtype=dpt.int32, device=device)\n",
    "\n",
    "    print(\"Running Device + Host reduction\")\n",
    "\n",
    "    actual = sum_reduce(A)\n",
    "    expected = N\n",
    "\n",
    "    print(\"Actual:  \", actual)\n",
    "    print(\"Expected:\", expected)\n",
    "\n",
    "    assert actual == expected\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 reduce_local_memory.sh; if [ -x \"$(command -v qsub)\" ]; then ./q reduce_local_memory.sh; else ./reduce_local_memory.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Atomic Operations\n",
    "\n",
    "Atomic operations enable __concurrent access to a memory location without introducing a data race__. When multiple atomic operations access the same memory, they are guaranteed not to overlap. \n",
    "\n",
    "To understand why atomic operations are necessary, let look at few kernel examples to perform reduction, addition of N number of elements:\n",
    "\n",
    "#### Serial Computation with single_task\n",
    "A simple way to perform reduction is by using a for-loop to add all items in a single_task kernel submission as show below, but it does not take advantage of parallelism in hardware.\n",
    "```cpp\n",
    "     for i in range(N):\n",
    "        sum += data[i]\n",
    "```\n",
    "\n",
    "#### Parallel Computation with parallel_for may encounter race conditions\n",
    "Using parallel_for for kernel submission will enable multiple work-items to execute concurrently but multiple work-item may try to update the same output variable causing __race conditions__.\n",
    "\n",
    "Parallel Computation with atomic operation avoid race conditions when multiple work-items are trying to update the same memory location\n",
    "\n",
    "The following code shows Atomics implementation of Reductions: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/atomics_kernel.py\n",
    "import dpnp as np\n",
    "\n",
    "import numba_dpex as ndpx\n",
    "\n",
    "\n",
    "@ndpx.kernel\n",
    "def atomic_reduction(a):\n",
    "    idx = ndpx.get_global_id(0)\n",
    "    ndpx.atomic.add(a, 0, a[idx])\n",
    "\n",
    "\n",
    "def main():\n",
    "    N = 1024\n",
    "    a = np.arange(N)\n",
    "\n",
    "    print(\"Using device ...\")\n",
    "    print(a.device)\n",
    "\n",
    "    atomic_reduction[ndpx.Range(N)](a)\n",
    "    print(\"Reduction sum =\", a[0])\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_atomics.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_atomics.sh; else ./run_atomics.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Reduction\n",
    "\n",
    "There are multiple ways of implementing reduction using numba_dpex. Here we demonstrate another way of implementing reduction using recursion to compute partial reductions in separate kernels.\n",
    "\n",
    "The following code shows a local-memory implementation of Recursive Reductions: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lab/recursive_reduction_kernel.py\n",
    "# SPDX-FileCopyrightText: 2020 - 2023 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "\"\"\"\n",
    "There are multiple ways of implementing reduction using numba_ndpx. Here we\n",
    "demonstrate another way of implementing reduction using recursion to compute\n",
    "partial reductions in separate kernels.\n",
    "\"\"\"\n",
    "\n",
    "import dpctl\n",
    "import dpctl.tensor as dpt\n",
    "from numba import int32\n",
    "\n",
    "import numba_dpex as ndpx\n",
    "\n",
    "\n",
    "@ndpx.kernel\n",
    "def sum_reduction_kernel(A, input_size, partial_sums):\n",
    "    local_id = ndpx.get_local_id(0)\n",
    "    global_id = ndpx.get_global_id(0)\n",
    "    group_size = ndpx.get_local_size(0)\n",
    "    group_id = ndpx.get_group_id(0)\n",
    "\n",
    "    local_sums = ndpx.local.array(64, int32)\n",
    "\n",
    "    local_sums[local_id] = 0\n",
    "\n",
    "    if global_id < input_size:\n",
    "        local_sums[local_id] = A[global_id]\n",
    "\n",
    "    # Loop for computing local_sums : divide workgroup into 2 parts\n",
    "    stride = group_size // 2\n",
    "    while stride > 0:\n",
    "        # Waiting for each 2x2 addition into given workgroup\n",
    "        ndpx.barrier(ndpx.LOCAL_MEM_FENCE)\n",
    "\n",
    "        # Add elements 2 by 2 between local_id and local_id + stride\n",
    "        if local_id < stride:\n",
    "            local_sums[local_id] += local_sums[local_id + stride]\n",
    "\n",
    "        stride >>= 1\n",
    "\n",
    "    if local_id == 0:\n",
    "        partial_sums[group_id] = local_sums[0]\n",
    "\n",
    "\n",
    "def sum_recursive_reduction(size, group_size, Dinp, Dpartial_sums):\n",
    "    result = 0\n",
    "    nb_work_groups = 0\n",
    "    passed_size = size\n",
    "\n",
    "    if size <= group_size:\n",
    "        nb_work_groups = 1\n",
    "    else:\n",
    "        nb_work_groups = size // group_size\n",
    "        if size % group_size != 0:\n",
    "            nb_work_groups += 1\n",
    "            passed_size = nb_work_groups * group_size\n",
    "\n",
    "    gr = ndpx.Range(passed_size)\n",
    "    lr = ndpx.Range(group_size)\n",
    "\n",
    "    sum_reduction_kernel[ndpx.NdRange(gr, lr)](Dinp, size, Dpartial_sums)\n",
    "\n",
    "    if nb_work_groups <= group_size:\n",
    "        sum_reduction_kernel[ndpx.NdRange(lr, lr)](\n",
    "            Dpartial_sums, nb_work_groups, Dinp\n",
    "        )\n",
    "        result = int(Dinp[0])\n",
    "    else:\n",
    "        result = sum_recursive_reduction(\n",
    "            nb_work_groups, group_size, Dpartial_sums, Dinp\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def sum_reduce(A):\n",
    "    global_size = len(A)\n",
    "    work_group_size = 64\n",
    "    nb_work_groups = global_size // work_group_size\n",
    "    if (global_size % work_group_size) != 0:\n",
    "        nb_work_groups += 1\n",
    "\n",
    "    partial_sums = dpt.zeros(nb_work_groups, dtype=A.dtype, device=A.device)\n",
    "    result = sum_recursive_reduction(\n",
    "        global_size, work_group_size, A, partial_sums\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def test_sum_reduce():\n",
    "    N = 20000\n",
    "    device = dpctl.select_default_device()\n",
    "    A = dpt.ones(N, dtype=dpt.int32, device=device)\n",
    "\n",
    "    print(\"Running recursive reduction\")\n",
    "\n",
    "    actual = sum_reduce(A)\n",
    "    expected = N\n",
    "\n",
    "    print(\"Actual:  \", actual)\n",
    "    print(\"Expected:\", expected)\n",
    "\n",
    "    assert actual == expected\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum_reduce()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_recursive_reduction.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_recursive_reduction.sh; else ./run_recursive_reduction.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Private Address Space\n",
    "Private Address Space refers to memory objects that are local to each work-item and is not shared with any other work-item. In the example below numba_dpex.private.array(shape, dtype) is the API used to allocate a static array in the private address space:\n",
    "\n",
    "<img src=\"Assets/workgroup.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Private Memory sample\n",
    "\n",
    "The following code shows a Private-memory implementation of numba-dpex: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/private_memory_kernel.py\n",
    "# SPDX-FileCopyrightText: 2020 - 2023 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import dpctl\n",
    "import dpctl.tensor as dpt\n",
    "import numpy as np\n",
    "from numba import float32\n",
    "\n",
    "import numba_dpex as ndpx\n",
    "\n",
    "\n",
    "def private_memory():\n",
    "    \"\"\"\n",
    "    This example demonstrates the usage of numba_dpex's `private.array`\n",
    "    intrinsic function. The function is used to create a static array\n",
    "    allocated on the devices private address space.\n",
    "    \"\"\"\n",
    "\n",
    "    @ndpx.kernel\n",
    "    def private_memory_kernel(A):\n",
    "        memory = ndpx.private.array(shape=1, dtype=np.float32)\n",
    "        i = ndpx.get_global_id(0)\n",
    "\n",
    "        # preload\n",
    "        memory[0] = i\n",
    "        ndpx.barrier(ndpx.LOCAL_MEM_FENCE)  # local mem fence\n",
    "\n",
    "        # memory will not hold correct deterministic result if it is not\n",
    "        # private to each thread.\n",
    "        A[i] = memory[0] * 2\n",
    "\n",
    "    N = 4\n",
    "    device = dpctl.select_default_device()\n",
    "\n",
    "    arr = dpt.zeros(N, dtype=dpt.float32, device=device)\n",
    "    orig = np.arange(N).astype(np.float32)\n",
    "\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "\n",
    "    global_range = ndpx.Range(N)\n",
    "    local_range = ndpx.Range(N)\n",
    "    private_memory_kernel[ndpx.NdRange(global_range, local_range)](arr)\n",
    "\n",
    "    arr_out = dpt.asnumpy(arr)\n",
    "    np.testing.assert_allclose(orig * 2, arr_out)\n",
    "    # the output should be `orig[i] * 2, i.e. [0, 2, 4, ..]``\n",
    "    print(arr_out)\n",
    "\n",
    "\n",
    "def main():\n",
    "    private_memory()\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_private_memory.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_private_memory.sh; else ./run_private_memory.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this module you will have learned the following:\n",
    "* Implementation of reduction operation using Local memory in numba-dpex\n",
    "* Implementation of reduction operation using atomics in numba-dpex\n",
    "* Implementation of Private memory in numba-dpex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.1)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.09px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
