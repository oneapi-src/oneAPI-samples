{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPAIRS Algorithm using Numba-dpex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Sections\n",
    "- [Gpairs algorithm](#Gpairs-algorithm)\n",
    "- _Code:_ [Implementation of Gpairs distance targeting CPU using Numba JIT](#Implementation-of-Gpairs-targeting-CPU-using-Numba-JIT)\n",
    "- _Code:_ [Implementation of GPairs targeting GPU using Kernels](#Implementation-of-Gpairs-targeting-GPU-using-Kernel)\n",
    "- _Code:_ [Plot the results for Gpairs on GPU](#Plot-the-results-for-Gpairs-on-GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* Build a Numba implementation of Gpairs targeting CPU and GPU using Numba Jit\n",
    "* Build a  Numba-dpex  implementation of Gpairs on CPU and GPU using Kernel approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numba-dpex\n",
    "\n",
    "Numba-dpex is a standalone extension to the Numba JIT compiler that adds SYCL programming capabilities to Numba. Numba-dpex is packaged as part of the IDP that comes with oneAPI base toolkit, and you don’t need to install any specific Conda packages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpairs algorithm\n",
    "The Gpairs distance application takes a set of multidimensional points and computes the Euclidean distance between every pair of points. For n observations, a common sub-task of different data analysis algorithms is to compute the symmetric matrix of distances between each pair of observations.\n",
    "\n",
    "The algorithm Naively counts Npairs(<r), the total number of pairs that are separated by a distance less than r, for each r**2 in the input rbins_squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Gpairs targeting CPU using Numba JIT\n",
    "In the following example, we introduce to a Gapirs pairwise distance implementation that targets a CPU using the Numba JIT.\n",
    "\n",
    "This is the decorator-based approach, where we offload data parallel code sections like parallel-for, and certain NumPy function calls. With the decorator method, a programmer needs to simply identify the most time-consuming parts of the program. If those parts can be parallelized, the programmer needs to just annotate those sections using Numba-dpex, and can expect those code sections to execute on a GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpairs.py\n",
    "\n",
    "# Copyright (C) 2017-2018 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "import base_gpairs\n",
    "import numpy as np\n",
    "from gaussian_weighted_pair_counts import count_weighted_pairs_3d_cpu\n",
    "\n",
    "def run_gpairs(x1, y1, z1, w1, x2, y2, z2, w2, d_rbins_squared):\n",
    "    x1 = x1.astype(np.float32)\n",
    "    y1 = y1.astype(np.float32)\n",
    "    z1 = z1.astype(np.float32)\n",
    "    w1 = w1.astype(np.float32)\n",
    "    x2 = x2.astype(np.float32)\n",
    "    y2 = y2.astype(np.float32)\n",
    "    z2 = z2.astype(np.float32)\n",
    "    w2 = w2.astype(np.float32)\n",
    "\n",
    "    result = np.zeros_like(d_rbins_squared)[:-1]\n",
    "    result = result.astype(np.float32)\n",
    "    results_test = np.zeros_like(result).astype(np.float64)\n",
    "    count_weighted_pairs_3d_cpu(\n",
    "        x1, y1, z1, w1, x2, y2, z2, w2, d_rbins_squared.astype(np.float32), results_test)\n",
    "\n",
    "base_gpairs.run(\"Gpairs Numba\",run_gpairs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpairs_jit.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_gpairs_jit.sh; else ./run_gpairs_jit.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Gpairs targeting GPU using Kernel\n",
    "\n",
    "## Writing Explicit Kernels in numba-dpex\n",
    "\n",
    "Writing a SYCL kernel using the `@numba_dpex.kernel` decorator has similar syntax to writing OpenCL kernels. As such, the numba-dpex module provides similar indexing and other functions as OpenCL. The indexing functions supported inside a `numba_dpex.kernel` are:\n",
    "\n",
    "* numba_dpex.get_local_id : Gets the local ID of the item\n",
    "* numba_dpex.get_local_size: Gets the local work group size of the device\n",
    "* numba_dpex.get_group_id : Gets the group ID of the item\n",
    "* numba_dpex.get_num_groups: Gets the number of gropus in a worksgroup\n",
    "\n",
    "Refer https://intelpython.github.io/numba-dpex/latest/user_guides/kernel_programming_guide/index.html for more details.\n",
    "\n",
    "In the following example we use the dpex-kernel approach for explicit kernel programming where, if the programmer wants to extract further performance from the offloaded code, the programmer can use the explicit kernel programming approach using dpex-kernels and tune the GPU parameters, where we take advantage of the workgroups and the work items in a device using the kernel approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpairs_gpu.py\n",
    "\n",
    "# Copyright (C) 2017-2018 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "import base_gpairs_naive\n",
    "import numpy as np\n",
    "import gaussian_weighted_pair_counts_gpu as gwpc\n",
    "import numba_dpex\n",
    "import dpctl\n",
    "\n",
    "\n",
    "def run_gpairs(\n",
    "    d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    blocks = 512\n",
    "\n",
    "    with dpctl.device_context(base_gpairs_naive.get_device_selector(is_gpu=True)):\n",
    "        gwpc.count_weighted_pairs_3d_intel_ver2[\n",
    "            d_x1.shape[0], numba_dpex.DEFAULT_LOCAL_SIZE\n",
    "        ](d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result)\n",
    "\n",
    "\n",
    "base_gpairs_naive.run(\"Gpairs Dpex kernel\", run_gpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpairs_jit_gpu.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_gpairs_jit_gpu.sh; else ./run_gpairs_jit_gpu.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shared Local Memory (SLM) Implementation\n",
    "\n",
    "In a parallel algorithm, there is a high degree of reuse, so instead of loading values from global memory each time we can load the values into local memory and perform the computation.  This will reduce the latency of accessing the data values.  The difference between this implementation and the ND-range implementation is the reading is done from global memory in the case of ND-range each time and in this implementation they dat is loaded into local memory and then computed.  \n",
    "\n",
    "When a work-group begins, the contents of its local memory are uninitialized, and local memory does not persist after a work-group finishes executing. Because of these properties, local memory may only be used for temporary storage while a work-group is executing.  For other devices though, such as many GPU devices, there are dedicated resources for local memory, and on these devices, communicating via local memory should perform better than communicating via global memory.\n",
    "\n",
    "In SYCL’s memory model, local memory is a contiguous region of memory allocated per work group and is visible to all the work items in that group. Local memory is device-only and cannot be accessed from the host. From the perspective offers the device, the local memory is exposed as a contiguous array of a specific types. The maximum available local memory is hardware-specific. The SYCL local memory concept is analogous to CUDA’s shared memory concept.\n",
    "\n",
    "Numba-dpex provides a special function numba_dpex.local.array to allocate local memory for a kernel. To simplify kernel development and accelerate communication between work-items in a work-group, SYCL defines a special local memory space specifically for communication between work-items in a work-group.\n",
    "\n",
    "Local Address Space refers to memory objects that need to be allocated in local memory pool and are shared by all work-items of a work-group. Numba-dpex does not support passing arguments that are allocated in the local address space to @numba_dpex.kernel. Users are allowed to allocate static arrays in the local address space inside the @numba_dpex.kernel. In the example below numba_dpex.local.array(shape, dtype) is the API used to allocate a static array in the local address space:\n",
    "These are used to compute an intermediate result which does not use global memory for repeated access for computation. \n",
    "\n",
    "Also notice that we used a barrier that helps to synchronize all of the work-items in the work-group.  The performance is much better than the initial ND-range samples and slightly better than the ND-range sample utilizing local memory.\n",
    "\n",
    "<img src=\"Assets/localmem.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Private Address Space\n",
    "Private Address Space refers to memory objects that are local to each work-item and is not shared with any other work-item. In the example below numba_dpex.private.array(shape, dtype) is the API used to allocate a static array in the private address space:\n",
    "\n",
    "<img src=\"Assets/workgroup.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpairs_gpu_private_memory.py\n",
    "\n",
    "# Copyright (C) 2017-2018 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "import base_gpairs_gpu\n",
    "import numpy as np\n",
    "import gwpc_private as gwpc\n",
    "import dpctl, dpctl.tensor as dpt\n",
    "from device_selector import get_device_selector\n",
    "import dpctl\n",
    "from numba_dppy import kernel, atomic, DEFAULT_LOCAL_SIZE\n",
    "import numba_dppy\n",
    "\n",
    "atomic_add = atomic.add\n",
    "\n",
    "\n",
    "@kernel\n",
    "def count_weighted_pairs_3d_intel(\n",
    "    x1, y1, z1, w1, x2, y2, z2, w2, rbins_squared, result\n",
    "):\n",
    "    \"\"\"Naively count Npairs(<r), the total number of pairs that are separated\n",
    "    by a distance less than r, for each r**2 in the input rbins_squared.\n",
    "    \"\"\"\n",
    "\n",
    "    start = numba_dppy.get_global_id(0)\n",
    "    stride = numba_dppy.get_global_size(0)\n",
    "\n",
    "    n1 = x1.shape[0]\n",
    "    n2 = x2.shape[0]\n",
    "    nbins = rbins_squared.shape[0]\n",
    "\n",
    "    for i in range(start, n1, stride):\n",
    "        px = x1[i]\n",
    "        py = y1[i]\n",
    "        pz = z1[i]\n",
    "        pw = w1[i]\n",
    "        for j in range(n2):\n",
    "            qx = x2[j]\n",
    "            qy = y2[j]\n",
    "            qz = z2[j]\n",
    "            qw = w2[j]\n",
    "            dx = px - qx\n",
    "            dy = py - qy\n",
    "            dz = pz - qz\n",
    "            wprod = pw * qw\n",
    "            dsq = dx * dx + dy * dy + dz * dz\n",
    "\n",
    "            k = nbins - 1\n",
    "            while dsq <= rbins_squared[k]:\n",
    "                atomic_add(result, k - 1, wprod)\n",
    "                k = k - 1\n",
    "                if k <= 0:\n",
    "                    break\n",
    "\n",
    "\n",
    "@kernel\n",
    "def count_weighted_pairs_3d_intel_ver2(\n",
    "    x1, y1, z1, w1, x2, y2, z2, w2, rbins_squared, result\n",
    "):\n",
    "    \"\"\"Naively count Npairs(<r), the total number of pairs that are separated\n",
    "    by a distance less than r, for each r**2 in the input rbins_squared.\n",
    "    \"\"\"\n",
    "\n",
    "    i = numba_dppy.get_global_id(0)\n",
    "    nbins = rbins_squared.shape[0]\n",
    "    n2 = x2.shape[0]\n",
    "\n",
    "    px = x1[i]\n",
    "    py = y1[i]\n",
    "    pz = z1[i]\n",
    "    pw = w1[i]\n",
    "    for j in range(n2):\n",
    "        qx = x2[j]\n",
    "        qy = y2[j]\n",
    "        qz = z2[j]\n",
    "        qw = w2[j]\n",
    "        dx = px - qx\n",
    "        dy = py - qy\n",
    "        dz = pz - qz\n",
    "        wprod = pw * qw\n",
    "        dsq = dx * dx + dy * dy + dz * dz\n",
    "\n",
    "        k = nbins - 1\n",
    "        while dsq <= rbins_squared[k]:\n",
    "            # disabled for now since it's not supported currently\n",
    "            # - could reenable later when it's supported (~April 2020)\n",
    "            # - could work around this to avoid atomics, which would perform better anyway\n",
    "            # cuda.atomic.add(result, k-1, wprod)\n",
    "            atomic_add(result, k - 1, wprod)\n",
    "            k = k - 1\n",
    "            if k <= 0:\n",
    "                break\n",
    "\n",
    "\n",
    "def ceiling_quotient(n, m):\n",
    "    return int((n + m - 1) / m)\n",
    "\n",
    "\n",
    "def count_weighted_pairs_3d_intel_no_slm(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    n_wi = 20\n",
    "    private_hist_size = 16\n",
    "    lws0 = 16\n",
    "    lws1 = 16\n",
    "\n",
    "    m0 = n_wi * lws0\n",
    "    m1 = n_wi * lws1\n",
    "\n",
    "    n_groups0 = ceiling_quotient(n, m0)\n",
    "    n_groups1 = ceiling_quotient(n, m1)\n",
    "\n",
    "    gwsRange = n_groups0 * lws0, n_groups1 * lws1\n",
    "    lwsRange = lws0, lws1\n",
    "\n",
    "    slm_hist_size = ceiling_quotient(nbins, private_hist_size) * private_hist_size\n",
    "\n",
    "    with dpctl.device_context(base_gpairs_gpu.get_device_selector(is_gpu=True)):\n",
    "        gwpc.count_weighted_pairs_3d_intel_no_slm_ker[gwsRange, lwsRange](\n",
    "            n,\n",
    "            nbins,\n",
    "            slm_hist_size,\n",
    "            private_hist_size,\n",
    "            d_x1,\n",
    "            d_y1,\n",
    "            d_z1,\n",
    "            d_w1,\n",
    "            d_x2,\n",
    "            d_y2,\n",
    "            d_z2,\n",
    "            d_w2,\n",
    "            d_rbins_squared,\n",
    "            d_result,\n",
    "        )\n",
    "\n",
    "\n",
    "def count_weighted_pairs_3d_intel_orig(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "\n",
    "    # create tmp result on device\n",
    "    result_tmp = np.zeros(nbins, dtype=np.float32)\n",
    "    d_result_tmp = dpt.usm_ndarray(\n",
    "        result_tmp.shape, dtype=result_tmp.dtype, buffer=\"device\"\n",
    "    )\n",
    "    d_result_tmp.usm_data.copy_from_host(result_tmp.reshape((-1)).view(\"|u1\"))\n",
    "\n",
    "    with dpctl.device_context(base_gpairs_gpu.get_device_selector()):\n",
    "        gwpc.count_weighted_pairs_3d_intel_orig_ker[n,](\n",
    "            n,\n",
    "            nbins,\n",
    "            d_x1,\n",
    "            d_y1,\n",
    "            d_z1,\n",
    "            d_w1,\n",
    "            d_x2,\n",
    "            d_y2,\n",
    "            d_z2,\n",
    "            d_w2,\n",
    "            d_rbins_squared,\n",
    "            d_result_tmp,\n",
    "        )\n",
    "        gwpc.count_weighted_pairs_3d_intel_agg_ker[\n",
    "            nbins,\n",
    "        ](d_result, d_result_tmp)\n",
    "\n",
    "\n",
    "def run_gpairs(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    count_weighted_pairs_3d_intel_no_slm(\n",
    "        n,\n",
    "        nbins,\n",
    "        d_x1,\n",
    "        d_y1,\n",
    "        d_z1,\n",
    "        d_w1,\n",
    "        d_x2,\n",
    "        d_y2,\n",
    "        d_z2,\n",
    "        d_w2,\n",
    "        d_rbins_squared,\n",
    "        d_result,\n",
    "    )\n",
    "\n",
    "\n",
    "base_gpairs_gpu.run(\"Gpairs Dpex kernel\", run_gpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpairs_private_gpu.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_gpairs_private_gpu.sh; else ./run_gpairs_private_gpu.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private memory implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpairs_gpu_private_memory_imp.py\n",
    "import base_gpairs_gpu\n",
    "import numpy as np\n",
    "import gwpc_private as gwpc\n",
    "import dpctl, dpctl.tensor as dpt\n",
    "from device_selector import get_device_selector\n",
    "import dpctl\n",
    "from numba_dpex import kernel, atomic, DEFAULT_LOCAL_SIZE\n",
    "import numba_dpex\n",
    "\n",
    "atomic_add = atomic.add\n",
    "\n",
    "\n",
    "@kernel\n",
    "def count_weighted_pairs_3d_intel(\n",
    "    x1, y1, z1, w1, x2, y2, z2, w2, rbins_squared, result\n",
    "):\n",
    "    \"\"\"Naively count Npairs(<r), the total number of pairs that are separated\n",
    "    by a distance less than r, for each r**2 in the input rbins_squared.\n",
    "    \"\"\"\n",
    "\n",
    "    start = numba_dpex.get_global_id(0)\n",
    "    stride = numba_dpex.get_global_size(0)\n",
    "\n",
    "    n1 = x1.shape[0]\n",
    "    n2 = x2.shape[0]\n",
    "    nbins = rbins_squared.shape[0]\n",
    "\n",
    "    for i in range(start, n1, stride):\n",
    "        px = x1[i]\n",
    "        py = y1[i]\n",
    "        pz = z1[i]\n",
    "        pw = w1[i]\n",
    "        for j in range(n2):\n",
    "            qx = x2[j]\n",
    "            qy = y2[j]\n",
    "            qz = z2[j]\n",
    "            qw = w2[j]\n",
    "            dx = px - qx\n",
    "            dy = py - qy\n",
    "            dz = pz - qz\n",
    "            wprod = pw * qw\n",
    "            dsq = dx * dx + dy * dy + dz * dz\n",
    "\n",
    "            k = nbins - 1\n",
    "            while dsq <= rbins_squared[k]:\n",
    "                atomic_add(result, k - 1, wprod)\n",
    "                k = k - 1\n",
    "                if k <= 0:\n",
    "                    break\n",
    "\n",
    "\n",
    "@kernel\n",
    "def count_weighted_pairs_3d_intel_ver2(\n",
    "    x1, y1, z1, w1, x2, y2, z2, w2, rbins_squared, result\n",
    "):\n",
    "    \"\"\"Naively count Npairs(<r), the total number of pairs that are separated\n",
    "    by a distance less than r, for each r**2 in the input rbins_squared.\n",
    "    \"\"\"\n",
    "\n",
    "    i = numba_dpex.get_global_id(0)\n",
    "    nbins = rbins_squared.shape[0]\n",
    "    n2 = x2.shape[0]\n",
    "\n",
    "    px = x1[i]\n",
    "    py = y1[i]\n",
    "    pz = z1[i]\n",
    "    pw = w1[i]\n",
    "    for j in range(n2):\n",
    "        qx = x2[j]\n",
    "        qy = y2[j]\n",
    "        qz = z2[j]\n",
    "        qw = w2[j]\n",
    "        dx = px - qx\n",
    "        dy = py - qy\n",
    "        dz = pz - qz\n",
    "        wprod = pw * qw\n",
    "        dsq = dx * dx + dy * dy + dz * dz\n",
    "\n",
    "        k = nbins - 1\n",
    "        while dsq <= rbins_squared[k]:\n",
    "            # disabled for now since it's not supported currently\n",
    "            # - could reenable later when it's supported (~April 2020)\n",
    "            # - could work around this to avoid atomics, which would perform better anyway\n",
    "            # cuda.atomic.add(result, k-1, wprod)\n",
    "            atomic_add(result, k - 1, wprod)\n",
    "            k = k - 1\n",
    "            if k <= 0:\n",
    "                break\n",
    "\n",
    "\n",
    "def ceiling_quotient(n, m):\n",
    "    return int((n + m - 1) / m)\n",
    "\n",
    "\n",
    "def count_weighted_pairs_3d_intel_no_slm(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    n_wi = 20\n",
    "    private_hist_size = 16\n",
    "    lws0 = 16\n",
    "    lws1 = 16\n",
    "\n",
    "    m0 = n_wi * lws0\n",
    "    m1 = n_wi * lws1\n",
    "\n",
    "    n_groups0 = ceiling_quotient(n, m0)\n",
    "    n_groups1 = ceiling_quotient(n, m1)\n",
    "\n",
    "    gwsRange = n_groups0 * lws0, n_groups1 * lws1\n",
    "    lwsRange = lws0, lws1\n",
    "\n",
    "    slm_hist_size = ceiling_quotient(nbins, private_hist_size) * private_hist_size\n",
    "\n",
    "    with dpctl.device_context(base_gpairs_gpu.get_device_selector(is_gpu=True)):\n",
    "        gwpc.count_weighted_pairs_3d_intel_no_slm_ker[gwsRange, lwsRange](\n",
    "            n,\n",
    "            nbins,\n",
    "            slm_hist_size,\n",
    "            private_hist_size,\n",
    "            d_x1,\n",
    "            d_y1,\n",
    "            d_z1,\n",
    "            d_w1,\n",
    "            d_x2,\n",
    "            d_y2,\n",
    "            d_z2,\n",
    "            d_w2,\n",
    "            d_rbins_squared,\n",
    "            d_result,\n",
    "        )\n",
    "\n",
    "\n",
    "def count_weighted_pairs_3d_intel_orig(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "\n",
    "    # create tmp result on device\n",
    "    result_tmp = np.zeros(nbins, dtype=np.float32)\n",
    "    d_result_tmp = dpt.usm_ndarray(\n",
    "        result_tmp.shape, dtype=result_tmp.dtype, buffer=\"device\"\n",
    "    )\n",
    "    d_result_tmp.usm_data.copy_from_host(result_tmp.reshape((-1)).view(\"|u1\"))\n",
    "\n",
    "    with dpctl.device_context(base_gpairs_gpu.get_device_selector()):\n",
    "        gwpc.count_weighted_pairs_3d_intel_orig_ker[n,](\n",
    "            n,\n",
    "            nbins,\n",
    "            d_x1,\n",
    "            d_y1,\n",
    "            d_z1,\n",
    "            d_w1,\n",
    "            d_x2,\n",
    "            d_y2,\n",
    "            d_z2,\n",
    "            d_w2,\n",
    "            d_rbins_squared,\n",
    "            d_result_tmp,\n",
    "        )\n",
    "        gwpc.count_weighted_pairs_3d_intel_agg_ker[\n",
    "            nbins,\n",
    "        ](d_result, d_result_tmp)\n",
    "\n",
    "\n",
    "def run_gpairs(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    count_weighted_pairs_3d_intel_no_slm(\n",
    "        n,\n",
    "        nbins,\n",
    "        d_x1,\n",
    "        d_y1,\n",
    "        d_z1,\n",
    "        d_w1,\n",
    "        d_x2,\n",
    "        d_y2,\n",
    "        d_z2,\n",
    "        d_w2,\n",
    "        d_rbins_squared,\n",
    "        d_result,\n",
    "    )\n",
    "\n",
    "\n",
    "base_gpairs_gpu.run(\"Gpairs Dpex kernel\", run_gpairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Private Memory differentials\n",
    "In the below sample we are not using atomics to calcualte the values but we sum the private memory kernels in each work group and then we finally do an aggregation of all the first items of each work group.\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpairs_gpu_private_memory_diff.py\n",
    "\n",
    "# Copyright (C) 2017-2018 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "import base_gpairs_diff\n",
    "import numpy as np\n",
    "import gwpc_private_diff as gwpc\n",
    "import dpctl, dpctl.tensor as dpt\n",
    "\n",
    "\n",
    "def ceiling_quotient(n, m):\n",
    "    return int((n + m - 1) / m)\n",
    "\n",
    "\n",
    "def count_weighted_pairs_3d_intel_diff(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    with dpctl.device_context(base_gpairs_diff.get_device_selector(is_gpu=True)):\n",
    "        gwpc.count_weighted_pairs_3d_intel_diff_ker[n, 64](\n",
    "            n,\n",
    "            nbins,\n",
    "            d_x1,\n",
    "            d_y1,\n",
    "            d_z1,\n",
    "            d_w1,\n",
    "            d_x2,\n",
    "            d_y2,\n",
    "            d_z2,\n",
    "            d_w2,\n",
    "            d_rbins_squared,\n",
    "            d_result,\n",
    "        )\n",
    "        gwpc.count_weighted_pairs_3d_intel_diff_agg_ker[\n",
    "            nbins,\n",
    "        ](d_result, n)\n",
    "\n",
    "\n",
    "def run_gpairs(\n",
    "    n, nbins, d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    count_weighted_pairs_3d_intel_diff(\n",
    "        n,\n",
    "        nbins,\n",
    "        d_x1,\n",
    "        d_y1,\n",
    "        d_z1,\n",
    "        d_w1,\n",
    "        d_x2,\n",
    "        d_y2,\n",
    "        d_z2,\n",
    "        d_w2,\n",
    "        d_rbins_squared,\n",
    "        d_result,\n",
    "    )\n",
    "\n",
    "\n",
    "base_gpairs_diff.run(\"Gpairs Dpex kernel\", run_gpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpairs_private_gpu_diff.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_gpairs_private_gpu_diff.sh; else ./run_gpairs_private_gpu_diff.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results for Gpairs on GPU\n",
    "\n",
    "The algorithm Naively counts Npairs(<r), the total number of pairs that are separated by a distance less than r, for each r**2 in the input rbins_squared.\n",
    "\n",
    "In the below graphs you will see a three dimensional view of the points and the second plot you can see the logirthmtic view of the __results__ that are computed based on the distance less than the distance defeined by the RBINS_SQUARED.\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/gpairs_gpu_graph.py\n",
    "\n",
    "# Copyright (C) 2017-2018 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: MIT\n",
    "\n",
    "import base_gpairs_gpu_graph\n",
    "import numpy as np\n",
    "import gaussian_weighted_pair_counts_gpu as gwpc\n",
    "import numba_dpex\n",
    "import dpctl\n",
    "\n",
    "\n",
    "def run_gpairs(\n",
    "    d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result\n",
    "):\n",
    "    blocks = 512\n",
    "\n",
    "    with dpctl.device_context(base_gpairs_gpu_graph.get_device_selector(is_gpu=True)):\n",
    "        gwpc.count_weighted_pairs_3d_intel_ver2[\n",
    "            d_x1.shape[0], numba_dpex.DEFAULT_LOCAL_SIZE\n",
    "        ](d_x1, d_y1, d_z1, d_w1, d_x2, d_y2, d_z2, d_w2, d_rbins_squared, d_result)\n",
    "\n",
    "\n",
    "base_gpairs_gpu_graph.run(\"Gpairs Dpex kernel\", run_gpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_gpairs_jit_gpu_graph.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_gpairs_jit_gpu_graph.sh; else ./run_gpairs_jit_gpu_graph.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### View the results\n",
    "Select the cell below and click run ▶ to view the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dictionary(fn):\n",
    "    import joblib\n",
    "    # Load data (deserialize)\n",
    "    with open(fn, 'rb') as handle:\n",
    "        dictionary = joblib.load(handle)\n",
    "    return dictionary\n",
    "resultsDict = read_dictionary('resultsDict.dat')\n",
    "limit = 10\n",
    "#D = resultsDict['D'][:limit,:limit]\n",
    "X1 = resultsDict['X1'][:limit]\n",
    "Y1 = resultsDict['Y1'][:limit]\n",
    "Z1 = resultsDict['Z1'][:limit]\n",
    "X2 = resultsDict['X2'][:limit]\n",
    "Y2 = resultsDict['Y2'][:limit]\n",
    "Z2 = resultsDict['Z2'][:limit]\n",
    "result = resultsDict['result']\n",
    "RBINS_SQAURED = resultsDict['DEFAULT_RBINS_SQUARED']\n",
    "#print(result)\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np \n",
    "Radius = .92\n",
    "index = np.where(result < Radius)\n",
    "plt.style.use('dark_background')\n",
    "#plt.gcf().set_size_inches((12, 5))\n",
    "# plt.grid()\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X1, Y1, Z1, c='blue', s = 40, alpha = .8)\n",
    "ax.scatter(X2, Y2, Z2, c='y', s = 40, alpha = .8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"magnitude of results\")\n",
    "plt.xlabel(\"index of results\")\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "nonzero = 1e-4\n",
    "plt.grid()\n",
    "plt.plot(result + nonzero,c = 'y');\n",
    "plt.plot(RBINS_SQAURED + nonzero,c = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Vtune reports\n",
    "Below exercises we use VTune™  analyzer as a way to see what is going on with each implementation. The information was the high-level hotspot generated from the collection and rendered in an HTML iframe. Depending on the options chosen, many of the VTune analyzer's performance collections can be rendered via HTML pages. The below vtune scripts collect GPU offload and GPU hotspots information.\n",
    "\n",
    "#### Learn more about VTune\n",
    "​\n",
    "There is extensive training on VTune, click [here](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/vtune-profiler.html#gs.2xmez3) to get deep dive training.\n",
    "\n",
    "```\n",
    "vtune -run-pass-thru=--no-altstack -collect=gpu-offload -result-dir=vtune_dir python lab/gpairs_gpu_private_memory.py --steps 1 --size 16384 --repeat 5 --json result_gpu.json\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -run-pass-thru=--no-altstack -collect=gpu-hotspots -result-dir=vtune_hotspots_dir_new python lab/gpairs_gpu_private_memory.py --steps 1 --size 16384 --repeat 5 --json result_gpu.json\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -report summary -result-dir vtune_dir -format html -report-output output.html\n",
    "```\n",
    "\n",
    "```\n",
    "vtune -report summary -result-dir vtune_hotspots_dir -format html -report-output output_hotspots.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run lab/mm_basic_vtune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summary\n",
    "In this module you will have learned the following:\n",
    "* Numba implementation of Gpairs targeting a CPU and GPU using Numba JIT\n",
    "* Numba-dpex  implementation of Gpairs on a GPU using the kernel approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2022.3)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.09px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
