{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ndrange Kernels, Local memory and Private memory in numba-dpex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand how ND-range improves parallelism over the basic implementation.\n",
    "- Able to explain why limiting global memory access can be advantageous.\n",
    "- Explain the differences between work-groups and work-items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## ND-Range Kernels\n",
    "\n",
    "Naive parallel kernel do not allow for performance optimizations at a hardware level. In these next two kernels we will utilize ND-Range kernels as a way to expresses parallelism enabling low level performance tuning by providing access to both global and local memory and mapping executions to compute units on hardware. The entire iteration space is divided into smaller groups called work-groups, work-items are organized into these work-groups and are scheduled on a single compute unit on the hardware.  Workgroup size must divide the entire ND-range size exactly in each dimension.  These sizes can all vary by hardware platform and by using the device queries below a developer can identify what is possible.  The workload must be considered to find the best mix of these values.\n",
    "\n",
    "<img src=\"Assets/ndrange-subgroup.png\">\n",
    "\n",
    "The grouping of kernel executions into work-groups allows control of resource usage and load balance work distribution. The functionality of nd_range kernels is exposed via nd_range and nd_item classes. nd_range class represents a grouped execution range using global execution range and the local execution range of each work-group. nd_item class represents an individual instance of a kernel function and allows you to query for work-group range and index.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Assets/ndrange.png\">\n",
    "\n",
    "The work-group size depends on the accelerator hardware capability, so we set this size using command-line argument. Some hardware requre the matrix size to divide equally by the work-group size, we will use work-group size of 16x16 (256) by default which works for all the accelerator hardware we will be using to test, we will eventually use different work-group sizes to see how it impacts the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shared Local Memory (SLM) Implementation\n",
    "\n",
    "In a parallel algorithm, there is a high degree of reuse, so instead of loading values from global memory each time we can load the values into local memory and perform the computation.  This will reduce the latency of accessing the data values.  The difference between this implementation and the ND-range implementation is the reading is done from global memory in the case of ND-range each time and in this implementation they dat is loaded into local memory and then computed.  \n",
    "\n",
    "When a work-group begins, the contents of its local memory are uninitialized, and local memory does not persist after a work-group finishes executing. Because of these properties, local memory may only be used for temporary storage while a work-group is executing.  For other devices though, such as many GPU devices, there are dedicated resources for local memory, and on these devices, communicating via local memory should perform better than communicating via global memory.\n",
    "\n",
    "In SYCL’s memory model, local memory is a contiguous region of memory allocated per work group and is visible to all the work items in that group. Local memory is device-only and cannot be accessed from the host. From the perspective offers the device, the local memory is exposed as a contiguous array of a specific types. The maximum available local memory is hardware-specific. The SYCL local memory concept is analogous to CUDA’s shared memory concept.\n",
    "\n",
    "Numba-dpex provides a special function numba_dpex.local.array to allocate local memory for a kernel. To simplify kernel development and accelerate communication between work-items in a work-group, SYCL defines a special local memory space specifically for communication between work-items in a work-group.\n",
    "\n",
    "Local Address Space refers to memory objects that need to be allocated in local memory pool and are shared by all work-items of a work-group. Numba-dpex does not support passing arguments that are allocated in the local address space to @numba_dpex.kernel. Users are allowed to allocate static arrays in the local address space inside the @numba_dpex.kernel. In the example below numba_dpex.local.array(shape, dtype) is the API used to allocate a static array in the local address space:\n",
    "These are used to compute an intermediate result which does not use global memory for repeated access for computation. \n",
    "\n",
    "Also notice that we used a barrier that helps to synchronize all of the work-items in the work-group. \n",
    "\n",
    "<img src=\"Assets/localmem.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local Memory sample\n",
    "The following DPC++ code shows a local-memory implementation of matrix multiplication: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/local_memory_kernel.py\n",
    "# Copyright 2020, 2021 Intel Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import dpctl\n",
    "import dpnp as np\n",
    "from numba import float32\n",
    "import dpctl.tensor as dpt\n",
    "\n",
    "import numba_dpex as dpex\n",
    "\n",
    "\n",
    "def no_arg_barrier_support():\n",
    "    \"\"\"\n",
    "    This example demonstrates the usage of numba_dpex's ``barrier``\n",
    "    intrinsic function. The ``barrier`` function is usable only inside\n",
    "    a ``kernel`` and is equivalent to OpenCL's ``barrier`` function.\n",
    "    \"\"\"\n",
    "\n",
    "    @dpex.kernel\n",
    "    def twice(A):\n",
    "        i = dpex.get_global_id(0)\n",
    "        d = A[i]\n",
    "        # no argument defaults to global mem fence\n",
    "        dpex.barrier()\n",
    "        A[i] = d * 2\n",
    "\n",
    "    N = 10\n",
    "\n",
    "    # Use the environment variable SYCL_DEVICE_FILTER to change the default device.\n",
    "    # See https://github.com/intel/llvm/blob/sycl/sycl/doc/EnvironmentVariables.md#sycl_device_filter.\n",
    "    device = dpctl.select_default_device()\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "    \n",
    "    arr = dpt.ones(N, dtype=dpt.float32, device=device)\n",
    "    #arr = np.arange(blocksize).astype(np.float32)\n",
    "    print(arr)     \n",
    "    twice[N, dpex.DEFAULT_LOCAL_SIZE](arr)\n",
    "\n",
    "    # the output should be `arr * 2, i.e. [0, 2, 4, 6, ...]`\n",
    "    print(arr)\n",
    "\n",
    "\n",
    "def local_memory():\n",
    "    \"\"\"\n",
    "    This example demonstrates the usage of numba-dpex's `local.array`\n",
    "    intrinsic function. The function is used to create a static array\n",
    "    allocated on the devices local address space.\n",
    "    \"\"\"\n",
    "    blocksize = 10\n",
    "\n",
    "    @dpex.kernel\n",
    "    def reverse_array(A):\n",
    "        lm = dpex.local.array(shape=10, dtype=float32)\n",
    "        i = dpex.get_global_id(0)\n",
    "\n",
    "        # preload\n",
    "        lm[i] = A[i]\n",
    "        # barrier local or global will both work as we only have one work group\n",
    "        dpex.barrier(dpex.LOCAL_MEM_FENCE)  # local mem fence\n",
    "        # write\n",
    "        A[i] += lm[blocksize - 1 - i]\n",
    "\n",
    "    \n",
    "    device = dpctl.select_default_device()\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "    \n",
    "    arr = np.arange(blocksize).astype(np.float32)\n",
    "    print(arr)   \n",
    "\n",
    "    \n",
    "    reverse_array[blocksize, dpex.DEFAULT_LOCAL_SIZE](arr)\n",
    "\n",
    "    # the output should be `orig[::-1] + orig, i.e. [9, 9, 9, ...]``\n",
    "    print(arr)\n",
    "\n",
    "\n",
    "def main():\n",
    "    no_arg_barrier_support()\n",
    "    local_memory()\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_local_memory.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_local_memory.sh; else ./run_local_memory.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Private Address Space\n",
    "Private Address Space refers to memory objects that are local to each work-item and is not shared with any other work-item. In the example below numba_dpex.private.array(shape, dtype) is the API used to allocate a static array in the private address space:\n",
    "\n",
    "<img src=\"Assets/workgroup.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Private Memory sample\n",
    "\n",
    "The following code shows a Private-memory implementation of numba-dpex: Inspect code; there are no modifications necessary:\n",
    "1. Inspect the following code cell and click Run (▶)to save the code to file.\n",
    "2. Next, run (▶) the cell in the __Build and Run__ section following the code to compile and execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/private_memory_kernel.py\n",
    "# SPDX-FileCopyrightText: 2020 - 2023 Intel Corporation\n",
    "#\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import dpctl\n",
    "import dpctl.tensor as dpt\n",
    "import numpy as np\n",
    "from numba import float32\n",
    "\n",
    "import numba_dpex as ndpx\n",
    "\n",
    "\n",
    "def private_memory():\n",
    "    \"\"\"\n",
    "    This example demonstrates the usage of numba_dpex's `private.array`\n",
    "    intrinsic function. The function is used to create a static array\n",
    "    allocated on the devices private address space.\n",
    "    \"\"\"\n",
    "\n",
    "    @ndpx.kernel\n",
    "    def private_memory_kernel(A):\n",
    "        memory = ndpx.private.array(shape=1, dtype=np.float32)\n",
    "        i = ndpx.get_global_id(0)\n",
    "\n",
    "        # preload\n",
    "        memory[0] = i\n",
    "        ndpx.barrier(ndpx.LOCAL_MEM_FENCE)  # local mem fence\n",
    "\n",
    "        # memory will not hold correct deterministic result if it is not\n",
    "        # private to each thread.\n",
    "        A[i] = memory[0] * 2\n",
    "\n",
    "    N = 4\n",
    "    device = dpctl.select_default_device()\n",
    "\n",
    "    arr = dpt.zeros(N, dtype=dpt.float32, device=device)\n",
    "    orig = np.arange(N).astype(np.float32)\n",
    "\n",
    "    print(\"Using device ...\")\n",
    "    device.print_device_info()\n",
    "\n",
    "    global_range = ndpx.Range(N)\n",
    "    local_range = ndpx.Range(N)\n",
    "    private_memory_kernel[ndpx.NdRange(global_range, local_range)](arr)\n",
    "\n",
    "    arr_out = dpt.asnumpy(arr)\n",
    "    np.testing.assert_allclose(orig * 2, arr_out)\n",
    "    # the output should be `orig[i] * 2, i.e. [0, 2, 4, ..]``\n",
    "    print(arr_out)\n",
    "\n",
    "\n",
    "def main():\n",
    "    private_memory()\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_private_memory.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_private_memory.sh; else ./run_private_memory.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this module you will have learned the following:\n",
    "* Implementation of Local memory in numba-dpex\n",
    "* Implementation of Private memory in numba-dpex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.09px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
