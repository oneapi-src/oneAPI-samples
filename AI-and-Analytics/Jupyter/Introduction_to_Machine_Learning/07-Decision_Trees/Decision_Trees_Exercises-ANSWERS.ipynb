{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Exercises\n",
    "\n",
    "![DecisionTree.png](Assets/DecisionTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Recognize Decision trees and how to use them for classification problems\n",
    "- Recognize how to identify the best split and the factors for splitting\n",
    "- Explain strengths and weaknesses of decision trees\n",
    "- Explain how regression trees help with classifying continuous values\n",
    "- Describe motivation for choosing Random Forest Classifier over Decision Trees\n",
    "- Apply Intel速 Extension for Scikit-learn* to leverage underlying compute capabilities of hardware for Random Forest Classifier\n",
    "\n",
    "# scikit-learn* \n",
    "\n",
    "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
    "\n",
    "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
    "\n",
    "These exercises will demonstrate how to apply **the Intel速 Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel速 oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
    "\n",
    "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code PRIOR to importing sklearn: \n",
    "\n",
    "- **from sklearnex import patch_sklearn**\n",
    "- **patch_sklearn()**\n",
    "\n",
    "## This exercise relies on installation of  Intel速 Extension for Scikit-learn*\n",
    "\n",
    "If you have not already done so, follow the instructions from Week 1 for instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is `Wine_Quality_Data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.099154Z",
     "start_time": "2021-09-24T17:36:23.003013Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['data']\n",
    "\n",
    "# This listener will import Intel Extnsions for Scikit-learn optimized versions \n",
    "# for any applicable imports from scikit-learn once this patch has been run\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from io import StringIO\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data and examine the features.\n",
    "* We will be using all of them to predict `color` (white or red), but the colors feature will need to be integer encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.131156Z",
     "start_time": "2021-09-24T17:36:25.102154Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = os.sep.join(data_path + ['Wine_Quality_Data.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.195158Z",
     "start_time": "2021-09-24T17:36:25.134157Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.211163Z",
     "start_time": "2021-09-24T17:36:25.198158Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the color feature to an integer. This is a quick way to do it using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.227165Z",
     "start_time": "2021-09-24T17:36:25.214161Z"
    }
   },
   "outputs": [],
   "source": [
    "data['color'] = data.color.replace('white',0).replace('red',1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Use `StratifiedShuffleSplit` to split data into train and test sets that are stratified by wine quality. If possible, preserve the indices of the split for question 5 below.\n",
    "* Check the percent composition of each quality level for both the train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.243158Z",
     "start_time": "2021-09-24T17:36:25.231159Z"
    }
   },
   "outputs": [],
   "source": [
    "# All data columns except for color\n",
    "feature_cols = [x for x in data.columns if x not in 'color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.273156Z",
     "start_time": "2021-09-24T17:36:25.246158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into two parts with 1000 points in the test data\n",
    "# This creates a generator\n",
    "strat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=42)\n",
    "\n",
    "# Get the index values from the generator\n",
    "train_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['color']))\n",
    "\n",
    "# Create the data sets\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'color']\n",
    "\n",
    "X_test = data.loc[test_idx, feature_cols]\n",
    "y_test = data.loc[test_idx, 'color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the percent composition of each quality level in the train and test data sets. The data set is mostly white wine, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.304160Z",
     "start_time": "2021-09-24T17:36:25.279163Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.320167Z",
     "start_time": "2021-09-24T17:36:25.308159Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Fit a decision tree classifier with no set limits on maximum depth, features, or leaves.\n",
    "* Determine how many nodes are present and what the depth of this (very large) tree is.\n",
    "* Using this tree, measure the prediction error in the train and test data sets. What do you think is going on here based on the differences in prediction error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.787712Z",
     "start_time": "2021-09-24T17:36:25.324160Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes and the maximum actual depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.803712Z",
     "start_time": "2021-09-24T17:36:25.789718Z"
    }
   },
   "outputs": [],
   "source": [
    "dt.tree_.node_count, dt.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to return error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.818714Z",
     "start_time": "2021-09-24T17:36:25.806712Z"
    }
   },
   "outputs": [],
   "source": [
    "def measure_error(y_true, y_pred, label):\n",
    "    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n",
    "                      'precision': precision_score(y_true, y_pred),\n",
    "                      'recall': recall_score(y_true, y_pred),\n",
    "                      'f1': f1_score(y_true, y_pred)},\n",
    "                      name=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree predicts a little better on the training data than the test data, which is consistent with (mild)  overfitting. Also notice the perfect recall score for the training data. In many instances, this prediction difference is even greater than that seen here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:25.881715Z",
     "start_time": "2021-09-24T17:36:25.821716Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The error on the training and test data sets\n",
    "y_train_pred = dt.predict(X_train)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "\n",
    "train_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n",
    "                              measure_error(y_test, y_test_pred, 'test')],\n",
    "                              axis=1)\n",
    "\n",
    "train_test_full_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Replace Decision Tree in the cell above with Random Forest algorithm\n",
    "* from sklearn.ensemble import RandomForestClassifier \n",
    "* patch sklearn to apply fast version from Intel Extensions for Sckit-learn*\n",
    "* Instantiate RandomForestClassifier(random_state=42)\n",
    "* Examine feature importance: dict(zip(X_train.columns, dt.feature_importances_))\n",
    "* Compare Metrics of Decision Tree to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dt = RandomForestClassifier(random_state=42)\n",
    "dt = dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(X_train.columns, dt.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_error(y_true, y_pred, label):\n",
    "    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n",
    "                      'precision': precision_score(y_true, y_pred),\n",
    "                      'recall': recall_score(y_true, y_pred),\n",
    "                      'f1': f1_score(y_true, y_pred)},\n",
    "                      name=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error on the training and test data sets\n",
    "y_train_pred = dt.predict(X_train)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "\n",
    "train_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n",
    "                              measure_error(y_test, y_test_pred, 'test')],\n",
    "                              axis=1)\n",
    "\n",
    "train_test_full_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Using grid search with cross validation, find a decision tree that performs well on the test data set. Use a different variable name for this decision tree model than in question 3 so that both can be used in question 6.\n",
    "* Determine the number of nodes and the depth of this tree.\n",
    "* Measure the errors on the training and test sets as before and compare them to those from the tree in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:37.919520Z",
     "start_time": "2021-09-24T17:36:25.884717Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt = dt.fit(X_train, y_train)\n",
    "param_grid = {'max_depth':range(1, dt.get_depth() + 1, 2),\n",
    "              'max_features': range(1, len(dt.feature_importances_)+1)}\n",
    "\n",
    "GR = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  n_jobs=-1)\n",
    "\n",
    "GR = GR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes and the maximum depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:37.935044Z",
     "start_time": "2021-09-24T17:36:37.923521Z"
    }
   },
   "outputs": [],
   "source": [
    "GR.best_estimator_.tree_.node_count, GR.best_estimator_.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These test errors are a little better than the previous ones. So it would seem the previous example overfit the data, but only slightly so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:38.030046Z",
     "start_time": "2021-09-24T17:36:37.940045Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred_gr = GR.predict(X_train)\n",
    "y_test_pred_gr = GR.predict(X_test)\n",
    "\n",
    "train_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n",
    "                                 measure_error(y_test, y_test_pred_gr, 'test')],\n",
    "                                axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:38.062045Z",
     "start_time": "2021-09-24T17:36:38.034047Z"
    }
   },
   "outputs": [],
   "source": [
    "train_test_gr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Re-split the data into `X` and `y` parts, this time with `residual_sugar` being the predicted (`y`) data. *Note:* if the indices were preserved from the `StratifiedShuffleSplit` output in question 2, they can be used again to split the data.\n",
    "* Using grid search with cross validation, find a decision tree **regression** model that performs well on the test data set.\n",
    "* Measure the errors on the training and test sets using mean squared error.\n",
    "* Make a plot of actual *vs* predicted residual sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:38.110046Z",
     "start_time": "2021-09-24T17:36:38.066045Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [x for x in data.columns if x != 'residual_sugar']\n",
    "\n",
    "# Create the data sets\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'residual_sugar']\n",
    "\n",
    "X_test = data.loc[test_idx, feature_cols]\n",
    "y_test = data.loc[test_idx, 'residual_sugar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:52.455160Z",
     "start_time": "2021-09-24T17:36:38.114046Z"
    }
   },
   "outputs": [],
   "source": [
    "dr = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "\n",
    "param_grid = {'max_depth':range(1, dr.tree_.max_depth+1, 2),\n",
    "              'max_features': range(1, len(dr.feature_importances_)+1)}\n",
    "\n",
    "GR_sugar = GridSearchCV(DecisionTreeRegressor(random_state=42),\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                      n_jobs=-1)\n",
    "\n",
    "GR_sugar = GR_sugar.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes and the maximum depth of the tree. This tree has lots of nodes, which is not so surprising given the continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:52.470683Z",
     "start_time": "2021-09-24T17:36:52.459163Z"
    }
   },
   "outputs": [],
   "source": [
    "GR_sugar.best_estimator_.tree_.node_count, GR_sugar.best_estimator_.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on train and test data sets. Since this is continuous, we will use mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:52.534206Z",
     "start_time": "2021-09-24T17:36:52.474207Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred_gr_sugar = GR_sugar.predict(X_train)\n",
    "y_test_pred_gr_sugar  = GR_sugar.predict(X_test)\n",
    "\n",
    "train_test_gr_sugar_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_gr_sugar),\n",
    "                                         'test':  mean_squared_error(y_test, y_test_pred_gr_sugar)},\n",
    "                                          name='MSE').to_frame().T\n",
    "\n",
    "train_test_gr_sugar_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of actual vs predicted residual sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:53.680272Z",
     "start_time": "2021-09-24T17:36:52.537209Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:54.918956Z",
     "start_time": "2021-09-24T17:36:53.694274Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes()\n",
    "\n",
    "ph_test_predict = pd.DataFrame({'test':y_test.values,\n",
    "                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()\n",
    "\n",
    "ph_test_predict.plot(marker='o', ls='', ax=ax)\n",
    "ax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 *(Optional)*\n",
    "\n",
    "This question is optional as it requires an additional command line program (GraphViz) and Python library (PyDotPlus). GraphViz can be installed with a package manager on Linux and Mac. For PyDotPlus, either `pip` or `conda` (`conda install -c conda-forge pydotplus`) can be used to install the library.\n",
    "\n",
    "Once these programs are installed:\n",
    "\n",
    "* Create a visualization of the decision tree from question 3, where wine color was predicted and the number of features and/or splits are not limited.\n",
    "* Create a visualization of the decision tree from question 4, where wine color was predicted but a grid search was used to find the optimal depth and number of features.\n",
    "\n",
    "The decision tree from question 5 will likely have too many nodes to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:54.949958Z",
     "start_time": "2021-09-24T17:36:54.922955Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pydotplus\n",
    "    pydotplus_installed = True\n",
    "    \n",
    "except:\n",
    "    print('PyDotPlus must be installed to execute the remainder of the cells associated with this question.')\n",
    "    print('Please see the instructions for this question for details.')\n",
    "    pydotplus_installed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree from question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:37:01.702905Z",
     "start_time": "2021-09-24T17:36:54.952956Z"
    }
   },
   "outputs": [],
   "source": [
    "if pydotplus_installed:\n",
    "    \n",
    "    # Create an output destination for the file\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    export_graphviz(dt, out_file=dot_data, filled=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "    # View the tree image\n",
    "    filename = 'wine_tree.png'\n",
    "    graph.write_png(filename)\n",
    "    img = Image(filename=filename)\n",
    "    display(img)\n",
    "    \n",
    "else:\n",
    "    print('This cell not executed because PyDotPlus could not be loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree fit with cross validation from question 4. This tree is much shallower than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:37:05.459176Z",
     "start_time": "2021-09-24T17:37:01.705907Z"
    }
   },
   "outputs": [],
   "source": [
    "if pydotplus_installed:\n",
    "    \n",
    "    # Create an output destination for the file\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    export_graphviz(GR.best_estimator_, out_file=dot_data, filled=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "    # View the tree image\n",
    "    filename = 'wine_tree_prune.png'\n",
    "    graph.write_png(filename)\n",
    "    img = Image(filename=filename) \n",
    "    display(img)\n",
    "    \n",
    "else:\n",
    "    print('This cell not executed because PyDotPlus could not be loaded.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
