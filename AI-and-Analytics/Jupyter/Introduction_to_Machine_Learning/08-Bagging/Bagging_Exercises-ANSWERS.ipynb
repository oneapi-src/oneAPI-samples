{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Exercises\n",
    "\n",
    "![Bagging.png](Assets/Bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Associate concepts of bootstrapping and aggregating (commonly known as “bagging”) to reduce variance\n",
    "- Apply Random Forest algorithm that further reduces the correlation seen in bagging models\n",
    "- Apply Intel® Extension for Scikit-learn* to leverage underlying compute capabilities of hardware\n",
    "\n",
    "# scikit-learn* \n",
    "\n",
    "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
    "\n",
    "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
    "\n",
    "These exercises will demonstrate how to apply **the Intel® Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel® oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
    "\n",
    "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code PRIOR to importing sklearn: \n",
    "\n",
    "- **from sklearnex import patch_sklearn**\n",
    "- **patch_sklearn()**\n",
    "\n",
    "## This exercise relies on installation of  Intel® Extension for Scikit-learn*\n",
    "\n",
    "If you have not already done so, follow the instructions from Week 1 for instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the customer churn data from the telecom industry that we used in week 1 for this week's exercises. The data file is called `Orange_Telecom_Churn_Data.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.428409Z",
     "start_time": "2021-09-24T17:36:00.226970Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = [ 'data']\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the customer churn data, which is found in the file `Orange_Telecom_Churn_Data.csv`. \n",
    "* Remove any columns that are likely not to be used for prediction.\n",
    "* Encode data types as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.475009Z",
     "start_time": "2021-09-24T17:36:02.431409Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = os.sep.join(data_path + ['Orange_Telecom_Churn_Data.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.538008Z",
     "start_time": "2021-09-24T17:36:02.480010Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction should definitely not include phone number. And it is unlikely that area code or state would be desired, unless there is some reason to assume the model has a very specific geographic factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.554021Z",
     "start_time": "2021-09-24T17:36:02.542011Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(['state', 'phone_number', 'area_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the data types. Everything looks OK other than `int_plan` and `voice_mail_plan` need to be boolean encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.584015Z",
     "start_time": "2021-09-24T17:36:02.557011Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.616012Z",
     "start_time": "2021-09-24T17:36:02.588011Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in ['intl_plan', 'voice_mail_plan']:\n",
    "    data[col] = data[col].replace('yes','True').replace('no','False').astype(np.bool)\n",
    "    \n",
    "    \n",
    "data[['intl_plan', 'voice_mail_plan']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Examine distribution of the predicted variable (`churned`).\n",
    "* Split the data into train and test sets. Decide if a stratified split should be used or not based on the distribution.\n",
    "* Examine the distribution of the predictor variable in the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.646014Z",
     "start_time": "2021-09-24T17:36:02.619013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data are skewed at ~85% towards non-churned customers\n",
    "# This will be important to remember when model building\n",
    "\n",
    "data.churned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.693013Z",
     "start_time": "2021-09-24T17:36:02.652014Z"
    }
   },
   "outputs": [],
   "source": [
    "data.churned.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the skew in the predictor variable, let's split the data with the *churned* values being stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.740012Z",
     "start_time": "2021-09-24T17:36:02.701016Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [x for x in data.columns if x != 'churned']\n",
    "\n",
    "\n",
    "# Split the data into two parts with 1500 points in the test data\n",
    "# This creates a generator\n",
    "strat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1500, random_state=42)\n",
    "\n",
    "# Get the index values from the generator\n",
    "train_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['churned']))\n",
    "\n",
    "# Create the data sets\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'churned']\n",
    "\n",
    "X_test = data.loc[test_idx, feature_cols]\n",
    "y_test = data.loc[test_idx, 'churned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.756016Z",
     "start_time": "2021-09-24T17:36:02.743014Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.787015Z",
     "start_time": "2021-09-24T17:36:02.761014Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Fit random forest models with a range of tree numbers and evaluate the out-of-bag error for each of these models.\n",
    "* Plot the resulting oob errors as a function of the number of trees.\n",
    "\n",
    "*Hint:* since the only thing changing is the number of trees, the `warm_start` flag can be used so that the model just adds more trees to the existing model each time. Use the `set_params` method to update the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:02.805018Z",
     "start_time": "2021-09-24T17:36:02.794019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress warnings about too few trees from the early models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:07.221036Z",
     "start_time": "2021-09-24T17:36:02.815017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the random forest estimator\n",
    "# Note that the number of trees is not setup here\n",
    "RF = RandomForestClassifier(oob_score=True, \n",
    "                            random_state=42, \n",
    "                            warm_start=True,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "oob_list = list()\n",
    "\n",
    "# Iterate through all of the possibilities for \n",
    "# number of trees\n",
    "for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n",
    "    \n",
    "    # Use this to set the number of trees\n",
    "    RF.set_params(n_estimators=n_trees)\n",
    "\n",
    "    # Fit the model\n",
    "    RF.fit(X_train, y_train)\n",
    "\n",
    "    # Get the oob error\n",
    "    oob_error = 1 - RF.oob_score_\n",
    "    \n",
    "    # Store it\n",
    "    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n",
    "\n",
    "rf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n",
    "\n",
    "rf_oob_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error looks like it has stabilized around 100-150 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:07.689699Z",
     "start_time": "2021-09-24T17:36:07.226039Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:08.237041Z",
     "start_time": "2021-09-24T17:36:07.689699Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_palette('dark')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = rf_oob_df.plot(legend=False, marker='o')\n",
    "ax.set(ylabel='out-of-bag error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Repeat question 3 using extra randomized trees (`ExtraTreesClassifier`). Note that the `bootstrap` parameter will have to be set to `True` for this model.\n",
    "* Compare the out-of-bag errors for the two different types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:12.197110Z",
     "start_time": "2021-09-24T17:36:08.239042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the random forest estimator\n",
    "# Note that the number of trees is not setup here\n",
    "EF = ExtraTreesClassifier(oob_score=True, \n",
    "                          random_state=42, \n",
    "                          warm_start=True,\n",
    "                          bootstrap=True,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "oob_list = list()\n",
    "\n",
    "# Iterate through all of the possibilities for \n",
    "# number of trees\n",
    "for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n",
    "    \n",
    "    # Use this to set the number of trees\n",
    "    EF.set_params(n_estimators=n_trees)\n",
    "    EF.fit(X_train, y_train)\n",
    "\n",
    "    # oob error\n",
    "    oob_error = 1 - EF.oob_score_\n",
    "    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n",
    "\n",
    "et_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n",
    "\n",
    "et_oob_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the two dataframes into a single one for easier plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:12.229111Z",
     "start_time": "2021-09-24T17:36:12.200110Z"
    }
   },
   "outputs": [],
   "source": [
    "oob_df = pd.concat([rf_oob_df.rename(columns={'oob':'RandomForest'}),\n",
    "                    et_oob_df.rename(columns={'oob':'ExtraTrees'})], axis=1)\n",
    "\n",
    "oob_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model performs consistently better than the extra randomized trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:12.809155Z",
     "start_time": "2021-09-24T17:36:12.232111Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_palette('dark')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = oob_df.plot(marker='o')\n",
    "ax.set(ylabel='out-of-bag error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "* Select one of the models that performs well and calculate error metrics and a confusion matrix on the test data set. \n",
    "* Given the distribution of the predicted class, which metric is most important? Which could be deceiving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:13.171163Z",
     "start_time": "2021-09-24T17:36:12.812158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest with 100 estimators\n",
    "model = RF.set_params(n_estimators=100)\n",
    "y_test = y_test.to_numpy()\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, recall is rather poor for the customers who churned (True) class since they are quite small. We are doing better than random guessing, though, as the accuracy is 0.932 (vs 0.85 for random guessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:13.218209Z",
     "start_time": "2021-09-24T17:36:13.174164Z"
    }
   },
   "outputs": [],
   "source": [
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)\n",
    "score_df = pd.DataFrame({'accuracy': accuracy_score(y_test, y_pred),\n",
    "                         'precision': precision_score(y_test, y_pred),\n",
    "                         'recall': recall_score(y_test, y_pred),\n",
    "                         'f1': f1_score(y_test, y_pred),\n",
    "                         'auc': roc_auc_score(y_test.astype(int), y_pred.astype(int))},\n",
    "                         index=pd.Index([0]))\n",
    "\n",
    "print(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Print or visualize the confusion matrix.\n",
    "* Plot the ROC-AUC and precision-recall curves.\n",
    "* Plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:13.599214Z",
     "start_time": "2021-09-24T17:36:13.221209Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n",
    "labels = ['False', 'True']\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_yticklabels(labels[::-1]);\n",
    "ax.set_ylabel('Prediction');\n",
    "ax.set_xlabel('Ground Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC and precision-recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:14.976342Z",
     "start_time": "2021-09-24T17:36:13.602216Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "fig, axList = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(11, 5)\n",
    "\n",
    "# Get the probabilities for each of the two categories\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "# Plot the ROC-AUC curve\n",
    "ax = axList[0]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1])\n",
    "ax.plot(fpr, tpr)\n",
    "# It is customary to draw a diagonal dotted line in ROC plots.\n",
    "# This is to indicate completely random prediction. Deviation from this\n",
    "# dotted line towards the upper left corner signifies the power of the model.\n",
    "ax.plot([0, 1], [0, 1], ls='--', color='black', lw=.3)\n",
    "ax.set(xlabel='False Positive Rate',\n",
    "       ylabel='True Positive Rate',\n",
    "       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n",
    "       title='ROC curve')\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "ax = axList[1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob[:,1])\n",
    "ax.plot(recall, precision)\n",
    "ax.set(xlabel='Recall', ylabel='Precision',\n",
    "       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n",
    "       title='Precision-Recall curve')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances. Total daily cost is the biggest predictor of customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:15.671426Z",
     "start_time": "2021-09-24T17:36:14.981344Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "ax = feature_imp.plot(kind='bar')\n",
    "ax.set(ylabel='Relative Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
