{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel® Neural Compressor Sample for TensorFlow*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Train a convolutional neural network (CNN) model by using Keras\n",
    "- Quantize the Keras model by using Intel® Neural Compressor\n",
    "- Compare the quantized model with the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import python packages and verify that the correct versions are installed.\n",
    "\n",
    "Required packages:\n",
    "- TensorFlow 2.2 and later\n",
    "- Intel® Neural Compressor 1.2.1 and later\n",
    "- Matplotlib\n",
    "\n",
    "**Note**: This code sample supports both the current package name for Intel® Neural Compressor (**neural_compressor**) and the old names (**lpot**, **ilit**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "try:\n",
    "    import neural_compressor as inc\n",
    "    print(\"neural_compressor version {}\".format(inc.__version__))  \n",
    "except:\n",
    "    try:\n",
    "        import lpot as inc\n",
    "        print(\"LPOT version {}\".format(inc.__version__)) \n",
    "    except:\n",
    "        import ilit as inc\n",
    "        print(\"iLiT version {}\".format(inc.__version__))       \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Intel Optimized TensorFlow 2.5.0 and later, you must set the **TF_ENABLE_MKL_NATIVE_FORMAT=0** environment variable before running Intel® Neural Compressor to quantize FP32 model or deploying the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TF_ENABLE_MKL_NATIVE_FORMAT=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN Model Based on Keras\n",
    "\n",
    "We prepared the `alexnet.py` script with the functions for training a CNN model.\n",
    "\n",
    "### Dataset\n",
    "Use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits. \n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alexnet\n",
    " \n",
    "data = alexnet.read_data()\n",
    "x_train, y_train, label_train, x_test, y_test, label_test = data\n",
    "print('train', x_train.shape, y_train.shape, label_train.shape)\n",
    "print('test', x_test.shape, y_test.shape, label_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "Build a CNN model like Alexnet by using Keras API based on TensorFlow.\n",
    "Use the Keras `summary()` method to print the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "width = 28\n",
    "channels = 1\n",
    "\n",
    "model = alexnet.create_model(width ,channels ,classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model with the Dataset\n",
    "\n",
    "Set the **epochs** parameter to **3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "alexnet.train_mod(model, data, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze and Save Model to Single PB\n",
    "\n",
    "Set the input node name to **x**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "def save_frezon_pb(model, mod_path):\n",
    "    # Convert Keras model to ConcreteFunction\n",
    "    full_model = tf.function(lambda x: model(x))\n",
    "    concrete_function = full_model.get_concrete_function(\n",
    "        x=tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n",
    "\n",
    "    # Get frozen ConcreteFunction\n",
    "    frozen_model = convert_variables_to_constants_v2(concrete_function)\n",
    "\n",
    "    # Generate frozen pb\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_model.graph,\n",
    "                      logdir=\".\",\n",
    "                      name=mod_path,\n",
    "                      as_text=False)\n",
    "fp32_frezon_pb_file = \"fp32_frezon.pb\"\n",
    "save_frezon_pb(model, fp32_frezon_pb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la fp32_frezon.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize FP32 Model by Using Intel® Neural Compressor\n",
    "\n",
    "Intel® Neural Compressor can quantize the model with a validation dataset for tuning.\n",
    "As a result, it returns a frozen quantized INT8 model.\n",
    "\n",
    "We prepare a python script `inc_quantize_model.py` to call Intel® Neural Compressor to finish the all quantization job.\n",
    "See the following code sample for explanations.\n",
    "\n",
    "### Define Dataloader\n",
    "\n",
    "The class **Dataloader** provides an iter function to return the image and label as batch size.\n",
    "We uses the validation data of MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_dataset\n",
    "import math\n",
    "\n",
    "\n",
    "class Dataloader(object):\n",
    "  def __init__(self, batch_size):\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "\n",
    "  def __iter__(self):\n",
    "    x_train, y_train, label_train, x_test, y_test,label_test = mnist_dataset.read_data()\n",
    "    batch_nums = math.ceil(len(x_test)/self.batch_size)\n",
    "\n",
    "    for i in range(batch_nums-1):\n",
    "        begin = i*self.batch_size\n",
    "        end = (i+1)*self.batch_size\n",
    "        yield x_test[begin: end], label_test[begin: end]\n",
    "\n",
    "    begin = (batch_nums-1)*self.batch_size\n",
    "    yield x_test[begin:], label_test[begin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Load FP32 Model\n",
    "Load the FP32 model that we saved in the previous step. See the `alexnet.load_pb(in_model)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat alexnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Yaml File\n",
    "\n",
    "We created `alexnet.yaml` to save the necessary parameters for Intel® Neural Compressor.\n",
    "In this case, we only need to change the input/output according to the FP32 model.\n",
    "\n",
    "The input node name is **x**.\n",
    "\n",
    "The output name is **Identity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat alexnet.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tuning Function\n",
    "We follow the template to create the tuning function. The function will return a frozen quantized model (INT8 model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def auto_tune(input_graph_path, yaml_config, batch_size):    \n",
    "    fp32_graph = alexnet.load_pb(input_graph_path)\n",
    "    quan = inc.Quantization(yaml_config)\n",
    "    dataloader = Dataloader(batch_size)\n",
    "    assert(dataloader)\n",
    "    q_model = quan(\n",
    "                        fp32_graph,\n",
    "                        q_dataloader=dataloader,\n",
    "                        eval_func=None,\n",
    "                        eval_dataloader=dataloader)\n",
    "    return q_model\n",
    "\n",
    "\n",
    "def save_int8_frezon_pb(q_model, path):\n",
    "    from tensorflow.python.platform import gfile\n",
    "    f = gfile.GFile(path, 'wb')\n",
    "    f.write(q_model.as_graph_def().SerializeToString())\n",
    "    print(\"Save to {}\".format(path))\n",
    "    \n",
    "yaml_file = \"alexnet.yaml\"\n",
    "batch_size = 200\n",
    "int8_pb_file = \"alexnet_int8_model.pb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Function to Quantize the Model\n",
    "\n",
    "Show the code in `inc_quantize_model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat inc_quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will execute `inc_quantize_model.py` to show the whole process of quantizing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python inc_quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script creates the file `alexnet_int8_model.pb` that contains the quantized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Quantized Model\n",
    "\n",
    "We prepare the script `profiling_inc.py` to test the performance of the PB model.\n",
    "\n",
    "If we run the code in the jupyter notebook, we will not get the correct performance data. So we run the script as a process.\n",
    "\n",
    "Let's take a look at `profiling_inc.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat profiling_inc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute `profiling_inc.py` with the FP32 model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python profiling_inc.py --input-graph=./fp32_frezon.pb --omp-num-threads=4 --num-inter-threads=1 --num-intra-threads=4 --index=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute `profiling_inc.py` with the INT8 model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python profiling_inc.py --input-graph=./alexnet_int8_model.pb --omp-num-threads=4 --num-inter-threads=1 --num-intra-threads=4 --index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat 32.json\n",
    "!echo \" \"\n",
    "!cat 8.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the functions to load and show the performance data from the `32.json` and `8.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def autolabel(ax, rects):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%0.2f' % float(height),\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "def draw_bar(x, t, y, subplot, color, x_lab, y_lab, width=0.2):\n",
    "    plt.subplot(subplot)\n",
    "    plt.xticks(x, t)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.set_xlabel(x_lab)\n",
    "    ax1.set_ylabel(y_lab, color=color)\n",
    "    rects1 = ax1.bar(x, y, color=color, width=width)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    autolabel(ax1, rects1)\n",
    "\n",
    "def load_res(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "\n",
    "res_32 = load_res('32.json')\n",
    "res_8 = load_res('8.json')\n",
    "   \n",
    "accuracys = [res_32['accuracy'], res_8['accuracy']]\n",
    "throughputs = [res_32['throughput'], res_8['throughput']]             \n",
    "latencys = [res_32['latency'], res_8['latency']]\n",
    "\n",
    "print('throughputs', throughputs)\n",
    "print('latencys', latencys)\n",
    "print('accuracys', accuracys)\n",
    "\n",
    "accuracys_perc = [accu*100 for accu in accuracys]\n",
    "\n",
    "t = ['FP32', 'INT8']\n",
    "x = [0, 1]\n",
    "plt.figure(figsize=(16,6))\n",
    "draw_bar(x, t, throughputs, 131, 'tab:green', 'Throughput(fps)', '', width=0.2)\n",
    "draw_bar(x, t,  latencys, 132, 'tab:blue', 'Latency(s)', '', width=0.2)\n",
    "draw_bar(x, t,  accuracys_perc, 133, '#28a99d', 'Accuracys(%)', '', width=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 vs INT8\n",
    "\n",
    "Compare the performance data of the INT8 model with that of the FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs_times = [1, throughputs[1]/throughputs[0]]\n",
    "latencys_times = [1, latencys[1]/latencys[0]]\n",
    "accuracys_times = [0, accuracys_perc[1] - accuracys_perc[0]]\n",
    "\n",
    "print('throughputs_times', throughputs_times)\n",
    "print('latencys_times', latencys_times)\n",
    "print('accuracys_times', accuracys_times)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "draw_bar(x, t, throughputs_times, 131, 'tab:green', 'Throughput Comparison (big is better)', '', width=0.2)\n",
    "draw_bar(x, t, latencys_times, 132, 'tab:blue', 'Latency Comparison (small is better)', '', width=0.2)\n",
    "draw_bar(x, t, accuracys_times, 133, '#28a99d', 'Accuracys Loss(%)', '', width=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code sample we have compared performance of the FP32 and INT8 models and demonstrated that the INT8 model is faster.\n",
    "\n",
    "The Second Generation Intel® Xeon® Scalable processors provide Intel® Deep Learning Boost that speeds up the INT8 inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
