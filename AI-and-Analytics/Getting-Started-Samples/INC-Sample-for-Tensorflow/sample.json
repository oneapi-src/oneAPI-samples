{
 "guid": "82e7612f-2810-4d12-9c75-c17fcbb946fa",
 "name": "Intel® Neural Compressor Accelerate Inference with Intel® Optimization for TensorFlow*",
 "categories": ["Toolkit/oneAPI AI And Analytics/Getting Started"],
 "description": "This sample illustrates how to run Intel® Neural Compressor to quantize the FP32 model trained by Keras on Tensorflow to INT8 model to speed up the inference.",
 "languages": [{"python":{}}],
 "dependencies": ["tensorflow","neural-compressor"],
 "os": ["linux"],
 "builder": ["cli"],
 "targetDevice": ["CPU"],
 "ciTests": {
	"linux": [
	{
		"env": ["pixi shell --manifest-path ../../Preset_environments/deep_learning_tensorflow_cpu/pixi.toml",
				"pixi add --pypi --feature test_sample pip",
				"pixi project environment add test --feature test_sample",
				"pixi shell -e test",
				"pip install jupyter ipykernel",
				"python -m ipykernel install --user --name=tensorflow"
        ],
		"id": "neural-compressor tensorflow",
		"steps": [
			"jupyter nbconvert --ExecutePreprocessor.enabled=True --ExecutePreprocessor.kernel_name=tensorflow --to notebook inc_sample_tensorflow.ipynb",
			"pixi shell -e default",
			"pixi project environment remove test",
			"pixi remove --pypi --feature test_sample 'pip'",
			"rm -rf ../../Preset_environments/deep_learning_tensorflow_cpu/.pixi/envs/test"
		 ]
	}
    ]
 },
 "expertise": "Getting Started"
}
