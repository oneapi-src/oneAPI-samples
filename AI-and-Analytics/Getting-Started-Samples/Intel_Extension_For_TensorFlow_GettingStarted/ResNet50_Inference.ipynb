{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Getting Started with Intel(R) Extension for TensorFlow\n",
    "This code sample will guide users how to run a tensorflow inference workload on both GPU and CPU by using oneAPI AI Analytics Toolkit and also analyze the GPU and CPU usage via oneDNN verbose logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50 Inference on both GPU and CPU\n",
    "***\n",
    "This section shows users how to run resnet50 inference on both GPU and CPU without code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the installation path of your Intel(R) oneAPI AI Analytics toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ONEAPI_INSTALL=/opt/intel/oneapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the resnet50 inference sample from ITEX github repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/intel/intel-extension-for-tensorflow/main/examples/infer_resnet50/infer_resnet50.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check TensorFlow* and ITEX verson in current ipython kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorTlow version:  2.10.0\n",
      "MKL enabled : False\n",
      "itex_version :  1.1.0\n",
      "scikit learn Version:  1.1.1\n",
      "neural_compressor version 1.14.2\n",
      "Arch :  ICX|CLX\n"
     ]
    }
   ],
   "source": [
    "run ../../version_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run resnet50 on GPU and CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on GPU via ITEX\n",
    "With ITEX, users could run infer_resnet50.py on Intel dGPU without any code change.\n",
    "There is a tensorflow-gpu conda environment with ITEX installation in current AI Kit installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "source activate user-tensorflow-gpu\n",
    "echo \"########## Executing the run\"\n",
    "DNNL_VERBOSE=1 python infer_resnet50.py > infer_rn50_gpu.csv\n",
    "echo \"########## Done with the run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submitting build.sh and run.sh to the job queue\n",
    "\n",
    "Now we can submit build.sh and run.sh to the job queue.\n",
    "\n",
    "Note that it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts either on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command qsub. If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 ../../q; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ];  then  ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on CPU via Intel TensorFlow\n",
    "Users also can run the same infer_resnet50.py on CPU with intel tensorflow or stock tensorflow. Please switch to the user-tensorflow jupyter kernel and execute again from prerequisites for CPU run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "source activate user-tensorflow\n",
    "echo \"########## Executing the run\"\n",
    "DNNL_VERBOSE=1 python infer_resnet50.py > infer_rn50_cpu.csv\n",
    "echo \"########## Done with the run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submitting build.sh and run.sh to the job queue\n",
    "\n",
    "Now we can submit build.sh and run.sh to the job queue.\n",
    "\n",
    "NOTE - it is possible to execute any of the build and run commands in local environments.\n",
    "To enable users to run their scripts either on the Intel DevCloud or in local environments, this and subsequent training checks for the existence of the job submission command qsub. If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 ../../q; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ];  then  ./q run.sh; else ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Verbose Logs\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download profile_utils.py to parse oneDNN verbose logs from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/oneapi-src/oneAPI-samples/master/Libraries/oneDNN/tutorials/profiling/profile_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: List out all oneDNN verbose logs\n",
    "users should see two verbose logs listed in the table below.\n",
    "\n",
    "|Log File Name | Description |\n",
    "|:-----|:----|\n",
    "|infer_rn50_cpu.csv| log for cpu run |\n",
    "|infer_rn50_gpu.csv| log for gpu run|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames= os.listdir (\".\") \n",
    "result = []\n",
    "keyword = \".csv\"\n",
    "for filename in filenames: \n",
    "    #if os.path.isdir(os.path.join(os.path.abspath(\".\"), filename)): \n",
    "    if filename.find(keyword) != -1:\n",
    "        result.append(filename)\n",
    "result.sort()\n",
    "\n",
    "index =0 \n",
    "for folder in result:\n",
    "    print(\" %d : %s \" %(index, folder))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Pick a verbose log by putting its index value below\n",
    "Users can pick either cpu or gpu log for analysis.   \n",
    "Once users finish Step 2 to Step 7 for one log file, they can go back to step 2 and select another log file for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FdIndex=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Parse verbose log and get the data back\n",
    "> Users will also get a oneDNN.json file with timeline information for oneDNN primitives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = result[FdIndex]\n",
    "print(logfile)\n",
    "from profile_utils import oneDNNUtils, oneDNNLog\n",
    "onednn = oneDNNUtils()\n",
    "log1 = oneDNNLog()\n",
    "log1.load_log(logfile)\n",
    "data = log1.data\n",
    "exec_data = log1.exec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Time breakdown for exec type\n",
    "The exec type includes exec and create. \n",
    "\n",
    "|exec type | Description |  \n",
    "|:-----|:----|  \n",
    "|exec | Time for primitives exection. Better to spend most of time on primitives execution. |  \n",
    "|create| Time for primitives creation. Primitives creation happens once. Better to spend less time on primitive creation. |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Time breakdown for architecture type\n",
    "The supported architectures include CPU and GPU.  \n",
    "For this simple net sample, we don't split computation among CPU and GPU,    \n",
    "so users should see either 100% CPU time or 100% GPU time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data,\"arch\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6: Time breakdown for primitives type\n",
    "The primitives type includes convolution, reorder, sum, etc.  \n",
    "For this simple convolution net example, convolution and inner product primitives are expected to spend most of time.  \n",
    "However, the exact time percentage of different primitivies may vary among different architectures.    \n",
    "Users can easily identify top hotpots of primitives executions with this time breakdown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data,\"type\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:  Time breakdown for JIT kernel type\n",
    "oneDNN uses just-in-time compilation (JIT) to generate optimal code for some functions based on input parameters and instruction set supported by the system.   \n",
    "Therefore, users can see different JIT kernel type among different CPU and GPU architectures.  \n",
    "For example, users can see avx_core_vnni JIT kernel if the workload uses VNNI instruction on Cascake Lake platform.  \n",
    "Users can also see different OCL kernels among different Intel GPU generations.  \n",
    "Moreover, users can identify the top hotspots of JIT kernel executions with this time breakdown.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onednn.breakdown(exec_data,\"jit\",\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output(both stdout and stderr) is displayed on the command line console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[CODE_SAMPLE_COMPLETED_SUCCESFULLY]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
