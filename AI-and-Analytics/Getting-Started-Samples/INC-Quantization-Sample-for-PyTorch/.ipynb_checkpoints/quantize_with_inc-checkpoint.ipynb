{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd4c355-e046-4c24-bcb5-92165c79ffbc",
   "metadata": {},
   "source": [
    "# Quantize PyTorch Models with Intel® Neural Compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a911a17-49b6-4412-8a90-390bb0c7b442",
   "metadata": {},
   "source": [
    "In this notebook we will look at a real-world use case of text classification using a Huggingface model. We will first use a stock FP32 PyTorch model to generate predictions. Then, we will perform INT8 Quantization with easy-to-use APIs provided by INC to see how speedups can be gained over stock PyTorch on Intel® hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405d69f-e912-4f89-9dec-6a110099fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b282ca-adc1-4793-beb4-e4827e9b83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "import neural_compressor # Intel® Neural Compressor is an open-source Python library that supports popular model compression techniques\n",
    "#######################################################################################################################################\n",
    "\n",
    "# check if Neural Compressor is above v2.0\n",
    "assert float(neural_compressor.__version__) >= 2.0, \"The below APIs work with Intel® Neural Compressor 2.0 and above\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b8c21-93f5-43f0-bde0-6ce6e00909a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constant seed for reprodcibility\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d64e53-fe42-41b1-92c9-766fb15057e9",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c1a70-47f0-4bb9-be39-9bd59274b140",
   "metadata": {},
   "source": [
    "Some functions to help us with loading the model and summarizing the optimizations. The functions below will help us record the time taken to run and, plot comparison charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd0588-b4e8-4423-a730-fb39d30b5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_inference_time(model, data):\n",
    "    \"\"\"\n",
    "    does a model warm up and times the model runtime\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # warm up\n",
    "        for _ in range(25):\n",
    "            model(input_ids=data[0], attention_mask=data[1])\n",
    "\n",
    "        # measure\n",
    "        import time\n",
    "        start = time.time()\n",
    "        for _ in range(25):\n",
    "            output = model(input_ids=data[0], attention_mask=data[1])\n",
    "        end = time.time()\n",
    "        average_inference_time = (end-start)/25*1000\n",
    "    \n",
    "    return average_inference_time\n",
    "\n",
    "def plot_speedup(inference_time_stock, inference_time_optimized):\n",
    "    \"\"\"\n",
    "    Plots a bar chart comparing the time taken by stock PyTorch model and time taken by\n",
    "    the quantized model\n",
    "    \"\"\"\n",
    "    data = {'FP32': inference_time_stock, 'INT8': inference_time_optimized}\n",
    "    model_type = list(data.keys())\n",
    "    times = list(data.values())\n",
    "\n",
    "    fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "    # creating the bar plot\n",
    "    plt.bar(model_type, times, color ='blue',\n",
    "            width = 0.4)\n",
    "\n",
    "    plt.ylabel(\"Runtime (ms)\")\n",
    "    plt.title(f\"Speedup acheived - {inference_time_stock/inference_time_optimized:.2f}x\")\n",
    "    plt.savefig('inc_speedup.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940a0fd-5172-4cee-bbbe-fb203ea77ca4",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58803daa-f38b-45d4-b0a6-cfdedcedbb40",
   "metadata": {},
   "source": [
    "Instantiate a FP32 BERT model finetuned on the IMDB dataset from Huggingface. In a real-world use-case this could be any model that has been trained and is ready to be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b4070-349d-4a13-b5b0-71fa32172585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_name = \"JiaqiLee/imdb-finetuned-bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52c79d-4ae8-41f2-9802-898d425a0503",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a9d31-1958-45a5-b1d2-57d4f00aa05f",
   "metadata": {},
   "source": [
    "We will use the IMDB dataset from the Huggingface datasets library to demonstrate the role that the dataset plays in quantizing the model using INC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f841f-578e-4d04-bb3f-1959882cddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# data = load_dataset(\"sms_spam\")\n",
    "data = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da75c1-61d1-487c-9f2e-cd999833e709",
   "metadata": {},
   "source": [
    "Let's look at what one example from the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e385a-61f4-43f9-ae7a-ecf8a17e3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'][20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524b269-f5d6-43c9-8f75-f5c044e4740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The torch.utils.data.Dataset class IMDBDataset allows us to prepare a tokenized dataset\n",
    "from dataset import IMDBDataset\n",
    "\n",
    "text = data['test']['text']\n",
    "labels = data['test']['label'] \n",
    "\n",
    "test_dataset = IMDBDataset(text, labels, tokenizer=tokenizer, data_size=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7c2e6-ab96-4d49-8e39-4eb495ff2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ec2d6-894e-44e8-9172-9e68e195a657",
   "metadata": {},
   "source": [
    "**Evaluation Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46239a4d-c389-4fa3-bde1-699b27acbdc7",
   "metadata": {},
   "source": [
    "The eval function is an important part of quantizing with INC. It contains metrics that we care about and want to preserve as much as possible post quantization, for example - accuracy, f1score etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6ae8c-0307-4157-a1d8-e0c6f52fba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following eval function computes accuracy\n",
    "def eval_func(model_q):\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    for _, batch in enumerate(test_loader):\n",
    "        inputs, labels = batch\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        pred = model_q(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "        )\n",
    "        # save predictions and labels for all loops to calculate accuracy later\n",
    "        test_preds.extend(pred.logits.argmax(-1))\n",
    "        test_labels.extend(labels)\n",
    "    return accuracy_score(test_preds, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55172db6-590a-47a9-b1ed-9167c4637263",
   "metadata": {},
   "source": [
    "Data for Benchmarking - Let's pick one sample from the test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7813321-8d8f-4098-a6c6-fe2a17e4d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "data = (batch[0]['input_ids'], batch[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a879df9-12f1-4484-8e23-202e2c1fc7e5",
   "metadata": {},
   "source": [
    "Benchmark Stock PyTorch Model (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf62eb-e277-4f95-8327-07cee7ae912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_time_stock = get_average_inference_time(model.eval(), data)\n",
    "\n",
    "print(f\"time taken for forward pass: {inference_time_stock} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa612b-f332-44f5-8890-edeb43d0aa8b",
   "metadata": {},
   "source": [
    "**Quantization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73e47e-2855-492b-9865-80a3848b586e",
   "metadata": {},
   "source": [
    "Quantization is a very popular deep learning model optimization technique for improving inference speeds. It minimizes the number of bits required to represent either the weights or activations in a NN. This is done by converting a set of real-valued numbers into their lower bit data representations, such as int8 and int4, mainly during the inference phase with minimal to no loss in accuracy.\n",
    "\n",
    "Intel® Neural Compressor provides three types of Quantization APIs:\n",
    "- post training dynamic quantization\n",
    "- post training static quantization\n",
    "- quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1dbc2-2167-4fbe-9cff-066088c1fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "from neural_compressor.quantization import fit\n",
    "from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion\n",
    "#######################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44154cd8-ff9f-440e-bc9c-2e2f67d84301",
   "metadata": {},
   "source": [
    "Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa89f2-db36-47a9-97d3-0b6bb345b363",
   "metadata": {},
   "source": [
    "The weights and activations of the neural network get quantized into int8 format from float32 format offline (post training)\n",
    "\n",
    "For this we need the min/max range of the bit representation. These are collected during inference runtime i.e when the data is passed through the model. We will see that for Static Quantization, this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3f411-3c57-49d4-a7d6-cb904a8e4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_criterion = TuningCriterion(max_trials=5)\n",
    "accuracy_criterion = AccuracyCriterion(tolerable_loss=0.73, criterion='absolute')\n",
    "conf = PostTrainingQuantConfig(approach=\"dynamic\", tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion)\n",
    "q_model = fit(model, conf=conf, eval_func=eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135345e-024c-4ec2-9f0f-1ef2daf628b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_time_optimized = get_average_inference_time(q_model.eval(), data)\n",
    "\n",
    "print(f\"time taken for forward pass: {inference_time_optimized} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e55ae5-efb5-4a00-b3b7-05eb573cd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance gain bar chart\n",
    "\n",
    "plot_speedup(inference_time_stock, inference_time_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a007a-fb34-4d6f-a6a5-c1673b0cd1a4",
   "metadata": {},
   "source": [
    "Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe6b74-970d-4b77-b43a-642e1f33016c",
   "metadata": {},
   "source": [
    "The weights and activations of the neural network get quantized into int8 format from float32 format offline (post training)\n",
    "\n",
    "For this we need the min/max range of the bit representation. These are collected using a calibration dataset. The calibration dataset should be able to represent the data distribution of unseen data. The calibration process runs on the original fp32 model and dumps out all the tensor distributions for Scale and ZeroPoint calculations. Usually preparing 100 samples are enough for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41d131-e900-4079-98f7-001fca3f9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_criterion = TuningCriterion(max_trials=5)\n",
    "conf = PostTrainingQuantConfig(approach=\"static\")\n",
    "q_model = fit(model, conf=conf, eval_func=eval_func, calib_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0ffca-aa6f-46ad-8f4c-55822afacf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_time_optimized = get_average_inference_time(q_model.eval(), image_channels_last)\n",
    "\n",
    "print(f\"time taken for forward pass: {inference_time_optimized} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec931ac7-3574-46f2-abd7-fa18e2fe4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance gain bar chart\n",
    "\n",
    "plot_speedup(inference_time_stock, inference_time_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56acf6-4a6b-43d3-a9f3-ffad63be0dc5",
   "metadata": {},
   "source": [
    "Intel® Neural Compressor provides a host of model compression tecnhiques apart from Quantization such as Advanced Mixed Precision, Pruning (Sparsity), Distillation, Orchestration, Benchmarking etc.\n",
    "\n",
    "Please visit our [GitHub](https://github.com/intel/neural-compressor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
