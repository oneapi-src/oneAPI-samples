{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel速 Low Precision Optimization Tool (iLiT) Sample for Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Train a CNN Model Based on Keras\n",
    "- Quantize Keras Model by ilit\n",
    "- Compare Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages and check version.\n",
    "\n",
    "Make sure the Tensorflow is **2.2** and iLiT, matplotlib are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ilit\n",
    "print(ilit.__path__)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN Model Based on Keras\n",
    "\n",
    "We prepare a script '**alexnet.py**' to provide the functions to train a CNN model.\n",
    "\n",
    "### Dataset\n",
    "Use [MNIST](http://yann.lecun.com/exdb/mnist/) dataset to recognize hand writing numbers. \n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alexnet\n",
    " \n",
    "data = alexnet.read_data()\n",
    "x_train, y_train, label_train, x_test, y_test, label_test = data\n",
    "print('train', x_train.shape, y_train.shape, label_train.shape)\n",
    "print('test', x_test.shape, y_test.shape, label_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "Build a CNN model like Alexnet by Keras API based on Tensorflow.\n",
    "Print the model structure by Keras API: summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "width = 28\n",
    "channels = 1\n",
    "\n",
    "model = alexnet.create_model(width ,channels ,classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model with the Dataset\n",
    "\n",
    "Set the **epochs** to \"**3**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "alexnet.train_mod(model, data, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze and Save Model to Single PB\n",
    "\n",
    "Set the input node name is \"**x**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "def save_frezon_pb(model, mod_path):\n",
    "    # Convert Keras model to ConcreteFunction\n",
    "    full_model = tf.function(lambda x: model(x))\n",
    "    concrete_function = full_model.get_concrete_function(\n",
    "        x=tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n",
    "\n",
    "    # Get frozen ConcreteFunction\n",
    "    frozen_model = convert_variables_to_constants_v2(concrete_function)\n",
    "\n",
    "    # Generate frozen pb\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_model.graph,\n",
    "                      logdir=\".\",\n",
    "                      name=mod_path,\n",
    "                      as_text=False)\n",
    "fp32_frezon_pb_file = \"fp32_frezon.pb\"\n",
    "save_frezon_pb(model, fp32_frezon_pb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la fp32_frezon.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize FP32 Model by iLiT\n",
    "\n",
    "iLiT supports to quantize the model with a validation dataset for tuning.\n",
    "Finally, it returns an frezon quantized model based on int8.\n",
    "\n",
    "We prepare a python script \"**ilit_quantize_model.py**\" to call iLiT to finish the all quantization job.\n",
    "Following code sample is used to explain the code.\n",
    "\n",
    "### Define Dataloader\n",
    "\n",
    "The class **Dataloader** provides an iter function to return the image and label as batch size.\n",
    "We uses the validation data of MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_dataset\n",
    "import math\n",
    "\n",
    "\n",
    "class Dataloader(object):\n",
    "  def __init__(self, batch_size):\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "\n",
    "  def __iter__(self):\n",
    "    x_train, y_train, label_train, x_test, y_test,label_test = mnist_dataset.read_data()\n",
    "    batch_nums = math.ceil(len(x_test)/self.batch_size)\n",
    "\n",
    "    for i in range(batch_nums-1):\n",
    "        begin = i*self.batch_size\n",
    "        end = (i+1)*self.batch_size\n",
    "        yield x_test[begin: end], label_test[begin: end]\n",
    "\n",
    "    begin = (batch_nums-1)*self.batch_size\n",
    "    yield x_test[begin:], label_test[begin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Load FP32 Model\n",
    "Load the saved fp32 model in previous step.\n",
    "\n",
    "It's defined as alexnet.load_pb(in_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat alexnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Yaml File\n",
    "\n",
    "We define alexnet.yaml to save the necessary parameters for iLiT.\n",
    "In this case, we only need to change the input/output according to the fp32 model.\n",
    "\n",
    "In this case, the input node name is '**x**'.\n",
    "\n",
    "Output name is '**Identity**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat alexnet.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tuning Function\n",
    "We follow the template to create the tuning function. The function will return a frezon quantized model (int8 model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ilit\n",
    "\n",
    "def auto_tune(input_graph_path, yaml_config, batch_size):    \n",
    "    fp32_graph = alexnet.load_pb(input_graph_path)\n",
    "    tuner = ilit.Tuner(yaml_config)\n",
    "    assert(tuner)\n",
    "    dataloader = Dataloader(batch_size)\n",
    "    assert(dataloader)\n",
    "    q_model = tuner.tune(\n",
    "                        fp32_graph,\n",
    "                        q_dataloader=dataloader,\n",
    "                        eval_func=None,\n",
    "                        eval_dataloader=dataloader)\n",
    "    return q_model\n",
    "\n",
    "\n",
    "def save_int8_frezon_pb(q_model, path):\n",
    "    from tensorflow.python.platform import gfile\n",
    "    f = gfile.GFile(path, 'wb')\n",
    "    f.write(q_model.as_graph_def().SerializeToString())\n",
    "    print(\"Save to {}\".format(path))\n",
    "    \n",
    "yaml_file = \"alexnet.yaml\"\n",
    "batch_size = 200\n",
    "int8_pb_file = \"alexnet_int8_model.pb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Function to Quantize the Model\n",
    "\n",
    "Show the code in \"**ilit_quantize_model.py**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat  ilit_quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will execute the \"**ilit_quantize_model.py**\" to show the whole process of quantizing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ilit_quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a quantized model file \"**alexnet_int8_model.pb**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Quantized Model\n",
    "\n",
    "Define a function to return validation dataset and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def val_data():\n",
    "    x_train, y_train, label_train, x_test, y_test,label_test = mnist_dataset.read_data()\n",
    "    return x_test, y_test, label_test\n",
    "\n",
    "def calc_accuracy(predictions, labels):\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    same = 0\n",
    "    for i, x in enumerate(predictions):\n",
    "        if x==labels[i]:\n",
    "            same += 1\n",
    "    if len(predictions)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return same/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define infer function to test the single frezon PB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def calc_accuracy(predictions, labels):\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    same = 0\n",
    "    for i, x in enumerate(predictions):\n",
    "        if x==labels[i]:\n",
    "            same += 1\n",
    "    if len(predictions)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return same/len(predictions)\n",
    "\n",
    "def get_concrete_function(graph_def, inputs, outputs, print_graph=False):\n",
    "    def imports_graph_def():\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "    wrap_function = tf.compat.v1.wrap_function(imports_graph_def, [])\n",
    "    graph = wrap_function.graph\n",
    "\n",
    "    return wrap_function.prune(\n",
    "        tf.nest.map_structure(graph.as_graph_element, inputs),\n",
    "        tf.nest.map_structure(graph.as_graph_element, outputs))\n",
    "\n",
    "def infer_perf_pb(pb_model_file, inputs=[\"x:0\"], outputs=[\"Identity:0\"]):\n",
    "    q_model = alexnet.load_pb(pb_model_file)\n",
    "    concrete_function = get_concrete_function(graph_def=q_model.as_graph_def(),\n",
    "                                              inputs=inputs,\n",
    "                                              outputs=outputs,\n",
    "                                              print_graph=True)\n",
    "    x_test, y_test, label_test = val_data()\n",
    "\n",
    "    bt = time.time()\n",
    "    _frozen_graph_predictions = concrete_function(x=tf.constant(x_test))[0]\n",
    "    et = time.time()\n",
    "\n",
    "    accuracy = calc_accuracy(_frozen_graph_predictions, label_test)\n",
    "    print('accuracy:', accuracy)\n",
    "    throughput = x_test.shape[0] / (et - bt)\n",
    "    print('max throughput(fps):', throughput)\n",
    "\n",
    "\n",
    "    #latency when BS=1\n",
    "    bt = time.time()\n",
    "    times = 1000\n",
    "    for i in range(times):\n",
    "        _frozen_graph_predictions = concrete_function(x=tf.constant(x_test[:1]))[0]\n",
    "    et = time.time()\n",
    "\n",
    "    latency = (et - bt) * 1000 / times\n",
    "    print('latency(ms):', latency)\n",
    "\n",
    "    return accuracy, throughput, latency\n",
    "\n",
    "#warm up\n",
    "_accuracy32, _throughput32, _latency32 = infer_perf_pb(fp32_frezon_pb_file)\n",
    "\n",
    "#test\n",
    "accuracy32, throughput32, latency32 = infer_perf_pb(fp32_frezon_pb_file)\n",
    "\n",
    "accuracy8, throughput8, latency8 = infer_perf_pb(int8_pb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the functions to get the performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(ax, rects):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%0.2f' % float(height),\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "def draw_bar(x, t, y, subplot, color, x_lab, y_lab, width=0.2):\n",
    "    plt.subplot(subplot)\n",
    "    plt.xticks(x, t)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.set_xlabel(x_lab)\n",
    "    ax1.set_ylabel(y_lab, color=color)\n",
    "    rects1 = ax1.bar(x, y, color=color, width=width)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    autolabel(ax1, rects1)\n",
    "\n",
    "\n",
    "accuracys = [accuracy32, accuracy8]\n",
    "throughputs = [throughput32, throughput8]\n",
    "latencys = [latency32, latency8]\n",
    "print('throughputs', throughputs)\n",
    "print('latencys', latencys)\n",
    "print('accuracys', accuracys)\n",
    "\n",
    "accuracys_perc = [accu*100 for accu in accuracys]\n",
    "\n",
    "t = ['FP32', 'INT8']\n",
    "x = [0, 1]\n",
    "plt.figure(figsize=(16,6))\n",
    "draw_bar(x, t, throughputs, 131, 'tab:green', 'Throughput(fps)', '', width=0.2)\n",
    "draw_bar(x, t,  latencys, 132, 'tab:blue', 'Latency(s)', '', width=0.2)\n",
    "draw_bar(x, t,  accuracys_perc, 133, '#28a99d', 'Accuracys(%)', '', width=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 vs INT8\n",
    "\n",
    "Compare the performance data based on data of FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs_times = [1, throughputs[1]/throughputs[0]]\n",
    "latencys_times = [1, latencys[1]/latencys[0]]\n",
    "accuracys_times = [0, accuracys_perc[1] - accuracys_perc[0]]\n",
    "\n",
    "print('throughputs_times', throughputs_times)\n",
    "print('latencys_times', latencys_times)\n",
    "print('accuracys_times', accuracys_times)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "draw_bar(x, t, throughputs_times, 131, 'tab:green', 'Throughput Comparison (big is better)', '', width=0.2)\n",
    "draw_bar(x, t, latencys_times, 132, 'tab:blue', 'Latency Comparison (small is better)', '', width=0.2)\n",
    "draw_bar(x, t, accuracys_times, 133, '#28a99d', 'Accuracys Loss(%)', '', width=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Running is Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Performance Improvement:\n",
    "\n",
    "- FP32 to INT8.\n",
    "- Intel速 Deep Learning Boost speed up INT8 if your CPU is the Second Generation Intel速 Xeon速 Scalable Processors which supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
