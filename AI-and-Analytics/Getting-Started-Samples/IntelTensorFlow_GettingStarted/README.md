# `Intel® TensorFlow* Getting Started` Sample

The `Intel® TensorFlow* Getting Started` sample demonstrates how to train a TensorFlow* model and run inference using Intel® oneAPI Math Kernel Library (oneMKL) and Intel® oneAPI Deep Neural Networks (Intel® oneDNN).

| Area                       | Description
|:---                        |:---
| What you will learn        | The basics of using Intel® Optimization for TensorFlow* and the Intel® Extension for TensorFlow*
| Time to complete           | 10 minutes
| Category                   | Getting Started

## Purpose

TensorFlow* is a widely-used machine learning framework in the deep learning arena, demanding efficient computational resource utilization. To take full advantage of Intel® architecture and to extract maximum performance, the TensorFlow* framework has been optimized using Intel® oneDNN primitives. This sample demonstrates how to train an example neural network and shows how Intel-optimized TensorFlow* enables Intel® oneDNN calls by default. Intel-optimized TensorFlow* is available as part of the Intel® AI Analytics Toolkit (AI Kit).

This sample code shows how to get started with Intel® Optimization for TensorFlow*. It implements an example neural network with one convolution layer and one ReLU layer. You can build and train a TensorFlow* neural network using a simple Python code. Also, by controlling the build-in environment variable, this sample attempts to demonstrate explicitly how Intel® oneDNN Primitives are called and shows their performance during the neural network training.

## Prerequisites

| Optimized for          | Description
|:---                    |:---
| OS                     | Ubuntu* 18.0.x (and newer) <br> Windows* 10 
| Hardware               | Intel® Xeon® Scalable processor family
| Software               | Intel® AI Analytics Toolkit (AI Kit)

TensorFlow* is ready for use once you finish the Intel AI Analytics Toolkit installation. You can refer to the oneAPI [product page](https://software.intel.com/en-us/oneapi) for toolkit installation and the *[Get Started with the Intel® AI Analytics Toolkit for Linux*](https://software.intel.com/en-us/get-started-with-intel-oneapi-linux-get-started-with-the-intel-ai-analytics-toolkit)* for post-installation steps and scripts.

## Key Implementation Details

You must export the environment variable `ONEDNN_VERBOSE=1` to display the deep learning primitives trace during execution.

 - The training data is generated by `np.random`.
 - The neural network with one convolution layer and one ReLU layer is created by `tf.nn.conv2d` and `tf.nn.relu`.
 - The TF session is initialized by `tf.global_variables_initializer`.
 - The train is implemented via the below for-loop:
    ```python
    for epoch in range(0, EPOCHNUM):
        for step in range(0, BS_TRAIN):
            x_batch = x_data[step*N:(step+1)*N, :, :, :]
            y_batch = y_data[step*N:(step+1)*N, :, :, :]
            s.run(train, feed_dict={x: x_batch, y: y_batch})
    ```

>**Note**: For convenience, code line os.environ["ONEDNN_VERBOSE"] = "1" has been added in the body of the script as an alternative method to setting this variable.

Runtime settings for `ONEDNN_VERBOSE`, `KMP_AFFINITY`, and `Inter/Intra-op` Threads are set within the script. You can read more about these settings in this dedicated document: *[Maximize TensorFlow* Performance on CPU: Considerations and Recommendations for Inference Workloads](https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference)*.

### Run the Sample on Intel GPUs

The sample code is CPU based, but you can run it using Intel® Extension for TensorFlow* with Intel® Data Center GPU Flex Series. If you are using the Intel GPU, refer to *[Intel GPU Software Installation Guide](https://intel.github.io/intel-extension-for-tensorflow/latest/docs/install/install_for_gpu.html)*. The sample should be able to run on GPU without any code changes. 

For details, refer to the *[Quick Example on Intel CPU and GPU](https://intel.github.io/intel-extension-for-tensorflow/latest/examples/quick_example.html)* topic of the *Intel® Extension for TensorFlow** documentation. 

## Run the Sample Locally

These instructions demonstrate how to build and run a sample on a machine where you have installed the AI Kit. 

> **Note**: If you have not already done so, set up your CLI
> environment by sourcing  the `setvars` script in the root of your oneAPI installation.
>
> Linux*:
> - For system wide installations: `. /opt/intel/oneapi/setvars.sh`
> - For private installations: ` . ~/intel/oneapi/setvars.sh`
> - For non-POSIX shells, like csh, use the following command: `bash -c 'source <install-dir>/setvars.sh ; exec csh'`
>
> Windows*:
> - `C:\Program Files (x86)\Intel\oneAPI\setvars.bat`
> - Windows PowerShell*, use the following command: `cmd.exe "/K" '"C:\Program Files (x86)\Intel\oneAPI\setvars.bat" && powershell'`
>
> For more information on configuring environment variables, see *[Use the setvars Script with Linux* or macOS*](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/oneapi-development-environment-setup/use-the-setvars-script-with-linux-or-macos.html)* or *[Use the setvars Script with Windows*](https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/oneapi-development-environment-setup/use-the-setvars-script-with-windows.html)*.

### Activate Conda with Root Access

By default, the AI Kit is installed in the `intel/oneapi` folder, which requires root privileges to manage it.

1. Activate Conda.
   ```
   conda activate tensorflow
   ```

### Activate Conda without Root Access (Optional)

If you would like to bypass using root access to manage your conda environment, then you can clone and active your desired conda environment.

1. Enter the following commands.
   ```
   conda create --name user_tensorflow --clone tensorflow
   conda activate user_tensorflow
   ```

### Run the Script

1. Change to the sample directory.
2. Run the Python script.
   ```
   python TensorFlow_HelloWorld.py
   ```

#### Troubleshooting

If you receive an error message, troubleshoot the problem using the **Diagnostics Utility for Intel® oneAPI Toolkits**. The diagnostic utility provides configuration and system checks to help find missing dependencies, permissions errors, and other issues. See the *[Diagnostics Utility for Intel® oneAPI Toolkits User Guide](https://www.intel.com/content/www/us/en/develop/documentation/diagnostic-utility-user-guide/top.html)* for more information on using the utility.

### Run the Sample on Intel® DevCloud (Optional)

>**Note**: For more information on using Intel® DevCloud, see the Intel® oneAPI [Get Started](https://devcloud.intel.com/oneapi/get_started/) page.

1. Open a terminal on a Linux* system.
2. Log in to the Intel® DevCloud.
   ```
   ssh devcloud
   ```
3. Change to the sample directory.
4. Configure the sample for the appropriate node. 

   <details>
   <summary>You can specify nodes using a single line script.</summary>

   ```
   qsub  -I  -l nodes=1:xeon:ppn=2 -d .
   ```

   - `-I` (upper case I) requests an interactive session.
   - `-l nodes=1:xeon:ppn=2` (lower case L) assigns one full GPU node.
   - `-d .` makes the current folder as the working directory for the task.

     |Available Nodes    |Command Options
     |:---               |:---
     |GPU	             |`qsub -l nodes=1:gpu:ppn=2 -d .`
     |CPU	             |`qsub -l nodes=1:xeon:ppn=2 -d .`

    >**Note**: For more information on how to specify compute nodes read *[Launch and manage jobs](https://devcloud.intel.com/oneapi/documentation/job-submission/)* in  the Intel® DevCloud Documentation.
   </details>

5. Run the supplied script, which contains all the instructions needed to run this workload.
   ```
   ./q ./run.sh
   ```
6. Review the output.
7. Disconnect from Intel® DevCloud.
	```
	exit
	```

## Example Output

1. One the initial run, you should see results similar to the following:

   ```
   0 0.4147554
   1 0.3561021
   2 0.33979267
   3 0.33283564
   4 0.32920069
   [CODE_SAMPLE_COMPLETED_SUCCESSFULLY]
   ```

2. Export `ONEDNN_VERBOSE` as 1 in the command line. The oneDNN run-time verbose trace should look similar to the following:
   ```
   export ONEDNN_VERBOSE=1
   Windows: set ONEDNN_VERBOSE=1
   ```
   >**Note**: The historical environment variables include `DNNL_VERBOSE` and `MKLDNN_VERBOSE`.

3. Run the sample again. You should see verbose results similar to the following:
   ```
   2022-04-24 16:56:02.497963: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
   To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
   onednn_verbose,info,oneDNN v2.5.0 (commit N/A)
   onednn_verbose,info,cpu,runtime:OpenMP
   onednn_verbose,info,cpu,isa:Intel AVX-512 with Intel DL Boost
   onednn_verbose,info,gpu,runtime:none
   onednn_verbose,info,prim_template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time
   onednn_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f dst_f32:p:blocked:Acdb16a:f,,,10x4x3x3,0.00195312
   onednn_verbose,exec,cpu,convolution,brgconv:avx512_core,forward_training,src_f32::blocked:acdb:f wei_f32:p:blocked:Acdb16a:f bia_f32::blocked:a:f dst_f32::blocked:acdb:f,attr-post-ops:eltwise_relu ,alg:convolution_direct,mb,4.96411
   onednn_verbose,exec,cpu,convolution,jit:avx512_common,backward_weights,src_f32::blocked:acdb:f wei_f32:p:blocked:Acdb16a:f bia_undef::undef::f dst_f32::blocked:acdb:f,,alg:convolution_direct,mb,0.567871
   ...
   ```

>**Note**: See the *[oneAPI Deep Neural Network Library Developer Guide and Reference](https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html)* for more details on the verbose log.

## License

Code samples are licensed under the MIT license. See
[License.txt](https://github.com/oneapi-src/oneAPI-samples/blob/master/License.txt) for details.

Third-party program Licenses can be found here: [third-party-programs.txt](https://github.com/oneapi-src/oneAPI-samples/blob/master/third-party-programs.txt).