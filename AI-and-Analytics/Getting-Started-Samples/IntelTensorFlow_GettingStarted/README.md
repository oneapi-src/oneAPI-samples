# TensorFlow HelloWorld Sample
TensorFlow* is a widely-used machine learning framework in the deep learning arena, demanding efficient utilization of computational resources. In order to take full advantage of Intel® architecture and to extract maximum performance, the TensorFlow framework has been optimized using Intel® Deep Neural Networks (Intel® DNNL) primitives. This sample demonstrates how to train an example neural network and shows how Intel-optimized TensorFlow enables Intel® DNNL calls by default. 

| Optimized for                       | Description
|:---                               |:---
| OS                                | Linux* Ubuntu* 18.04 
| Hardware                          | Intel® Xeon® Scalable processor family or newer
| Software                          | Intel® oneAPI AI Analytics Toolkit
| What you will learn               | How to get start to use Intel optimization for TensorFlow*
| Time to complete                  | 10 minutes

## Purpose
This sample code shows how to get started with Intel Optimization for TensorFlow*. It implements an example neural network with one convolution layer and one ReLU layer. Developers can quickly build and train a Tensorflow neural network using simple python code. In addition, by controlling the build-in environment variable, the sample attempts to show explicitly how Intel® DNNL Primitives are called as well as their performance during the neural network training. 

Intel-optimized Tensorflow is available as part of Intel® AI Analytics Toolkit. For more information on the optimizations as well as performance data, see this blog post [TensorFlow* Optimizations on Modern Intel® Architecture](https://software.intel.com/content/www/us/en/develop/articles/tensorflow-optimizations-on-modern-intel-architecture.html).

## Key implementation details
*Please* **export the environment variable `MKLDNN_VERBOSE=1`** *to display the deep learning primitives trace during execution.*

 - The train data is generated by `np.random`. 
 - The nerual network with one convoluation layer and one ReLU layer is created by `tf.nn.conv2d` and `tf.nn.relu`.
 - The TF session is inistialized by `tf.global_variables_initializer`.
 - The train is implemented via the below for-loop: 
    ```python
    for epoch in range(0, EPOCHNUM):
        for step in range(0, BS_TRAIN):
            x_batch = x_data[step*N:(step+1)*N, :, :, :]
            y_batch = y_data[step*N:(step+1)*N, :, :, :]
            s.run(train, feed_dict={x: x_batch, y: y_batch})
    ```
    
##### Note: For convenience, code line os.environ["MKLDNN_VERBOSE"] = "1" has been added in the body of the script as an alternative method to setting this variable.

Runtime settings for `MKLDNN_VERBOSE`, `KMP_AFFINITY`, and `Inter/Intra-op` Threads are set within the script. You can read more about these settings in this dedicated document: [Maximize TensorFlow Performance on CPU: Considerations and Recommendations for Inference Workloads](https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference) 
    
## License  
This code sample is licensed under MIT license.

## Build and Run the Sample

### Running Samples In DevCloud (Optional)
If running a sample in the Intel DevCloud, please follow the below steps to build the python environment. Also remember that you must specify the compute node (CPU, GPU, FPGA) as well whether to run in batch or interactive mode. For more information see the Intel® oneAPI Base Toolkit Get Started Guide (https://devcloud.intel.com/oneapi/get-started/base-toolkit/) 

### Pre-requirement

TensorFlow is ready for use once you finish the Intel AI Analytics Toolkit installation, and have run post installation script.

You can refer to the oneAPI [main page](https://software.intel.com/en-us/oneapi) for toolkit installation, and the Toolkit [Getting Started Guide for Linux](https://software.intel.com/en-us/get-started-with-intel-oneapi-linux-get-started-with-the-intel-ai-analytics-toolkit) for post-installation steps and scripts.

### On a Linux* System
#### Activate conda environment With Root Access

Please follow the Getting Started Guide steps (above) to set up your oneAPI environment with the setvars.sh script. Then navigate in linux shell to your oneapi installation path, typically `~/intel/oneapi`. Activate the conda environment with the following command:

```
source /opt/intel/oneapi/setvars.sh
source activate tensorflow
```

please replace ~/intel/oneapi for your oneapi installation path.

#### Activate conda environment Without Root Access (Optional)

By default, the Intel AI Analytics toolkit is installed in the inteloneapi folder, which requires root privileges to manage it. If you would like to bypass using root access to manage your conda environment, then you can clone your desired conda environment using the following command:

```
conda create --name user_tensorflow --clone tensorflow
```

Then activate your conda environment with the following command:

```
source activate user_tensorflow
```

## Running the Sample

To run the program on Linux* or the environment of Intel DevCloud, type the following command in the terminal with Python installed:
```
    python TensorFlow_HelloWorld.py
```
### Example of Output
With successful execution, it will print out the following results:

```
    0 0.4147554
    1 0.3561021
    2 0.33979267
    3 0.33283564
    4 0.32920069
    PASSED_CICD 
```

The mkldnn run-time verbose trace should look similar to what is shown below:

```
2019-07-23 13:54:42.599871: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
mkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_nhwc out:f32_nchw,num:1,64x4x128x128,22.9941
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_hwio out:f32_Ohwi16o,num:1,10x4x3x3,0.0249023
mkldnn_verbose,exec,convolution,jit:avx512_common,forward_training,fsrc:nchw fwei:Ohwi16o fbia:undef fdst:nChw16c,alg:convolution_direct,mb64_g1ic4oc10_ih128oh128kh3sh1dh0ph1_iw128ow128kw3sw1dw0pw1,1.302
mkldnn_verbose,exec,reorder,simple:any,undef,in:f32_nChw16c out:f32_nhwc,num:1,64x10x128x128,5.146
mkldnn_verbose,exec,eltwise,jit:avx512_common,forward_training,fdata:blocked fdiff:undef,alg:eltwise_relu,mb64ic128ih128iw10,0.744141
mkldnn_verbose,exec,eltwise,jit:avx512_common,backward_data,fdata:blocked fdiff:blocked,alg:eltwise_relu,mb64ic128ih128iw10,3.96899
```
Please see the [DNNL Developer's Guide](https://intel.github.io/mkl-dnn/dev_guide_verbose.html) for more details on the verbose log. 

