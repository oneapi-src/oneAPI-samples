{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel® Low Precision Optimization Tool (LPOT) Sample for Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "- Train a CNN Model Based on Keras\n",
    "- Quantize Keras Model by LPOT\n",
    "- Compare Quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LPOT Release and Sample \n",
    "\n",
    "This sample code is always updated for the LPOT release in latest oneAPI release.\n",
    "\n",
    "If you want to get the sample code for old oneAPI release, please checkout the old sample code release by git tag.\n",
    "\n",
    "1. Check tags\n",
    "\n",
    "```\n",
    "git pull\n",
    "git tag\n",
    "\n",
    "2021.1-beta08\n",
    "2021.1-beta09\n",
    "2021.1-beta10\n",
    "\n",
    "```\n",
    "\n",
    "2. Checkout old code release\n",
    "```\n",
    "git checkout 2021.1-beta10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages and check version.\n",
    "\n",
    "Make sure the Tensorflow is **2.2** or newer and LPOT is **1.0, 1,1 ** or **1.1**, matplotlib are installed.\n",
    "\n",
    "Note， LPOT has an old name **ilit**. Following script supports to old package name **ilit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "try:\n",
    "    import lpot\n",
    "    print(\"LPOT version {}\".format(lpot.__version__)) \n",
    "except:\n",
    "    import ilit as lpot\n",
    "    print(\"iLiT version {}\".format(lpot.__version__))       \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN Model Based on Keras\n",
    "\n",
    "We prepare a script '**alexnet.py**' to provide the functions to train a CNN model.\n",
    "\n",
    "### Dataset\n",
    "Use [MNIST](http://yann.lecun.com/exdb/mnist/) dataset to recognize hand writing numbers. \n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alexnet\n",
    " \n",
    "data = alexnet.read_data()\n",
    "x_train, y_train, label_train, x_test, y_test, label_test = data\n",
    "print('train', x_train.shape, y_train.shape, label_train.shape)\n",
    "print('test', x_test.shape, y_test.shape, label_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "Build a CNN model like Alexnet by Keras API based on Tensorflow.\n",
    "Print the model structure by Keras API: summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "width = 28\n",
    "channels = 1\n",
    "\n",
    "model = alexnet.create_model(width ,channels ,classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model with the Dataset\n",
    "\n",
    "Set the **epochs** to \"**3**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "alexnet.train_mod(model, data, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze and Save Model to Single PB\n",
    "\n",
    "Set the input node name is \"**x**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "def save_frezon_pb(model, mod_path):\n",
    "    # Convert Keras model to ConcreteFunction\n",
    "    full_model = tf.function(lambda x: model(x))\n",
    "    concrete_function = full_model.get_concrete_function(\n",
    "        x=tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n",
    "\n",
    "    # Get frozen ConcreteFunction\n",
    "    frozen_model = convert_variables_to_constants_v2(concrete_function)\n",
    "\n",
    "    # Generate frozen pb\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_model.graph,\n",
    "                      logdir=\".\",\n",
    "                      name=mod_path,\n",
    "                      as_text=False)\n",
    "fp32_frezon_pb_file = \"fp32_frezon.pb\"\n",
    "save_frezon_pb(model, fp32_frezon_pb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la fp32_frezon.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize FP32 Model by LPOT\n",
    "\n",
    "LPOT supports to quantize the model with a validation dataset for tuning.\n",
    "Finally, it returns an frezon quantized model based on int8.\n",
    "\n",
    "We prepare a python script \"**LPOT_quantize_model.py**\" to call LPOT to finish the all quantization job.\n",
    "Following code sample is used to explain the code.\n",
    "\n",
    "### Define Dataloader\n",
    "\n",
    "The class **Dataloader** provides an iter function to return the image and label as batch size.\n",
    "We uses the validation data of MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_dataset\n",
    "import math\n",
    "\n",
    "\n",
    "class Dataloader(object):\n",
    "  def __init__(self, batch_size):\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "\n",
    "  def __iter__(self):\n",
    "    x_train, y_train, label_train, x_test, y_test,label_test = mnist_dataset.read_data()\n",
    "    batch_nums = math.ceil(len(x_test)/self.batch_size)\n",
    "\n",
    "    for i in range(batch_nums-1):\n",
    "        begin = i*self.batch_size\n",
    "        end = (i+1)*self.batch_size\n",
    "        yield x_test[begin: end], label_test[begin: end]\n",
    "\n",
    "    begin = (batch_nums-1)*self.batch_size\n",
    "    yield x_test[begin:], label_test[begin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Load FP32 Model\n",
    "Load the saved fp32 model in previous step.\n",
    "\n",
    "It's defined as alexnet.load_pb(in_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat alexnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Yaml File\n",
    "\n",
    "We define alexnet.yaml to save the necessary parameters for LPOT.\n",
    "In this case, we only need to change the input/output according to the fp32 model.\n",
    "\n",
    "In this case, the input node name is '**x**'.\n",
    "\n",
    "Output name is '**Identity**'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat alexnet.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tuning Function\n",
    "We follow the template to create the tuning function. The function will return a frezon quantized model (int8 model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def auto_tune(input_graph_path, yaml_config, batch_size):    \n",
    "    fp32_graph = alexnet.load_pb(input_graph_path)\n",
    "    quan = lpot.Quantization(yaml_config)\n",
    "    dataloader = Dataloader(batch_size)\n",
    "    assert(dataloader)\n",
    "    q_model = quan(\n",
    "                        fp32_graph,\n",
    "                        q_dataloader=dataloader,\n",
    "                        eval_func=None,\n",
    "                        eval_dataloader=dataloader)\n",
    "    return q_model\n",
    "\n",
    "\n",
    "def save_int8_frezon_pb(q_model, path):\n",
    "    from tensorflow.python.platform import gfile\n",
    "    f = gfile.GFile(path, 'wb')\n",
    "    f.write(q_model.as_graph_def().SerializeToString())\n",
    "    print(\"Save to {}\".format(path))\n",
    "    \n",
    "yaml_file = \"alexnet.yaml\"\n",
    "batch_size = 200\n",
    "int8_pb_file = \"alexnet_int8_model.pb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Function to Quantize the Model\n",
    "\n",
    "Show the code in \"**lpot_quantize_model.py**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat  lpot_quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will execute the \"**lpot_quantize_model.py**\" to show the whole process of quantizing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python lpot_quantize_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a quantized model file \"**alexnet_int8_model.pb**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Quantized Model\n",
    "\n",
    "We prepare a script **profiling_lpot.py** to test the performance of PB model.\n",
    "\n",
    "There is no correct performance data if run the code by jupyter notebook. So we run the script as process.\n",
    "\n",
    "Let learn **profiling_lpot.py**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat profiling_lpot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the **profiling_lpot.py** with FP32 model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python profiling_lpot.py --input-graph=./fp32_frezon.pb --omp-num-threads=4 --num-inter-threads=1 --num-intra-threads=4 --index=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the **profiling_lpot.py** with int8 model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python profiling_lpot.py --input-graph=./alexnet_int8_model.pb --omp-num-threads=4 --num-inter-threads=1 --num-intra-threads=4 --index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 32.json\n",
    "!echo \" \"\n",
    "!cat 8.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the functions to load and show the performance data from 32.json & 8.sjon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def autolabel(ax, rects):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%0.2f' % float(height),\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "def draw_bar(x, t, y, subplot, color, x_lab, y_lab, width=0.2):\n",
    "    plt.subplot(subplot)\n",
    "    plt.xticks(x, t)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.set_xlabel(x_lab)\n",
    "    ax1.set_ylabel(y_lab, color=color)\n",
    "    rects1 = ax1.bar(x, y, color=color, width=width)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    autolabel(ax1, rects1)\n",
    "\n",
    "def load_res(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "\n",
    "res_32 = load_res('32.json')\n",
    "res_8 = load_res('8.json')\n",
    "   \n",
    "accuracys = [res_32['accuracy'], res_8['accuracy']]\n",
    "throughputs = [res_32['throughput'], res_8['throughput']]             \n",
    "latencys = [res_32['latency'], res_8['latency']]\n",
    "\n",
    "print('throughputs', throughputs)\n",
    "print('latencys', latencys)\n",
    "print('accuracys', accuracys)\n",
    "\n",
    "accuracys_perc = [accu*100 for accu in accuracys]\n",
    "\n",
    "t = ['FP32', 'INT8']\n",
    "x = [0, 1]\n",
    "plt.figure(figsize=(16,6))\n",
    "draw_bar(x, t, throughputs, 131, 'tab:green', 'Throughput(fps)', '', width=0.2)\n",
    "draw_bar(x, t,  latencys, 132, 'tab:blue', 'Latency(s)', '', width=0.2)\n",
    "draw_bar(x, t,  accuracys_perc, 133, '#28a99d', 'Accuracys(%)', '', width=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32 vs INT8\n",
    "\n",
    "Compare the performance data based on data of FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs_times = [1, throughputs[1]/throughputs[0]]\n",
    "latencys_times = [1, latencys[1]/latencys[0]]\n",
    "accuracys_times = [0, accuracys_perc[1] - accuracys_perc[0]]\n",
    "\n",
    "print('throughputs_times', throughputs_times)\n",
    "print('latencys_times', latencys_times)\n",
    "print('accuracys_times', accuracys_times)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "draw_bar(x, t, throughputs_times, 131, 'tab:green', 'Throughput Comparison (big is better)', '', width=0.2)\n",
    "draw_bar(x, t, latencys_times, 132, 'tab:blue', 'Latency Comparison (small is better)', '', width=0.2)\n",
    "draw_bar(x, t, accuracys_times, 133, '#28a99d', 'Accuracys Loss(%)', '', width=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Running is Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[CODE_SAMPLE_COMPLETED_SUCCESFULLY]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Performance Improvement:\n",
    "\n",
    "- FP32 to INT8.\n",
    "- Intel® Deep Learning Boost speed up INT8 if your CPU is the Second Generation Intel® Xeon® Scalable Processors which supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-vlpot] *",
   "language": "python",
   "name": "conda-env-miniconda3-vlpot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
